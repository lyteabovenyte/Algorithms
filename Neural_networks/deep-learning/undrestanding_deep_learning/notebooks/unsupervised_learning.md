#### Unsupervised learning:

so far in earlier notebook we discussed supervised learning which and architecture which leverage parameter sharing and allowing parallel computational path. now let's get into unsupervised learning era.

The defining characteristic of unsupervised learning models is that they are learned
from a set of observed data $\{xi\}$ in the *absence of labels*. All unsupervised models share this property, but they have diverse goals. They may be used to generate plausible new samples from the dataset or to manipulate, denoise, interpolate between, or compress
examples. They can also be used to reveal the internal structure of a dataset (e.g., by
dividing it into coherent clusters) or to distinguish whether new examples belong to the
same dataset or are outliers.

A common strategy in unsupervised learning is to define a mapping between the data
examples $x$ and a set of unseen **latent variables $z$**. These latent variables capture underlying structure in the dataset and usually have a lower dimension than the original data; in this sense, a latent variable $z$ can be considered a *compressed* version of a data example $x$ that captures its essential qualities. so generally, Unsupervised learning refers to any model trained on datasets without labels. Generative models can synthesize (generate) new examples with similar statistics to the training data. A subset of these are probabilistic and define a distribution over the data. We
draw samples from this distribution to generate new examples. Latent variable models define a mapping between an underlying explanatory (latent) variable and the data and may fall into any of the above categories.

<img src=../images/taxamony_unsupervised_learning.png width=350>


some models map from data $x$ to latent variable $z$, but some go on the other direction by Considering a distribution $Pr(z)$ over the latent variable $z$ in these models. New examples can now be generated by (i) drawing from this distribution and (ii) mapping the sample to the data space $x$. Accordingly, these are termed **generative models**.

for example, **Generative adversarial networks** learn to generate data examples $x^âˆ—$
from latent variables $z$, using a loss that encourages the generated samples to be indistinguishable from real examples.

*Normalizing flows, variational autoencoders, and diffusion models* are **probabilistic generative models**. In addition to generating new examples, they assign a
probability $Pr(x|\phi)$ to each data point $x$. This will depend on the model parameters $\phi$, and in training, we maximize the probability of the observed data {xi}, so the loss is the sum of the negative log-likelihoods:
$$L[\phi] = - \sum_{i=1}^{I} \log \left[ \text{Pr}(x_i|\phi) \right]$$

$?$ benefits of assigning a probability distribution to each generated sample $?$

Since *probability distributions* must sum to one, this implicitly reduces the probability
of examples that lie far from the observed data. As well as providing a training criterion,
assigning probabilities is useful in its own right; the probability on a test set can be
used to compare two models quantitatively, and the probability for an example can be
thresholded to determine if it belongs to the same dataset or is an **outlier**.

**Marginal class distribution:**

$$
\text{Marginal class distribution} \\
\Pr(y) = \frac{1}{I} \sum_{i=1}^{I} \Pr(y \mid \mathbf{x}_i^*)
$$


The marginal class distribution refers to the distribution of class labels independently of any other features in a dataset. It tells you how frequently each class appears overall, regardless of input features. This metric is only sensible for generative models of the ImageNet database and is sensitive to the particular classification model; retraining this model can give quite
different numerical results. Moreover, it does not reward diversity within an object class; it returns a high value if the model only generates one realistic example of each class.


so, **Overall**:

Unsupervised models learn about the structure of a dataset in the absence of labels. A
subset of these models is generative and can synthesize new data examples. A further
subset is probabilistic in that they can both generate new examples and assign a probability to observed data. The models considered in the following four notebooks on the topic start with
a latent variable $z$ which has a known distribution. A deep neural network then maps
from the latent variable to the observed data space. We considered desirable properties
of generative models and introduced metrics that attempt to quantify their performance.
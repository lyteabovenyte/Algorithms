{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring performance"
      ],
      "metadata": {
        "id": "rtA8SwvOOGob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previous notebooks described neural network models, loss functions, and training algorithms. This one considers how to measure the performance of the trained models.\n",
        "With suï¬€icient capacity (i.e., number of hidden units), a neural network model will often\n",
        "perform perfectly on the training data. However, this does not necessarily mean it will\n",
        "generalize well to new test data.\n",
        "We will see that the test errors have three distinct causes and that their relative\n",
        "contributions depend on (i) the inherent uncertainty in the task, (ii) the amount of\n",
        "training data, and (iii) the choice of model. The latter dependency raises the issue of\n",
        "*hyperparameter* search."
      ],
      "metadata": {
        "id": "zlZ8XBylOMqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. model parameters -> the number of hidden units and the number of hidden layers\n",
        "2. learning algorithm hyperparameters -> learning rate and batch size"
      ],
      "metadata": {
        "id": "FVqzV7-qOnE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three possible sources of error, which are known as *noise*, *bias*, and *variance* respectively"
      ],
      "metadata": {
        "id": "qt_uc6t9tosb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical formulation of test error\n",
        "\n",
        "We now make the notions of noise, bias, and variance mathematically precise. Consider a 1D regression problem where the data generation process has additive noise with variance $\\sigma^2$; we can observe different outputs $y$ for the same input $x$, so for each $x$, there is a distribution $P_{r}(y|x)$ with expected value (mean) $\\mu[x]$:\n",
        "\n",
        "$$\n",
        "\\mu[x] = \\mathbb{E}_y[y|x] = \\int y|x| P_{r}(y|x) dy,\n",
        "$$\n",
        "\n",
        "and fixed noise $\\sigma^2 = \\mathbb{E}_y [(\\mu[x] - y[x])^2]$. Here we have used the notation $y[x]$ to specify that we are considering the output $y$ at a given input position $x$.\n",
        "\n",
        "Now consider a least squares loss between the model prediction $f[x, \\phi]$ at position $x$ and the observed value $y[x]$ at that position:\n",
        "\n",
        "$$\n",
        "L[x] = (f[x, \\phi] - y[x])^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\left((f[x, \\phi] - \\mu[x]) + (\\mu[x] - y[x])\\right)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (f[x, \\phi] - \\mu[x])^2 + 2(f[x, \\phi] - \\mu[x])(\\mu[x] - y[x]) + (\\mu[x] - y[x])^2,\n",
        "$$\n",
        "\n",
        "where we have both added and subtracted the mean $\\mu[x]$ of the underlying function in the second line and have expanded out the squared term in the third line.\n",
        "\n",
        "The underlying function is stochastic, so this loss depends on the particular $y[x]$ we observe. The expected loss is:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_y [L[x]] = \\mathbb{E}_y \\left[ (f[x, \\phi] - \\mu[x])^2 + 2(f[x, \\phi] - \\mu[x])(\\mu[x] - y[x]) + (\\mu[x] - y[x])^2 \\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (f[x, \\phi] - \\mu[x])^2 + 2(f[x, \\phi] - \\mu[x])(\\mu[x] - \\mathbb{E}_y [y[x]]) + \\mathbb{E}_y [(\\mu[x] - y[x])^2]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (f[x, \\phi] - \\mu[x])^2 + 2(f[x, \\phi] - \\mu[x]) \\cdot 0 + \\mathbb{E}_y [(\\mu[x] - y[x])^2]\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (f[x, \\phi] - \\mu[x])^2 + \\sigma^2,\n",
        "$$\n",
        "\n",
        "where we have made use of the rules for manipulating expectations. In the second line, we have distributed the expectation operator and removed it from terms with no dependence on $y[x]$, and in the third line, we note that the second term is zero since $\\mathbb{E}_y [y[x]] = \\mu[x]$ by definition. Finally, in the fourth line, we have substituted in the definition of the fixed noise $\\sigma^2$. We can see that the expected loss has been broken down into two terms; the\n",
        "first term is the squared deviation between the model and the true function mean, and\n",
        "the second term is the noise."
      ],
      "metadata": {
        "id": "5vrHWjVO3RiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first term can be further partitioned into bias and variance. The parameters $\\phi$ of the model $f[x, \\phi]$ depend on the training dataset $D = \\{x_i, y_i\\}$, so more properly, we should write $f[x, \\phi[D]]$. The training dataset is a random sample from the data generation process; with a different sample of training data, we would learn different parameter values. The expected model output $f_\\mu[x]$ with respect to all possible datasets $D$ is hence:\n",
        "\n",
        "$$\n",
        "f_\\mu[x] = \\mathbb{E}_D [f[x, \\phi[D]]].\n",
        "$$\n",
        "\n",
        "Returning to the first term of equation 8.3, we add and subtract $f_\\mu[x]$ and expand:\n",
        "\n",
        "$$\n",
        "(f[x, \\phi[D]] - \\mu[x])^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (f[x, \\phi[D]] - f_\\mu[x] + (f_\\mu[x] - \\mu[x]))^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (f[x, \\phi[D]] - f_\\mu[x])^2 + 2(f[x, \\phi[D]] - f_\\mu[x])(f_\\mu[x] - \\mu[x]) + (f_\\mu[x] - \\mu[x])^2.\n",
        "$$\n",
        "\n",
        "We then take the expectation with respect to the training dataset $D$:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_D [(f[x, \\phi[D]] - \\mu[x])^2] = \\mathbb{E}_D [(f[x, \\phi[D]] - f_\\mu[x])^2] + (f_\\mu[x] - \\mu[x])^2,\n",
        "$$\n",
        "\n",
        "where we have simplified using similar steps as for equation 8.3. Finally, we substitute this result into equation 8.3:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_D [\\mathbb{E}_y [L[x]]] = \\mathbb{E}_D [(f[x, \\phi[D]] - f_\\mu[x])^2] + (f_\\mu[x] - \\mu[x])^2 + \\sigma^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbb{E}_D [(f[x, \\phi[D]] - f_\\mu[x])^2]+(f_\\mu[x]- \\mu[x])^2 + \\sigma^2\n",
        "$$\n",
        "\n",
        "This equation says that the expected loss after considering the uncertainty in the training data $D$ and the test data $y$ consists of three additive components. The variance is uncertainty in the fitted model due to the particular training dataset we sample. The bias is the systematic deviation of the model from the mean of the function we are modeling. The noise is the inherent uncertainty in the true mapping from input to output. These three sources of error will be present for any task. They combine additively for regression tasks with a least squares loss. However, their interaction can be more complex for other types of problems."
      ],
      "metadata": {
        "id": "zD_SenAZ5MZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reducing variance**:\n",
        "Recall that the variance results from limited noisy training data. Fitting the model\n",
        "to two different training sets results in slightly different parameters. It follows we can\n",
        "reduce the variance by increasing the quantity of training data. This averages out the\n",
        "inherent noise and ensures that the input space is well sampled.\n",
        "the effect of training with 6, 10, and 100 samples. For each dataset\n",
        "size, we show the best-fitting model for three training datasets. With only six samples,\n",
        "the fitted function is quite different each time: the variance is significant. As we increase\n",
        "the number of samples, the fitted models become very similar, and the variance reduces.\n",
        "In general, adding training data almost always improves test performance."
      ],
      "metadata": {
        "id": "8xzKdcPhfq87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reducing bias**:\n",
        "The bias term results from the inability of the model to describe the true underlying\n",
        "function. This suggests that we can reduce this error by making the model more flexible.\n",
        "This is usually done by increasing the model capacity. For neural networks, this means\n",
        "adding more hidden units and/or hidden layers."
      ],
      "metadata": {
        "id": "5tGcahzOhRHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also there exists an unexpected side-effect of increasing the model capacity.\n",
        "For a fixed-size training dataset, the variance term typically increases as the model\n",
        "capacity increases. Consequently, increasing the model capacity does not necessarily\n",
        "reduce the test error. This is known as the bias-variance trade-off."
      ],
      "metadata": {
        "id": "FSSnxQQjhXDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "modeling the noise with more flexibility: *Overfitting*.\n",
        "\n",
        "as we add capacity to the model, the bias decreases, but the variance\n",
        "increases for a fixed-size training dataset."
      ],
      "metadata": {
        "id": "BfzuazlBh3zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Aspect            | Noise                          | Bias                          | Variance                      |\n",
        "|-------------------|--------------------------------|-------------------------------|-------------------------------|\n",
        "| **Source**        | Inherent in the data          | Modelâ€™s simplicity            | Modelâ€™s complexity            |\n",
        "| **Effect**        | Irreducible error             | Underfitting                  | Overfitting                   |\n",
        "| **Train vs Test** | Affects both equally          | Poor on both                  | Good on train, poor on test  |\n",
        "| **Remedy**        | Better data (if possible)     | Increase model complexity     | Reduce complexity, add regularization |"
      ],
      "metadata": {
        "id": "ggC1HofZlsuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Double descent**:\n",
        "1. classical or under-parameterized regime.\n",
        "2. modern or over-parameterized regime.\n",
        "3. critical regime.\n"
      ],
      "metadata": {
        "id": "fD6MJ42_nImJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tendency of a model to prioritize one solution over another as\n",
        "it extrapolates between data points is known as its **inductive bias**."
      ],
      "metadata": {
        "id": "vE5w3R8pqPJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tendency of the volume of\n",
        "high-dimensional space to overwhelm the number of training points is termed the **curse\n",
        "of dimensionality**."
      ],
      "metadata": {
        "id": "Wo_jnHXjrjH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing capacity (hidden units) allows smoother interpolation between sparse data points."
      ],
      "metadata": {
        "id": "M5j0JZGN1_U4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###the reason behing double decsend:\n",
        "Itâ€™s certainly true that as we add more capacity to the\n",
        "model, it will have the capability to create smoother functions. we've shown the\n",
        "smoothest possible functions that still pass through the data points as we increase the\n",
        "number of hidden units. When the number of parameters is very close to the number\n",
        "of training data examples, the model is forced to contort itself to fit the\n",
        "training data exactly, resulting in erratic predictions. This explains why the peak in the\n",
        "double descent curve is so pronounced. As we add more hidden units, the model has the\n",
        "ability to construct smoother functions that are likely to generalize better to new data."
      ],
      "metadata": {
        "id": "Otq0oBo52SVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Any factor that biases a model toward\n",
        "a subset of the solutions with a similar training loss is known as a **regularizer**."
      ],
      "metadata": {
        "id": "AFWmqz1a22My"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a deep network, the model capacity depends on the numbers of hidden layers\n",
        "and hidden units per layer as well as other aspects of architecture, Furthermore, the choice of learning algorithm and any associated parameters\n",
        "(learning rate, etc.) also affects the test performance. These elements are collectively\n",
        "termed *hyperparameters*. The process of finding the best hyperparameters is termed\n",
        "**hyperparameter search** or (when focused on network structure)** neural architecture search**."
      ],
      "metadata": {
        "id": "WxRlGxHU3YOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters are typically chosen empirically."
      ],
      "metadata": {
        "id": "jvrqB0Tx4LE1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "test error for regression problems with least\n",
        "squares loss decomposes into the sum of noise, bias, and variance terms. These factors are\n",
        "all present for models with other losses, but their interaction is typically more complicated."
      ],
      "metadata": {
        "id": "YdW4Sqtv4iNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification problems, there are some counter-intuitive predictions; for example, if the model is biased toward selecting the wrong class in a region of\n",
        "the input space, then increasing the variance can improve the classification rate as this pushes\n",
        "some of the predictions over the threshold to be classified correctly."
      ],
      "metadata": {
        "id": "0qys6rUV5-2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it is typical to divide the data into three parts: training\n",
        "data (which is used to learn the model parameters), validation data (which is used to choose\n",
        "the hyperparameters), and test data (which is used to estimate the final performance). This\n",
        "approach is known as cross-validation. However, this division may cause problems where the\n",
        "total number of data examples is limited; if the number of training examples is comparable to\n",
        "the model capacity, then the variance will be large. One way to mitigate this problem is to use **k-fold cross-validation**. The training and validation\n",
        "data are partitioned into K disjoint subsets. For example, we might divide these data into\n",
        "five parts. We train with four and validate with the fifth for each of the five permutations\n",
        "and choose the hyperparameters based on the average validation performance. The final test\n",
        "performance is assessed using the average of the predictions from the five models with the best\n",
        "hyperparameters on an entirely different test set. There are many variations of this idea, but\n",
        "all share the general goal of using a larger proportion of the data to train the model, thereby\n",
        "reducing variance"
      ],
      "metadata": {
        "id": "GdlxDO396IE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used the term capacity informally to mean the number of parameters or\n",
        "hidden units in the model (and hence indirectly, the ability of the model to fit functions of\n",
        "increasing complexity). The **representational capacity** of a model describes the space of possible\n",
        "functions it can construct when we consider all possible parameter values. When we take into\n",
        "account the fact that an optimization algorithm may not be able to reach all of these solutions,\n",
        "what is left is the effective capacity."
      ],
      "metadata": {
        "id": "iDMhU_ES6SfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are three main reasons why real-world performance may be worse than the test perfor-\n",
        "mance implies. First, the statistics of the input data x may change; we may now be observing\n",
        "parts of the function that were sparsely sampled or not sampled at all during training. This\n",
        "is known as covariate shift. Second, the statistics of the output data y may change; if some\n",
        "output values are infrequent during training, then the model may learn not to predict these in\n",
        "ambiguous situations and will make mistakes if they are more common in the real world. This\n",
        "is known as prior shift. Third, the relationship between input and output may change. This is\n",
        "known as concept shift."
      ],
      "metadata": {
        "id": "y9wMUKLo7ImP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "further research: Bayesian optimization??\n",
        "**hyperparameter tuning algorithms?**"
      ],
      "metadata": {
        "id": "E5ZLXydmYqe1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "be0UGGmdZW67"
      }
    }
  ]
}
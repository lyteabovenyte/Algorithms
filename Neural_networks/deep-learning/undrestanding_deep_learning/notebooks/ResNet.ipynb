{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (Residual nerual networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous notebook described how image classification performance improved as the\n",
    "depth of convolutional networks was extended from eight layers (AlexNet) to nineteen\n",
    "layers (VGG). This led to experimentation with even deeper networks. However, performance decreased again when many more layers were added.\n",
    "This notebook introduces *residual blocks*. Here, each network layer computes an *additive change* to the current representation instead of transforming it directly. This allows\n",
    "deeper networks to be trained but causes an exponential increase in the activation magnitudes at initialization. Residual blocks employ **batch normalization** to compensate for\n",
    "this, which re-centers and rescales the activations at each layer.\n",
    "Residual blocks with batch normalization allow much deeper networks to be trained,\n",
    "and these networks improve performance across a variety of tasks. Architectures that\n",
    "combine residual blocks to tackle image classification, medical image segmentation, and\n",
    "human pose estimation are described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every network we have seen so far processes the data sequentially; each layer receives\n",
    "the previous layer’s output and passes the result to the next. For example,\n",
    "a three-layer network is defined by:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{h}_1 &= f_1[\\mathbf{x}, \\phi_1] \\\\\n",
    "\\mathbf{h}_2 &= f_2[\\mathbf{h}_1, \\phi_2] \\\\\n",
    "\\mathbf{h}_3 &= f_3[\\mathbf{h}_2, \\phi_3] \\\\\n",
    "\\mathbf{y} &= f_4[\\mathbf{h}_3, \\phi_4],\n",
    "\\end{align*}$$\n",
    "\n",
    "where $h_1, h_2, h_3$ denote the intermediate hidden layers, x is the network input, y\n",
    "is the output, and the functions $f_k[\\bullet,\\phi_k]$ perform the processing.\n",
    "In a standard neural network, each layer consists of a linear transformation followed\n",
    "by an activation function, and the parameters $\\phi_k$ comprise the weights and biases of the linear transformation. In a convolutional network, each layer consists of a set of convolutions followed by an activation function, *and the parameters comprise the convolutional kernels and biases.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shattered gradient:\n",
    "<img src=../images/shattered_gradient.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual or skip connections are branches in the computational path, whereby the input\n",
    "to each network layer $f[\\bullet]$ is added back to the output. By analogy to, the residual network is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_1 &= \\mathbf{x} + f_1[\\mathbf{x}, \\phi_1] \\\\\n",
    "\\mathbf{h}_2 &= \\mathbf{h}_1 + f_2[\\mathbf{h}_1, \\phi_2] \\\\\n",
    "\\mathbf{h}_3 &= \\mathbf{h}_2 + f_3[\\mathbf{h}_2, \\phi_3] \\\\\n",
    "\\mathbf{y} &= \\mathbf{h}_3 + f_4[\\mathbf{h}_3, \\phi_4],\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the first term on the right-hand side of each line is the residual connection. Each\n",
    "function $f_k$ **learns an additive change** to the current representation. It follows that their\n",
    "outputs must be the same size as their inputs. Each additive combination of the input\n",
    "and the processed output is known as a residual block or residual layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the overall layout for residual connections:\n",
    "<img src=../images/residual_connection.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one interpretation is that residual connections turn the original network into an *ensemble* of\n",
    "these smaller networks whose outputs are summed to compute the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***He initialization in initializing network parameters:*** \n",
    "\n",
    "we initialize the network parameters so that the expected variance of the\n",
    "activations (in the forward pass) and gradients (in the backward pass) remains the same\n",
    "between layers. *He initialization* achieves this for ReLU activations by\n",
    "initializing the biases $\\beta$ to zero and choosing normally distributed weights $\\Omega$ with mean\n",
    "zero and variance $2/{D_h}$ where $D_h$ is the number of hidden units in the previous layer.\n",
    "\n",
    "Now consider a residual network. We do not have to worry about the intermediate\n",
    "values or gradients vanishing with network depth since there exists a path whereby\n",
    "each layer directly contributes to the network output.\n",
    "However, even if we use He initialization within the residual block, the values in the\n",
    "forward pass increase exponentially as we move through the network.\n",
    "To see why, consider that we add the result of the processing in the residual block back\n",
    "to the input. Each branch has some (uncorrelated) variability. Hence, the overall variance\n",
    "increases when we recombine them. With ReLU activations and He initialization, the\n",
    "expected variance is unchanged by the processing in each block. Consequently, when\n",
    "we recombine with the input, the variance doubles, growing exponentially\n",
    "with the number of residual blocks. This limits the possible network depth before floating\n",
    "point precision is exceeded in the forward pass. A similar argument applies to the\n",
    "gradients in the backward pass of the backpropagation algorithm.\n",
    "Hence, residual networks still suffer from unstable forward propagation and exploding\n",
    "gradients even with He initialization. One approach that would stabilize the forward and\n",
    "backward passes would be to use He initialization and then multiply the combined output\n",
    "of each residual block by $1/\\sqrt{2}$ to compensate for the doubling. However,\n",
    "it is more usual to use batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/BN_in_ResNet.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization or BatchNorm shifts and rescales each activation hso that its mean\n",
    "and variance across the batch Bbecome values that are learned during training. First,\n",
    "the empirical mean $m_h$ and standard deviation $s_h$ are computed:\n",
    "\n",
    "$$\n",
    "m_h = \\frac{1}{|B|} \\sum_{i \\in B} h_i\n",
    "$$\n",
    "$$\n",
    "s_h = \\sqrt{\\frac{1}{|B|} \\sum_{i \\in B} (h_i - m_h)^2}\n",
    "$$\n",
    "where all quantities are scalars. Then we use these statistics to *standardize* the batch activations to have mean zero and unit variance:\n",
    "$$\n",
    "h_i \\leftarrow \\frac{h_i - m_h}{s_h + \\epsilon} \\quad \\forall i \\in B\n",
    "$$\n",
    "where $\\epsilon$ is a small number that prevents division by zero if $h_i$ is the same for every\n",
    "member of the batch and $s_h = 0$.\n",
    "\n",
    "Finally, the normalized variable is scaled by $\\gamma$ and shifted by $\\delta$:\n",
    "$$\n",
    "h_i \\leftarrow \\gamma h_i + \\delta \\quad \\forall i \\in B\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this operation, the activations have mean $\\delta$ and standard deviation $\\gamma$ across all\n",
    "members of the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Batch normalization is applied independently to each hidden unit. In a standard\n",
    "neural network with K layers, each containing D hidden units, there would be KD\n",
    "learned offsets δ and KD learned scales γ. In a convolutional network, the normalizing\n",
    "statistics are computed over both the batch and the spatial position. If there were K\n",
    "layers, each containing C channels, there would be KC offsets and KC scales. At test\n",
    "time, we do not have a batch from which we can gather statistics. To resolve this, the\n",
    "statistics mh and sh are calculated across the whole training dataset (rather than just a\n",
    "batch) and frozen in the final network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch normalization makes the network invariant to rescaling the weights and biases that\n",
    "contribute to each activation; if these are doubled, then the activations also double, the\n",
    "estimated standard deviation $s_h$ doubles, and the normalization in equation above compensates for these changes. This happens separately for each hidden unit. Consequently,\n",
    "there will be a large family of weights and biases that all produce the same effect. Batch\n",
    "normalization also adds two parameters, γ and δ, at every hidden unit, which makes the\n",
    "model somewhat larger. Hence, it both creates redundancy in the weights and biases and\n",
    "adds extra parameters to compensate for that redundancy. This is obviously ineﬀicient,\n",
    "but batch normalization also provides several benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "benefits of BatchNorm:\n",
    "- Stable forward propagation\n",
    "- Higher learning rates\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual blocks were first used in convolutional networks for image classification. The\n",
    "resulting networks are known as residual networks, or ResNets for short. In ResNets, each\n",
    "residual block contains a batch normalization operation, a ReLU activation function, and\n",
    "a convolutional layer. This is followed by the same sequence again before being added back to the input. Trial and error have shown that this order of operations\n",
    "works well for image classification.\n",
    "For very deep networks, the number of parameters may become undesirably large.\n",
    "*Bottleneck residual blocks* make more eﬀicient use of parameters using three convolutions.\n",
    "The first has a 1×1 kernel and reduces the number of channels. The second is a regular\n",
    "3×3 kernel, and the third is another 1×1 kernel to increase the number of channels back\n",
    "to the original amount. In this way, we can integrate information over a\n",
    "3×3 pixel area using fewer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/bottleneck_ResNet_block.png width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **DenseNet** architecture uses concatenation so that the input to a layer comprises\n",
    "the concatenated outputs from all previous layers. These are processed to\n",
    "create a new representation that is itself concatenated with the previous representation\n",
    "and passed to the next layer. This concatenation means there is a direct contribution\n",
    "from earlier layers to the output, so the loss surface behaves reasonably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "DenseNet is built around the idea of \"dense connectivity,\" where each layer in the network receives inputs from all preceding layers and passes its own feature maps to all subsequent layers. Concatenation is the mechanism that facilitates this process. Specifically, at each layer, the feature maps produced by that layer are concatenated (combined) with the feature maps from all previous layers, rather than being summed or averaged as in other architectures like ResNet (Residual Networks). In contrast to ResNet, which uses addition to combine features (residual connections), DenseNet’s concatenation keeps all features intact rather than merging them. This distinction allows DenseNet to maintain a more diverse set of features, which can improve the network’s ability to learn complex patterns. concatenation in DenseNet is the key to its dense connectivity, enabling feature reuse, better gradient flow, and parameter efficiency, ultimately leading to a highly effective and compact deep learning architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further research:\n",
    "- shattered gradients??"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN (Residual nerual networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous notebook described how image classification performance improved as the\n",
    "depth of convolutional networks was extended from eight layers (AlexNet) to nineteen\n",
    "layers (VGG). This led to experimentation with even deeper networks. However, performance decreased again when many more layers were added.\n",
    "This notebook introduces *residual blocks*. Here, each network layer computes an *additive change* to the current representation instead of transforming it directly. This allows\n",
    "deeper networks to be trained but causes an exponential increase in the activation magnitudes at initialization. Residual blocks employ **batch normalization** to compensate for\n",
    "this, which re-centers and rescales the activations at each layer.\n",
    "Residual blocks with batch normalization allow much deeper networks to be trained,\n",
    "and these networks improve performance across a variety of tasks. Architectures that\n",
    "combine residual blocks to tackle image classification, medical image segmentation, and\n",
    "human pose estimation are described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every network we have seen so far processes the data sequentially; each layer receives\n",
    "the previous layerâ€™s output and passes the result to the next. For example,\n",
    "a three-layer network is defined by:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{h}_1 &= f_1[\\mathbf{x}, \\phi_1] \\\\\n",
    "\\mathbf{h}_2 &= f_2[\\mathbf{h}_1, \\phi_2] \\\\\n",
    "\\mathbf{h}_3 &= f_3[\\mathbf{h}_2, \\phi_3] \\\\\n",
    "\\mathbf{y} &= f_4[\\mathbf{h}_3, \\phi_4],\n",
    "\\end{align*}$$\n",
    "\n",
    "where $h_1, h_2, h_3$ denote the intermediate hidden layers, x is the network input, y\n",
    "is the output, and the functions $f_k[\\bullet,\\phi_k]$ perform the processing.\n",
    "In a standard neural network, each layer consists of a linear transformation followed\n",
    "by an activation function, and the parameters $\\phi_k$ comprise the weights and biases of the linear transformation. In a convolutional network, each layer consists of a set of convolutions followed by an activation function, *and the parameters comprise the convolutional kernels and biases.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shattered gradient:\n",
    "<img src=../images/shattered_gradient.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual or skip connections are branches in the computational path, whereby the input\n",
    "to each network layer $f[\\bullet]$ is added back to the output. By analogy to, the residual network is defined as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_1 &= \\mathbf{x} + f_1[\\mathbf{x}, \\phi_1] \\\\\n",
    "\\mathbf{h}_2 &= \\mathbf{h}_1 + f_2[\\mathbf{h}_1, \\phi_2] \\\\\n",
    "\\mathbf{h}_3 &= \\mathbf{h}_2 + f_3[\\mathbf{h}_2, \\phi_3] \\\\\n",
    "\\mathbf{y} &= \\mathbf{h}_3 + f_4[\\mathbf{h}_3, \\phi_4],\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the first term on the right-hand side of each line is the residual connection. Each\n",
    "function $f_k$ learns an additive change to the current representation. It follows that their\n",
    "outputs must be the same size as their inputs. Each additive combination of the input\n",
    "and the processed output is known as a residual block or residual layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the overall layout for residual connections:\n",
    "<img src=../images/residual_connection.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one interpretation is that residual connections turn the original network into an *ensemble* of\n",
    "these smaller networks whose outputs are summed to compute the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### further research:\n",
    "- shattered gradients??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

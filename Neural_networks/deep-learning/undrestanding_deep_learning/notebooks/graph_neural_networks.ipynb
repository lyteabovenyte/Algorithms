{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8fe4dc",
   "metadata": {},
   "source": [
    "#### Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b710f9e",
   "metadata": {},
   "source": [
    "the paper called [Graph Signal Processing for Geometric Data and\n",
    "Beyond: Theory and Applications](https://arxiv.org/pdf/2008.01918) worth noting in this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63ce1de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "- Graph domains:\n",
    "\n",
    "<img src=../images/graph_domain.png width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2556144",
   "metadata": {},
   "source": [
    "the concept of node embedding and edge embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f6f4d8",
   "metadata": {},
   "source": [
    "Adjacency Matrix($A$), Node embedding Matrix ($X$) and Edge embedding Matrix($E$):\n",
    "\n",
    "- the properties of *Adjacency Matrix*:\n",
    "    - In general, if we raise the adjacency matrix to the power of $L$, the entry at position $(m,n)$ of $A^L$ contains the number of unique walks of length $L$ from node $m$ to node $n$, by the way sometimes This is not the same as the number of unique paths since the walks include routes that visit the same node more than once. Nonetheless, $A^L$ still contains valuable information about the graph connectivity; a non-zero entry at position $(m,n)$ indicates that the distance from $m$ to $n$ must be less than or equal to $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10de47b",
   "metadata": {},
   "source": [
    "permutation of node indices in graphs -> in contrast with images where the permutation of the pixels results in changing the whole images and hence results in completely new images\n",
    "\n",
    "The operation of exchanging node indices can be expressed mathematically by a permutation matrix, $P$. This is a matrix where exactly one entry in each row and\n",
    "column take the value one, and the remaining values are zero. When position $(m,n)$ of the permutation matrix is set to one, it indicates that node $m$ will become node $n$ after the permutation. To map from one indexing to another, we use the operations:\n",
    "\n",
    "$$\n",
    "X' = XP\n",
    "$$\n",
    "$$\n",
    "A' = P^T AP,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2fa01",
   "metadata": {},
   "source": [
    "where post-multiplying by $P$ permutes the columns and pre-multiplying by $P_T$ permutes\n",
    "the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431686d9-2887-40c3-a129-737b6d512926",
   "metadata": {},
   "source": [
    "#### Graph neural networks, input, output, tasks and loss-function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8215f9f3-96f8-4bfc-a69b-bbb2eb76c5bf",
   "metadata": {},
   "source": [
    "A graph neural network is a model that takes the node embeddings $X$ and the adjacency\n",
    "matrix $A$ as inputs and passes them through a series of $K$ layers. The node embeddings\n",
    "are updated at each layer to create intermediate “hidden” representations $H_k$ before\n",
    "finally computing output embeddings $H_K$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50379202-d287-44d3-a659-78ef42b8e34b",
   "metadata": {},
   "source": [
    "$?$ similarity with word-embed and transformers architecture:\n",
    "\n",
    "At the start of this network, each column of the input node embeddings $X$ just contains information about the node itself. At the end, each column of the model output $H_K$ includes information about the node and its context within the graph. This is similar to\n",
    "word embeddings passing through a transformer network. These represent words at the\n",
    "start, but represent the word meanings in the context of the sentence at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c303481f-fcb9-40a3-89e8-4a6df2ebe1fe",
   "metadata": {},
   "source": [
    "Supervised graph problems usually fall into one of three categories:\n",
    "\n",
    "1. Graph-level tasks: The network assigns a label or estimates one or more values from\n",
    "the entire graph, exploiting both the structure and node embeddings. For example, we\n",
    "might want to predict the temperature at which a molecule becomes liquid (a regression\n",
    "task) or whether a molecule is poisonous to human beings or not (a classification task).\n",
    "For graph-level tasks, the output node embeddings are combined (e.g., by averaging),\n",
    "and the resulting vector is mapped via a linear transformation or neural network to a\n",
    "fixed-size vector. For regression, the mismatch between the result and the ground truth\n",
    "values is computed using the least squares loss. For binary classification, the output\n",
    "is passed through a sigmoid function, and the mismatch is calculated using the binary\n",
    "cross-entropy loss. Here, the probability that the graph belongs to class one might be\n",
    "given by:\n",
    "$$\n",
    "\\Pr(y = 1 | X, A) = \\text{sig}[\\beta_K + \\omega_K H_K 1/N],\n",
    "$$\n",
    "\n",
    "where the scalar $\\beta_k$ and $1 \\times D$ vector $\\omega_k$ are learned parameters. Post-multiplying the\n",
    "output embedding matrix $H_K$ by the column vector $1$ that contains ones has the effect\n",
    "of summing together all the embeddings and subsequently dividing by the number of\n",
    "nodes $N$ computes the average. This is known as *mean pooling*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e640c4-da20-454e-a7e6-e321508c5c67",
   "metadata": {},
   "source": [
    "2. Node-level tasks: The network assigns a label (classification) or one or more values\n",
    "(regression) to each node of the graph, using both the graph structure and node embeddings. For example, given a graph constructed from a 3D point cloud, the goal might be to classify the nodes according to whether they belong\n",
    "to the wings or fuselage. Loss functions are defined in the same way as for graph-level\n",
    "tasks, except that now this is done independently at each node $n$:\n",
    "$$\n",
    "\\operatorname{Pr}(y^{(n)} = 1 | \\mathbf{X}, \\mathbf{A}) = \\operatorname{sig} \\left[ \\beta \\mathbf{k} + \\mathbf{w} \\mathbf{h}_K^{(n)} \\right]. \\\\\n",
    "$$\n",
    "\n",
    "3. Edge prediction tasks: The network predicts whether or not there should be an edge\n",
    "between nodes n and m. For example, in the social network setting, the network might\n",
    "predict whether two people know and like each other and suggest that they connect if\n",
    "that is the case. This is a binary classification task where the two node embeddings must\n",
    "be mapped to a single number representing the probability that the edge is present. One\n",
    "possibility is to take the dot product of the node embeddings and pass the result through\n",
    "a sigmoid function to create the probability:\n",
    "\n",
    "$$\\operatorname{Pr}(y^{(m)} = 1 | \\mathbf{X}, \\mathbf{A}) = \\operatorname{sig} \\left[ \\mathbf{h}^{(m) \\top} \\mathbf{h}^{(n)} \\right].$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1697f",
   "metadata": {},
   "source": [
    "### Graph convolutional network:\n",
    "\n",
    "#### spatial-based convolutional graph neural networks, or GCNs for short:\n",
    "These models are convolutional\n",
    "in that they update each node by aggregating information from nearby nodes. As such,\n",
    "they induce a *relational inductive bias* (i.e., a bias toward prioritizing information from\n",
    "neighbors). They are **spatial-based** because they use the original graph structure. This\n",
    "contrasts with **spectral-based** methods, which apply convolutions in the Fourier domain.\n",
    "\n",
    "Each layer of the GCN is a function $F[\\bullet]$ with parameters $\\phi$ that takes the node\n",
    "embeddings and adjacency matrix and outputs new node embeddings. The network can\n",
    "hence be written as:\n",
    "$$\n",
    "\\mathbf{H}_1 = \\mathbf{F}[\\mathbf{X}, \\mathbf{A}, \\phi_0] \\\\\n",
    "\n",
    "\\mathbf{H}_2 = \\mathbf{F}[\\mathbf{H}_1, \\mathbf{A}, \\phi_1] \\\\\n",
    "\n",
    "\\mathbf{H}_3 = \\mathbf{F}[\\mathbf{H}_2, \\mathbf{A}, \\phi_2] \\\\\n",
    "\n",
    "\\dots \\\\\n",
    "\n",
    "\\mathbf{H}_K = \\mathbf{F}[\\mathbf{H}_{K-1}, \\mathbf{A}, \\phi_{K-1}]\n",
    "$$\n",
    "\n",
    "where X is the input, A is the adjacency matrix, Hk contains the modified node embeddings at the kth layer, and ϕk denotes the parameters that map from layer k to\n",
    "layer k+1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e39c4",
   "metadata": {},
   "source": [
    "Equivariance and invariance to permutations:\n",
    "\n",
    "\n",
    "$$ \\mathbf{H}_{k+1}\\mathbf{P} = \\mathbf{F}[\\mathbf{H}_k\\mathbf{P}, \\mathbf{P}^T\\mathbf{AP}, \\phi_k]. $$\n",
    "\n",
    "$$ y = \\text{sig}\\left[\\beta_k + \\omega_k \\mathbf{H}_k 1 / N\\right] = \\text{sig}\\left[\\beta_k + \\omega_k \\mathbf{H}_k \\mathbf{P} 1 / N\\right], $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2184e63",
   "metadata": {},
   "source": [
    "- simple graph CNN Layer:\n",
    "\n",
    "<img src=../images/simple_graph_CNN.png width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafcc1d6",
   "metadata": {},
   "source": [
    "Example of simple GCN Layer:\n",
    "\n",
    "$$ \\text{agg}[n, k] = \\sum_{m \\in \\text{NE}[n]} \\mathbf{h}_k^{(m)}, $$\n",
    "\n",
    "$$ \\mathbf{h}_{k+1}^{(n)} = \\text{a} \\left[ \\beta_k + \\Omega_k \\cdot \\mathbf{h}_k^{(n)} + \\Omega_k \\cdot \\text{agg}[n, k] \\right]. $$\n",
    "\n",
    "We can write this more succinctly by noting that post-multiplication of a matrix\n",
    "by a vector returns a weighted sum of its columns. The $n^th$ column of the adjacency\n",
    "matrix $A$ contains ones at the positions of the neighbors. Hence, if we collect the node embeddings into the $D \\times N$ matrix $H_k$ and post-multiply by the adjacency matrix $A$, the nth column of the result is $agg[n,k]$. The update for the nodes is now:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{H}_{k+1} &= \\text{a} \\left[ \\beta_k \\mathbf{1}^T + \\Omega_k \\mathbf{H}_k + \\Omega_k \\mathbf{H}_k \\mathbf{A} \\right] \\\\\n",
    "&= \\text{a} \\left[ \\beta_k \\mathbf{1}^T + \\Omega_k \\mathbf{H}_k (\\mathbf{A} + \\mathbf{I}) \\right],\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "so, This layer satisfies the design considerations: it is equivariant to permutations of the node indices, can cope with any number of neighbors, exploits the graph structure to provide a relational inductive bias, and shares parameters throughout the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e36dd9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### 1. graph classification:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbf{H}_1 &= a\\left[ \\boldsymbol{\\beta}_0 \\mathbf{1}^\\top + \\boldsymbol{\\Omega}_0 \\mathbf{X} (\\mathbf{A} + \\mathbf{I}) \\right] \\\\\n",
    "\\mathbf{H}_2 &= a\\left[ \\boldsymbol{\\beta}_1 \\mathbf{1}^\\top + \\boldsymbol{\\Omega}_1 \\mathbf{H}_1 (\\mathbf{A} + \\mathbf{I}) \\right] \\\\\n",
    "&\\vdots \\\\\n",
    "\\mathbf{H}_K &= a\\left[ \\boldsymbol{\\beta}_{K-1} \\mathbf{1}^\\top + \\boldsymbol{\\Omega}_{K-1} \\mathbf{H}_{K-1} (\\mathbf{A} + \\mathbf{I}) \\right] \\\\\n",
    "f[\\mathbf{X}, \\mathbf{A}, \\boldsymbol{\\Phi}] &= \\sigma\\left( \\boldsymbol{\\beta}_K + \\boldsymbol{\\omega}_K \\mathbf{H}_K \\frac{\\mathbf{1}}{N} \\right)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15cfc49",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "$\\mathbf{H}_{k-1} (\\mathbf{A} + \\mathbf{I})$ performs a message aggregation step across the graph:\n",
    "\n",
    "$$\\left[\\mathbf{H}{k-1} (\\mathbf{A} + \\mathbf{I})\\right]{:, v} = \\sum_{u \\in \\mathcal{N}(v) \\cup \\{v\\}} \\mathbf{h}_{u}^{(k-1)}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{H}_{k-1} \\in \\mathbb{R}^{D \\times N}$: each column $\\mathbf{h}_v$ is the embedding of node $v$ at layer $k-1$\n",
    "- $\\mathbf{A} + \\mathbf{I} \\in \\mathbb{R}^{N \\times N}$: adjacency matrix with self-loops.\n",
    "\n",
    "So the new embedding of node $v$ is computed by summing up (or linearly combining) the embeddings of its neighbors $\\mathcal{N}(v)$, plus itself (because of $\\mathbf{I}$).\n",
    "\n",
    "\n",
    "to further undrestand the concept, think of this in isolation and without the Identity function($I$), the equation is something like:\n",
    "$$\n",
    "(\\mathbf{A}){uv} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if node } u \\text{ is connected to } v \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "\\Rightarrow \\left[\\mathbf{H}{k-1} \\mathbf{A}\\right]{:, v} = \\sum{u \\in \\mathcal{N}(v)} \\mathbf{h}^{(k-1)}_u\n",
    "$$\n",
    "\n",
    "so the node $v$ is aggregating it's neighbors embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c0a481",
   "metadata": {},
   "source": [
    "##### Inductive and Tranductive:\n",
    "\n",
    "wikipedia: In logic, statistical inference, and supervised learning, *transduction* or transductive inference is reasoning from observed, specific cases to specific cases. In contrast, *induction* is reasoning from observed training cases to general rules, which are then applied to the test cases. The distinction is most interesting in cases where the predictions of the transductive model are not achievable by any inductive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1e5d9",
   "metadata": {},
   "source": [
    "**inductive**: we exploid a training\n",
    "set of labeled data to learn the relation between the inputs and outputs. Then we apply this to new test data. One way to think of this is that we are learning the rule that maps inputs to outputs and then applying it elsewhere.\n",
    "\n",
    "**transductive**: in contrast, the transductive model considers both labeled and unlabeled data at the same time. It does not produce a rule but merely a labeling for the unknown outputs. This is sometimes termed *semi-supervised* learning. It has the advantage that it can use patterns in the unlabeled data to help make its decisions. However, it has the disadvantage that the model needs to be retrained when extra unlabeled data are added.\n",
    "\n",
    "Both problem types are commonly encountered for graphs. Sometimes,\n",
    "we have many labeled graphs and learn a mapping between the graph and the labels.\n",
    "For example, we might have many molecules, each labeled according to whether it is\n",
    "toxic to humans. We learn the rule that maps the graph to the toxic/non-toxic label and\n",
    "then apply this rule to new molecules. However, sometimes there is a single monolithic\n",
    "graph. In the graph of scientific paper citations, we might have labels indicating the field\n",
    "(physics, biology, etc.) for some nodes and wish to label the remaining nodes. Here, the\n",
    "training and test data are irrevocably connected. Graph-level tasks only occur in the inductive setting where there are training and test\n",
    "graphs. However, node-level tasks and edge prediction tasks can occur in either setting.\n",
    "In the transductive case, the loss function minimizes the mismatch between the model\n",
    "output and the ground truth where this is known. New predictions are computed by\n",
    "running the forward pass and retrieving the results where the ground truth is unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d882c8cc",
   "metadata": {},
   "source": [
    "##### 2. Node classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25361b4c",
   "metadata": {},
   "source": [
    "in this category, Training this network raises two problems. First, it is logistically diﬀicult to train a\n",
    "graph neural network of this size. Consider that we must store the node embeddings at\n",
    "every network layer in the forward pass. This will involve both storing and processing\n",
    "a structure several times the size of the entire graph, and this may not be practical.\n",
    "Second, we have only a single graph, so it’s not obvious how to perform stochastic\n",
    "gradient descent. How can we form a batch if there is only a single object?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dee272",
   "metadata": {},
   "source": [
    "**choosing a batch**:\n",
    "\n",
    "One way to form a batch is to choose a random subset of labeled nodes at each training\n",
    "step. Each node depends on its neighbors in the previous layer. These, in turn, depend on their neighbors in the layer before, so (similarly to convolutional networks) each node has a receptive field. The receptive field size is termed the *k-hop neighborhood*.\n",
    "We can hence perform a gradient descent step using the graph that forms the union of\n",
    "the k-hop neighborhoods of the batch nodes; the remaining inputs do not contribute. Unfortunately, if there are many layers and the graph is densely connected, every\n",
    "input node may be in the receptive field of every output, and this may not reduce the\n",
    "graph size at all. This is known as the *graph expansion problem*. Two approaches that\n",
    "tackle this problem are neighborhood sampling and graph partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f9d547",
   "metadata": {},
   "source": [
    "##### Some notes on the [GCN from Microsoft](https://youtu.be/zCEYiCxrL_0?si=7cE5t0IahAtWSCnF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c257cf",
   "metadata": {},
   "source": [
    "- the process of updating nodes:\n",
    "\n",
    "<img src=../images/microsoft_GCN1.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9ad66a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

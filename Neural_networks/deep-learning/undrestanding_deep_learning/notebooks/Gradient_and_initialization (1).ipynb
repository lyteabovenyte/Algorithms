{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gradients and initialization"
      ],
      "metadata": {
        "id": "H-x8xOk13msF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first part of this notebook describes the ***backpropagation algorithm***, which\n",
        "computes these derivatives eﬀiciently. In the second part of the notebooks, we consider how to initialize the network parameters\n",
        "before we commence training. We describe methods to choose the initial weights $\\Omega_k$ and\n",
        "biases $\\beta_k$ so that training is stable."
      ],
      "metadata": {
        "id": "FbBQc9lO4Ff7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization algorithms**:\n",
        "The derivatives of the loss tell us how the loss changes when we make a small change\n",
        "to the parameters. Optimization algorithms exploit this information to manipulate the\n",
        "parameters so that the loss becomes smaller"
      ],
      "metadata": {
        "id": "D5SR0XUM-EMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 1**: Each weight (element of $\\Omega_k$ ) multiplies the activation at a source hidden\n",
        "unit and adds the result to a destination hidden unit in the next layer. It follows that the\n",
        "effect of any small change to the weight is amplified or attenuated by the activation at\n",
        "the source hidden unit. Hence, we run the network for each data example in the batch\n",
        "and store the activations of all the hidden units. This is known as the forward pass. The stored activations will subsequently be used to compute the gradients.\n",
        "\n",
        "In the context of training a deep learning model, \"storing the activations\" refers to keeping track of the intermediate outputs (or activations) of the hidden units (neurons) in a neural network during the forward pass.\n",
        "\n",
        "The forward pass is the process of feeding an input (like a data example from your batch) through the network, layer by layer, to compute the final output (e.g., a prediction). As the data moves through each layer, every hidden unit calculates its activation based on the inputs it receives from the previous layer.\n",
        "\n",
        "How It Works in Practice:\n",
        "1. Forward Pass: For each data example in the batch, the network computes the activations of all hidden units, layer by layer, and stores them (e.g., in memory).\n",
        "2. Backward Pass: Using these stored activations, the network calculates the gradients of the loss function with respect to each weight. This involves combining the stored activations with the error signals propagated backward through the network.\n",
        "3. Weight Update: The gradients are then used to adjust the weights (e.g., via gradient descent) to reduce the error."
      ],
      "metadata": {
        "id": "MdavB4Uu-djq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation 2**:\n",
        "A small change in a bias or weight causes a ripple effect of changes\n",
        "through the subsequent network. The change modifies the value of its destination hidden."
      ],
      "metadata": {
        "id": "cItPNaqo_fP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we move backward through the network, we see that most of the terms we need\n",
        "were already calculated in the previous step, so we do not need to re-compute them.\n",
        "Proceeding backward through the network in this way to compute the derivatives is\n",
        "known as the **backward pass**."
      ],
      "metadata": {
        "id": "wQnzRIDPDg6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The backpropagation algorithm is an eﬀicient method for computing all of these\n",
        "derivatives at once. It consists of ($i$) a forward pass, in which we compute and store a\n",
        "series of intermediate values and the network output, and ($ii$) a backward pass, in which we calculate the derivatives of each parameter, starting at the end of the network, and\n",
        "reusing previous calculations as we move toward the start."
      ],
      "metadata": {
        "id": "xK4URvtBLVPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the whole process of backward pass and applying chain rule:\n",
        "<br>\n",
        "<br>\n",
        "$$\\frac{\\partial l_i}{\\partial \\beta_k} = \\frac{\\partial f_k}{\\partial \\beta_k} \\frac{\\partial l_i}{\\partial f_k}$$\n",
        "<br>\n",
        "$$\\frac{\\partial l_i}{\\partial w_k} = \\frac{\\partial f_k}{\\partial w_k} \\frac{\\partial l_i}{\\partial f_k}$$"
      ],
      "metadata": {
        "id": "Khmj0hfldEFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assume we have a neural net with 3 layers as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathbf{f}_0 &= \\beta_0 + \\Omega_0 \\mathbf{x}_i, \\\\\n",
        "\\mathbf{h}_1 &= \\mathbf{a}[\\mathbf{f}_0], \\\\\n",
        "\\mathbf{f}_1 &= \\beta_1 + \\Omega_1 \\mathbf{h}_1, \\\\\n",
        "\\mathbf{h}_2 &= \\mathbf{a}[\\mathbf{f}_1], \\\\\n",
        "\\mathbf{f}_2 &= \\beta_2 + \\Omega_2 \\mathbf{h}_2, \\\\\n",
        "\\mathbf{h}_3 &= \\mathbf{a}[\\mathbf{f}_2], \\\\\n",
        "\\mathbf{f}_3 &= \\beta_3 + \\Omega_3 \\mathbf{h}_3, \\\\\n",
        "\\ell_i &= \\mathbf{l}[\\mathbf{f}_3, \\mathbf{y}_i],\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "DQVJBv__eu1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "just for reference:\n",
        "$$\\begin{align*}\n",
        "\\frac{\\partial \\ell_i}{\\partial f_2} &= \\frac{\\partial h_3}{\\partial f_2} \\left( \\frac{\\partial \\ell_i}{\\partial f_3} \\right) \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial h_2} &= \\frac{\\partial f_2}{\\partial h_2} \\left( \\frac{\\partial \\ell_i}{\\partial f_2} \\frac{\\partial f_2}{\\partial h_3} \\frac{\\partial \\ell_i}{\\partial f_3} \\right) \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial f_1} &= \\frac{\\partial h_2}{\\partial f_1} \\left( \\frac{\\partial \\ell_i}{\\partial h_2} \\frac{\\partial f_2}{\\partial h_2} \\frac{\\partial \\ell_i}{\\partial f_2} \\frac{\\partial f_2}{\\partial h_3} \\frac{\\partial \\ell_i}{\\partial f_3} \\right) \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial h_1} &= \\frac{\\partial f_1}{\\partial h_1} \\left( \\frac{\\partial \\ell_i}{\\partial f_1} \\frac{\\partial h_2}{\\partial f_1} \\frac{\\partial \\ell_i}{\\partial h_2} \\frac{\\partial f_2}{\\partial h_2} \\frac{\\partial \\ell_i}{\\partial f_2} \\frac{\\partial f_2}{\\partial h_3} \\frac{\\partial \\ell_i}{\\partial f_3} \\right) \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial f_0} &= \\frac{\\partial h_1}{\\partial f_0} \\left( \\frac{\\partial \\ell_i}{\\partial h_1} \\frac{\\partial f_1}{\\partial h_1} \\frac{\\partial \\ell_i}{\\partial f_1} \\frac{\\partial h_2}{\\partial f_1} \\frac{\\partial \\ell_i}{\\partial h_2} \\frac{\\partial f_2}{\\partial h_2} \\frac{\\partial \\ell_i}{\\partial f_2} \\frac{\\partial f_2}{\\partial h_3} \\frac{\\partial \\ell_i}{\\partial f_3} \\right)\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "JT0a7NRf-e76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in our previous declared neural-nets:\n",
        "The derivative $\\frac{\\partial h_3}{\\partial f_2}$ of the output $h_3$ of the activation function with respect to\n",
        "its input $f_2$ will depend on the activation function. It will be a ***diagonal matrix***\n",
        "since each activation only depends on the corresponding pre-activation. For ReLU\n",
        "functions, the diagonal terms are zero everywhere $f_2$ is less than zero and one otherwise. Rather than multiply by this matrix, we extract the diagonal\n",
        "terms as a vector $I[f_2 \\gt 0]$ and pointwise multiply, which is more eﬀicient.\n",
        "As\n",
        "we progress back through the network, we alternately (i) multiply by the transpose of\n",
        "the weight matrices $\\Omega_{k}^T$ and (ii) threshold based on the inputs $f_k −1$ to the hidden layer.\n",
        "These inputs were stored during the forward pass."
      ],
      "metadata": {
        "id": "abwcPckJRXvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\\n",
        "\\frac{\\partial \\mathbf{f}_3}{\\partial \\mathbf{h}_3} = \\frac{\\partial}{\\partial \\mathbf{h}_3} \\left( \\beta_3 + \\Omega_3 \\mathbf{h}_3 \\right) = \\Omega_3^T.\n",
        "$$"
      ],
      "metadata": {
        "id": "Hnx4TR50ivvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating the loss with respect to the Biases($\\beta$) and weights($\\Omega$):\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\ell_i}{\\partial \\beta_k} &= \\frac{\\partial \\mathbf{f}_k}{\\partial \\beta_k} \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} \\\\\n",
        "&= \\frac{\\partial}{\\partial \\beta_k} \\left( \\beta_k + \\Omega_k \\mathbf{h}_k \\right) \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} \\\\\n",
        "&= \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k}, \\\\[2ex]\n",
        "\\frac{\\partial \\ell_i}{\\partial \\Omega_k} &= \\frac{\\partial \\mathbf{f}_k}{\\partial \\Omega_k} \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} \\\\\n",
        "&= \\frac{\\partial}{\\partial \\Omega_k} \\left( \\beta_k + \\Omega_k \\mathbf{h}_k \\right) \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} \\\\\n",
        "&= \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} \\mathbf{h}_k^T.\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "X_rSnFy4j5Om"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "summerization of backpropagation algorithm:\n",
        "Consider a deep neural\n",
        "network $f[x_i,\\phi]$ that takes input $x_i$, has $K$ hidden layers with ReLU activations, and\n",
        "individual loss term $l_i = l[f[x_i,\\phi],yi]$. The goal of backpropagation is to compute the\n",
        "derivatives $\\frac{\\partial l_i}{\\partial \\beta_k}$ and $\\frac{\\partial l_i}{\\partial \\Omega_k}$ with respect to the biases $\\beta_k$ and weights $\\Omega_k$"
      ],
      "metadata": {
        "id": "1pwPzEWjlOM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward pass equations:\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\frac{\\partial \\ell_i}{\\partial \\beta_k} &= \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} & k \\in \\{ K, K-1, \\ldots, 1 \\} \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial \\Omega_k} &= \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} \\mathbf{h}_k^T & k \\in \\{ K, K-1, \\ldots, 1 \\} \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_{k-1}} &= I[\\mathbf{f}_{k-1} > 0] \\odot \\left( \\Omega_k^T \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_k} \\right), & k \\in \\{ K, K-1, \\ldots, 1 \\} \\\\[2ex]\n",
        "\\frac{\\partial \\ell_i}{\\partial \\beta_0} &= \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_0} \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial \\Omega_0} &= \\frac{\\partial \\ell_i}{\\partial \\mathbf{f}_0} \\mathbf{x}_i^T.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "where $\\odot$ denotes pointwise multiplication, and $I[f_k −1 > 0]$ is a vector containing ones\n",
        "where $f_k −1$ is greater than zero and zeros elsewhere.\n",
        "<br>\n",
        "We calculate these derivatives for every training example in the batch and sum them\n",
        "together to retrieve the gradient for the SGD update"
      ],
      "metadata": {
        "id": "nZ2Jfo8YvE1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the backpropagation algorithm is extremely eﬀicient; the most demanding\n",
        "computational step in both the forward and backward pass is matrix multiplication (by $\\Omega$\n",
        "and $\\Omega^T$ , respectively) which only requires additions and multiplications. However, it is\n",
        "not memory eﬀicient; the intermediate values in the forward pass must all be stored, and\n",
        "this can limit the size of the model we can train."
      ],
      "metadata": {
        "id": "47Lye_dMv9UU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter initialization:\n",
        "#### ( vanishing gradient problem, exploding gradient problem )\n"
      ],
      "metadata": {
        "id": "-HX4V34I06kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\begin{align*}\n",
        "\\mathbb{E}[f'_i] &= \\mathbb{E} \\left[ \\beta_i + \\sum_{j=1}^{D_h} \\Omega_{i,j} h_j \\right] \\\\\n",
        "&= \\mathbb{E}[\\beta_i] + \\sum_{j=1}^{D_h} \\mathbb{E}[\\Omega_{i,j} h_j] \\\\\n",
        "&= \\mathbb{E}[\\beta_i] + \\sum_{j=1}^{D_h} \\mathbb{E}[\\Omega_{i,j}] \\mathbb{E}[h_j] \\\\\n",
        "&= 0 + \\sum_{j=1}^{D_h} 0 \\cdot \\mathbb{E}[h_j] = 0, \\\\\n",
        "\\sigma^2_{f'_i} &= \\mathbb{E}[f'^2_i] - \\mathbb{E}[f'_i]^2 \\\\\n",
        "&= \\mathbb{E} \\left[ \\left( \\beta_i + \\sum_{j=1}^{D_h} \\Omega_{i,j} h_j \\right)^2 \\right] - 0 \\\\\n",
        "&= \\mathbb{E} \\left[ \\left( \\sum_{j=1}^{D_h} \\Omega_{i,j} h_j \\right)^2 \\right] \\\\\n",
        "&= \\sum_{j=1}^{D_h} \\mathbb{E}[\\Omega^2_{i,j}] \\mathbb{E}[h^2_j] \\\\\n",
        "&= \\sum_{j=1}^{D_h} \\sigma^2_\\Omega \\mathbb{E}[h^2_j] = \\sigma^2_\\Omega \\sum_{j=1}^{D_h} \\mathbb{E}[h^2_j],\n",
        "\\end{align*}$$"
      ],
      "metadata": {
        "id": "XOnG8U-l8qu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "and finally with the ReLU activation function (which clips the values less that zero), we get:\n",
        "$$\\sigma^2_{f'_i} = \\sigma^2_\\Omega \\sum_{j=1}^{D_h} \\frac{\\sigma^2_{f_j}}{2} = \\frac{1}{2} D_h \\sigma^2_\\Omega \\sigma^2_f.$$"
      ],
      "metadata": {
        "id": "3p4q4_kRqW4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This, in turn, implies that if we want the variance $\\sigma_{f'}^2$ of the subsequent pre-activations $f'$\n",
        "to be the same as the variance $\\sigma_f^2$ of the original pre-activations $f$ during the forward\n",
        "pass, we should set:\n",
        "$$\\sigma^2_\\Omega = \\frac{2}{D_h}$$\n",
        "\n",
        "where $D_h$ is the dimension of the original layer to which the weights were applied. This\n",
        "is known as ***He initialization***"
      ],
      "metadata": {
        "id": "mdYeuXxUsYGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "but what if the hidden units in consequitive layers differs:\n",
        "$$\\sigma_\\Omega^2 = \\frac{4}{D_h + D_{h'}}$$"
      ],
      "metadata": {
        "id": "N4rE2SKMtIxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# define input size, hidden layer size, output size\n",
        "D_i, D_k, D_o = 10, 40, 5\n",
        "\n",
        "# create model with two hidden layers\n",
        "model = nn.Sequential(\n",
        "  nn.Linear(D_i, D_k),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(D_k, D_k),\n",
        "  nn.ReLU(),\n",
        "  nn.Linear(D_k, D_o))\n",
        "\n",
        "# He initialization of weights\n",
        "def weights_init(layer_in):\n",
        "  if isinstance(layer_in, nn.Linear):\n",
        "    nn.init.kaiming_normal_(layer_in.weight)\n",
        "    layer_in.bias.data.fill_(0.0)\n",
        "\n",
        "model.apply(weights_init)\n",
        "\n",
        "# choose least squares loss function\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# construct SGD optimizer and initialize learning rate and momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1, momentum=0.9)\n",
        "\n",
        "# object that decreases learning rate by half every 10 epochs\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# create 100 random data points and store in data loader class\n",
        "x = torch.randn(100, D_i)\n",
        "y = torch.randn(100, D_o)\n",
        "data_loader = DataLoader(TensorDataset(x,y), batch_size=10, shuffle=True)\n",
        "\n",
        "# loop over the dataset 100 times\n",
        "for epoch in range(100):\n",
        "  epoch_loss = 0.0\n",
        "  # loop over batches\n",
        "  for i, data in enumerate(data_loader):\n",
        "    # retrieve inputs and labels for this batch\n",
        "    x_batch, y_batch = data\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    # forward pass\n",
        "    pred = model(x_batch)\n",
        "    loss = criterion(pred, y_batch)\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    # SGD update\n",
        "    optimizer.step()\n",
        "    # update statistics\n",
        "    epoch_loss += loss.item()\n",
        "  # print error\n",
        "  print(f'Epoch {epoch:5d}, loss {epoch_loss:.3f}')\n",
        "  # tell scheduler to consider updating learning rate\n",
        "  scheduler.step()"
      ],
      "metadata": {
        "id": "s5t9_33T0PmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d142db84-29d3-4c05-caac-b9295e8a9cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch     0, loss 14.346\n",
            "Epoch     1, loss 8.229\n",
            "Epoch     2, loss 7.697\n",
            "Epoch     3, loss 6.941\n",
            "Epoch     4, loss 6.468\n",
            "Epoch     5, loss 6.067\n",
            "Epoch     6, loss 5.638\n",
            "Epoch     7, loss 5.403\n",
            "Epoch     8, loss 5.219\n",
            "Epoch     9, loss 4.992\n",
            "Epoch    10, loss 4.684\n",
            "Epoch    11, loss 4.106\n",
            "Epoch    12, loss 3.780\n",
            "Epoch    13, loss 3.473\n",
            "Epoch    14, loss 3.313\n",
            "Epoch    15, loss 3.091\n",
            "Epoch    16, loss 2.969\n",
            "Epoch    17, loss 2.906\n",
            "Epoch    18, loss 2.790\n",
            "Epoch    19, loss 2.765\n",
            "Epoch    20, loss 2.680\n",
            "Epoch    21, loss 2.511\n",
            "Epoch    22, loss 2.476\n",
            "Epoch    23, loss 2.416\n",
            "Epoch    24, loss 2.373\n",
            "Epoch    25, loss 2.344\n",
            "Epoch    26, loss 2.299\n",
            "Epoch    27, loss 2.291\n",
            "Epoch    28, loss 2.253\n",
            "Epoch    29, loss 2.222\n",
            "Epoch    30, loss 2.184\n",
            "Epoch    31, loss 2.159\n",
            "Epoch    32, loss 2.145\n",
            "Epoch    33, loss 2.129\n",
            "Epoch    34, loss 2.112\n",
            "Epoch    35, loss 2.108\n",
            "Epoch    36, loss 2.098\n",
            "Epoch    37, loss 2.083\n",
            "Epoch    38, loss 2.076\n",
            "Epoch    39, loss 2.067\n",
            "Epoch    40, loss 2.045\n",
            "Epoch    41, loss 2.041\n",
            "Epoch    42, loss 2.034\n",
            "Epoch    43, loss 2.030\n",
            "Epoch    44, loss 2.025\n",
            "Epoch    45, loss 2.019\n",
            "Epoch    46, loss 2.013\n",
            "Epoch    47, loss 2.008\n",
            "Epoch    48, loss 2.001\n",
            "Epoch    49, loss 1.995\n",
            "Epoch    50, loss 1.984\n",
            "Epoch    51, loss 1.981\n",
            "Epoch    52, loss 1.978\n",
            "Epoch    53, loss 1.975\n",
            "Epoch    54, loss 1.972\n",
            "Epoch    55, loss 1.971\n",
            "Epoch    56, loss 1.968\n",
            "Epoch    57, loss 1.965\n",
            "Epoch    58, loss 1.963\n",
            "Epoch    59, loss 1.962\n",
            "Epoch    60, loss 1.957\n",
            "Epoch    61, loss 1.956\n",
            "Epoch    62, loss 1.954\n",
            "Epoch    63, loss 1.953\n",
            "Epoch    64, loss 1.952\n",
            "Epoch    65, loss 1.951\n",
            "Epoch    66, loss 1.950\n",
            "Epoch    67, loss 1.948\n",
            "Epoch    68, loss 1.948\n",
            "Epoch    69, loss 1.946\n",
            "Epoch    70, loss 1.944\n",
            "Epoch    71, loss 1.943\n",
            "Epoch    72, loss 1.942\n",
            "Epoch    73, loss 1.942\n",
            "Epoch    74, loss 1.941\n",
            "Epoch    75, loss 1.941\n",
            "Epoch    76, loss 1.940\n",
            "Epoch    77, loss 1.939\n",
            "Epoch    78, loss 1.939\n",
            "Epoch    79, loss 1.938\n",
            "Epoch    80, loss 1.937\n",
            "Epoch    81, loss 1.937\n",
            "Epoch    82, loss 1.937\n",
            "Epoch    83, loss 1.936\n",
            "Epoch    84, loss 1.936\n",
            "Epoch    85, loss 1.936\n",
            "Epoch    86, loss 1.936\n",
            "Epoch    87, loss 1.935\n",
            "Epoch    88, loss 1.935\n",
            "Epoch    89, loss 1.934\n",
            "Epoch    90, loss 1.933\n",
            "Epoch    91, loss 1.933\n",
            "Epoch    92, loss 1.933\n",
            "Epoch    93, loss 1.932\n",
            "Epoch    94, loss 1.932\n",
            "Epoch    95, loss 1.932\n",
            "Epoch    96, loss 1.932\n",
            "Epoch    97, loss 1.931\n",
            "Epoch    98, loss 1.931\n",
            "Epoch    99, loss 1.931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "further research:\n",
        "- Activation normalization (ActNorm)?\n",
        "- Layer-sequential unit variance initialization?\n",
        "- Batch-Norm\n",
        "- *ConvolutionOrthogonal* initializer\n",
        "- *Fixup* for residual networks\n",
        "- synchronous training in data parallelism"
      ],
      "metadata": {
        "id": "7vgEOIRf1c5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 7.6\n",
        "\n",
        "Show that for $\\mathbf{z} = \\beta + \\Omega \\mathbf{h}$:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{h}} = \\Omega^{\\top},\n",
        "$$\n",
        "\n",
        "where $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{h}}$ is a matrix containing the term $\\frac{\\partial z_i}{\\partial h_j}$ in its $i$-th column and $j$-th row. To do this, first find an expression for the constituent elements $\\frac{\\partial z_i}{\\partial h_j}$, and then consider the form that the matrix $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{h}}$must take.\n",
        "\n",
        "## Answer\n",
        "\n",
        "We have:\n",
        "\n",
        "$$\n",
        "z_i = \\beta_i + \\sum_j w_{ij} h_j\n",
        "$$\n",
        "\n",
        "and so when we take the derivative, we get:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z_i}{\\partial h_j} = w_{ij}\n",
        "$$\n",
        "\n",
        "This will be at the $i$-th column and $j$-th row (i.e., at position $(j,i)$ of the matrix $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{h}}$, which is hence $\\Omega^{\\top}$."
      ],
      "metadata": {
        "id": "GCnZSiqxBzUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### totally in backprop:\n",
        "\n",
        "imagine:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "f_{0} &=& \\beta_{0} + \\omega_{0} x_i\\nonumber\\\\\n",
        "h_{1} &=& \\sin[f_{0}]\\nonumber\\\\\n",
        "f_{1} &=& \\beta_{1} + \\omega_{1}h_{1}\\nonumber\\\\\n",
        "h_{2} &=& \\exp[f_{1}]\\nonumber\\\\\n",
        "f_{2} &=& \\beta_{2} + \\omega_{2} h_{2}\\nonumber\\\\\n",
        "h_{3} &=& \\cos[f_{2}]\\nonumber\\\\\n",
        "f_{3} &=& \\beta_{3} + \\omega_{3}h_{3}\\nonumber\\\\\n",
        "l_i &=& (f_3-y_i)^2\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "\n",
        "we want:\n",
        "\n",
        "\\begin{align}\n",
        "\\quad \\frac{\\partial \\ell_i}{\\partial f_3}, \\quad \\frac{\\partial \\ell_i}{\\partial h_3}, \\quad \\frac{\\partial \\ell_i}{\\partial f_2}, \\quad\n",
        "\\frac{\\partial \\ell_i}{\\partial h_2}, \\quad \\frac{\\partial \\ell_i}{\\partial f_1}, \\quad \\frac{\\partial \\ell_i}{\\partial h_1},  \\quad\\text{and} \\quad \\frac{\\partial \\ell_i}{\\partial f_0}.\n",
        "\\end{align}\n",
        "\n",
        "<br>\n",
        "\n",
        "so we get:\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial \\ell_i}{\\partial f_{2}} &=& \\frac{\\partial h_{3}}{\\partial f_{2}}\\left(\n",
        "\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right)\n",
        "\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial h_{2}} &=& \\frac{\\partial f_{2}}{\\partial h_{2}}\\left(\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}}\\right)\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial f_{1}} &=& \\frac{\\partial h_{2}}{\\partial f_{1}}\\left( \\frac{\\partial f_{2}}{\\partial h_{2}}\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right)\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial h_{1}} &=& \\frac{\\partial f_{1}}{\\partial h_{1}}\\left(\\frac{\\partial h_{2}}{\\partial f_{1}} \\frac{\\partial f_{2}}{\\partial h_{2}}\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right)\\nonumber \\\\\n",
        "\\frac{\\partial \\ell_i}{\\partial f_{0}} &=& \\frac{\\partial h_{1}}{\\partial f_{0}}\\left(\\frac{\\partial f_{1}}{\\partial h_{1}}\\frac{\\partial h_{2}}{\\partial f_{1}} \\frac{\\partial f_{2}}{\\partial h_{2}}\\frac{\\partial h_{3}}{\\partial f_{2}}\\frac{\\partial f_{3}}{\\partial h_{3}}\\frac{\\partial \\ell_i}{\\partial f_{3}} \\right).\n",
        "\\end{align}\n"
      ],
      "metadata": {
        "id": "v_CWe7TCCZQA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aqc3rMMvsfIo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
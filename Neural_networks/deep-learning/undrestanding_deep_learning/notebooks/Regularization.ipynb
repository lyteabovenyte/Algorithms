{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATMgTHps0ruQ"
      },
      "source": [
        "## Regularization\n",
        "This notebook discusses regularization techniques. These are a family of methods that\n",
        "*reduce the generalization gap between training and test performance*. Strictly speaking,\n",
        "regularization involves adding explicit terms to the loss function that favor certain parameter choices. However, in machine learning, this term is commonly used to refer to\n",
        "any strategy that improves generalization.\n",
        "We start by considering regularization in its strictest sense. Then we show how\n",
        "the stochastic gradient descent algorithm itself favors certain solutions. This is known\n",
        "as *implicit* regularization. Following this, we consider a set of *heuristic* methods that\n",
        "improve test performance. These include **early stopping**, **ensembling**, **dropout**, **label\n",
        "smoothing**, and **transfer learning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhI7mUg0w2w"
      },
      "source": [
        "\n",
        "$$\\hat{\\phi} = \\arg\\max_{\\phi} \\left[ \\prod_{i=1}^{I} \\Pr(y_i | x_i, \\phi) \\right].$$\n",
        "\n",
        "\n",
        "$$\n",
        "\\hat{\\phi} = \\arg\\max_{\\phi} \\left[ \\prod_{i=1}^{I} \\Pr(y_i | x_i, \\phi) \\Pr(\\phi) \\right].$$\n",
        "\n",
        "\n",
        "$$\\lambda \\cdot g[\\phi] = -\\log[\\Pr(\\phi)].$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fIby1cLpFRn"
      },
      "source": [
        "### L2-Regularization:\n",
        "\n",
        "$$\\hat{\\phi} = \\arg\\min_{\\phi} \\left[ \\sum_{i=1}^{I} \\ell_i[X_i, y_i] + \\lambda \\sum_{j} \\phi_j^2 \\right],$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvamS8MsqqpA"
      },
      "source": [
        "For neural networks, L2 regularization is usually applied to the weights but not\n",
        "the biases and is hence referred to as a *weight decay* term. The effect is to encourage\n",
        "smaller weights, so the output function is smoother"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt3ziR8Qq03Z"
      },
      "source": [
        "### implicit regularization:\n",
        "\n",
        "An intriguing recent finding is that neither gradient descent nor stochastic gradient\n",
        "descent moves neutrally to the minimum of the loss function; each exhibits a preference\n",
        "for some solutions over others. This is known as implicit regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bc9-My2outCc"
      },
      "source": [
        "stochastic gradient descend implicit regularization\n",
        "\n",
        "\\begin{align*}\n",
        "\\tilde{L}_{\\text{SGD}}[\\phi] &= \\tilde{L}_{\\text{GD}}[\\phi] + \\frac{\\alpha}{4B} \\sum_{b=1}^{B} \\left\\| \\frac{\\partial L_b}{\\partial \\phi} - \\frac{\\partial L}{\\partial \\phi} \\right\\|^2 \\\\\n",
        "&= L[\\phi] + \\frac{\\alpha}{4} \\left\\| \\frac{\\partial L}{\\partial \\phi} \\right\\|^2 + \\frac{\\alpha}{4B} \\sum_{b=1}^{B} \\left\\| \\frac{\\partial L_b}{\\partial \\phi} - \\frac{\\partial L}{\\partial \\phi} \\right\\|^2 \\\\\n",
        "L &= \\frac{1}{I} \\sum_{i=1}^{I} \\ell_i[\\mathbf{x}_i, y_i] \\quad \\text{and} \\quad L_b = \\frac{1}{|B|} \\sum_{i \\in B_b} \\ell_i[\\mathbf{x}_i, y_i].\n",
        "\\end{align*}\n",
        "\n",
        "Here, $L_b$ is the loss for the $b^{th}$ of the B batches in an epoch, and both $L$ and $L_b$ now\n",
        "represent the means of the $I$ individual losses in the full dataset and the $|B|$ individual\n",
        "losses in the batch, respectively we get the last line of the equations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We’ve seen that explicit regularization encourages the training algorithm to find a good\n",
        "solution by adding extra terms to the loss function. This also occurs implicitly as an unintended (but seemingly helpful) byproduct of stochastic gradient descent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implicit vs. Explicit Regularization in Deep Neural Networks\n",
        "\n",
        "| Aspect             | Implicit Regularization                          | Explicit Regularization                          |\n",
        "|--------------------|--------------------------------------------------|--------------------------------------------------|\n",
        "| **Mechanism**      | Naturally emerges from training process or architecture | Deliberately added via a penalty term in the loss function |\n",
        "| **Control**        | Less direct; relies on emergent properties      | Precise control via hyperparameters (e.g., $\\lambda)$ |\n",
        "| **Examples**       | SGD bias to flat minima, dropout, batch norm    | L2 (weight decay), L1 (sparsity), Elastic Net   |\n",
        "| **Intent**         | Often a side effect, not the primary goal       | Purposefully designed to reduce overfitting     |\n",
        "\n",
        "- **Implicit**: Regularization happens as a byproduct (e.g., noisy updates in SGD or dropout’s random masking).\n",
        "- **Explicit**: Regularization is explicitly defined (e.g., adding \\(\\lambda \\sum w^2\\) for L2).\n",
        "- **In Practice**: Both are often combined (e.g., SGD + L2 + dropout) for effective generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### more regularization techniques for better performance:\n",
        "1. **Early stopping**: Early stopping refers to stopping the training procedure before it has fully converged.\n",
        "This can reduce overfitting if the model has already captured the coarse shape of the\n",
        "underlying function but has not yet had time to overfit to the noise. One\n",
        "way of thinking about this is that since the weights are initialized to small values, they simply don’t have time to become large, so early stopping has a similar\n",
        "effect to explicit L2 regularization. A different view is that early stopping reduces the\n",
        "effective model complexity. Hence, we move back down the bias/variance trade-off curve\n",
        "from the critical region, and performance improves.\n",
        "Early stopping has a single hyperparameter, the number of steps after which learning\n",
        "is terminated. As usual, this is chosen empirically using a validation set.\n",
        "However, for early stopping, the hyperparameter can be selected without the need to\n",
        "train multiple models. The model is trained once, the performance on the validation set\n",
        "is monitored every $T$ iterations, and the associated parameters are stored. The stored\n",
        "parameters where the validation performance was best are selected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Ensembling**: Another approach to reducing the generalization gap between training and test data is\n",
        "to build several models and average their predictions. A group of such models is known as an *ensemble*. This technique reliably improves test performance at the cost of training\n",
        "and storing multiple models and performing inference multiple times.\n",
        "The models can be combined by taking the mean of the outputs (for regression\n",
        "problems) or the mean of the pre-softmax activations (for classification problems). The\n",
        "assumption is that model errors are independent and will cancel out. Alternatively,\n",
        "we can take the median of the outputs (for regression problems) or the most frequent\n",
        "predicted class (for classification problems) to make the predictions more robust.\n",
        "One way to train different models is just to use different random initializations. This\n",
        "may help in regions of input space far from the training data. Here, the fitted function\n",
        "is relatively unconstrained, and different models may produce different predictions, so\n",
        "the average of several models may generalize better than any single model.\n",
        "A second approach is to generate several different datasets by re-sampling the train-\n",
        "ing data with replacement and training a different model from each. This is known as\n",
        "bootstrap aggregating or bagging for short. It has the effect of smoothing\n",
        "out the data; if a data point is not present in one training set, the model will interpolate from nearby points; hence, if that point was an outlier, the fitted function will be\n",
        "more moderate in this region. Other approaches include training models with different\n",
        "hyperparameters or training completely different families of models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Dropout**: Dropout clamps a random subset (typically 50%) of hidden units to zero at each iteration\n",
        "of SGD. This makes the network less dependent on any given hidden unit and\n",
        "encourages the weights to have smaller magnitudes so that the change in the function\n",
        "due to the presence or absence of any specific hidden unit is reduced.\n",
        "This technique has the positive benefit that it can eliminate undesirable “kinks” in\n",
        "the function that are far from the training data and don’t affect the loss. For example,\n",
        "consider three hidden units that become active sequentially as we move along the curve\n",
        ". The first hidden unit causes a large increase in the slope. A second hidden unit decreases the slope, so the function goes back down. Finally, the third unit cancels\n",
        "out this decrease and returns the curve to its original trajectory. These three units\n",
        "conspire to make an undesirable local change in the function. This will not change the\n",
        "training loss but is unlikely to generalize well.\n",
        "When several units conspire in this way, eliminating one (as would happen in dropout)\n",
        "causes a considerable change to the output function in the half-space where that unit\n",
        "was active. A subsequent gradient descent step will attempt to compensate\n",
        "for the change that this induces, and such dependencies will be eliminated over time.\n",
        "The overall effect is that large unnecessary changes between training data points are\n",
        "gradually removed even though they contribute nothing to the loss.\n",
        "At test time, we can run the network as usual with all the hidden units active;\n",
        "however, the network now has more hidden units than it was trained with at any given\n",
        "iteration, so we multiply the weights by one minus the dropout probability to compensate.\n",
        "This is known as the weight scaling inference rule. A different approach to inference is\n",
        "to use Monte Carlo dropout, in which we run the network multiple times with different\n",
        "random subsets of units clamped to zero (as in training) and combine the results. This\n",
        "is closely related to ensembling in that every random version of the network is a different\n",
        "model; however, we do not have to train or store multiple networks here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "further research:\n",
        "\n",
        "**adversarial training and label smoothing?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian inference:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The maximum likelihood approach is generally overconfident; it selects the most likely\n",
        "parameters during training and uses these to make predictions. However, many parameter values may be broadly compatible with the data and only slightly less likely. The\n",
        "Bayesian approach treats the parameters as unknown variables and computes a distribution $Pr(\\phi|{xi,yi})$ over these parameters ϕconditioned on the training data $\\{xi,yi\\}$\n",
        "using *Bayes’ rule*:\n",
        "\n",
        "$$\\begin{equation}\n",
        "\\Pr(\\phi|\\{\\mathbf{x}_i, y_i\\}) = \\frac{\\prod_{i=1}^{I} \\Pr(y_i|\\mathbf{x}_i, \\phi) \\Pr(\\phi)}{\\int \\prod_{i=1}^{I} \\Pr(y_i|\\mathbf{x}_i, \\phi) \\Pr(\\phi) \\, d\\phi}\n",
        "\\end{equation}\n",
        "$$\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\Pr(y|\\mathbf{x}, \\{\\mathbf{x}_i, y_i\\}) = \\int \\Pr(y|\\mathbf{x}, \\phi) \\Pr(\\phi|\\{\\mathbf{x}_i, y_i\\}) \\, d\\phi.\n",
        "\\end{equation}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Bayesian approach is elegant and can provide more robust predictions than\n",
        "those that derive from maximum likelihood. Unfortunately, for complex models like\n",
        "neural networks, there is no practical way to represent the full probability distribution over the parameters or to integrate over it during the inference phase. Consequently, all\n",
        "current methods of this type make approximations of some kind, and typically these add\n",
        "considerable complexity to learning and inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in this approach The\n",
        "parameters are treated as uncertain. The posterior probability $Pr(\\phi|{xi,yi})$ for\n",
        "a set of parameters is determined by their compatibility with the data $\\{xi,yi\\}$\n",
        "and a prior distribution $Pr(\\phi)$. Two sets of parameters are sampled from the posterior using normally distributed priors with mean\n",
        "zero and three variances. When the prior variance $\\sigma_{\\phi}^2$ is small, the parameters\n",
        "also tend to be small, and the functions smoother. Inference proceeds by\n",
        "taking a weighted sum (i.e an integeral) over all possible parameter values where the weights are\n",
        "the posterior probabilities. This produces both a prediction of the mean and the associated uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deep neural networks\n",
    "\n",
    "deep neural networks have more than one hidden layer.\n",
    "With $ReLU$ activation functions, both shallow and deep networks describe piecewise\n",
    "linear mappings from input to output. Deep networks can produce many more linear regions than shallow networks for a given number of parameters. Hence, from a practical standpoint, they can be used to describe a broader family of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "think about the difference of two shallow neural networks with are composed and one deep neural network.\n",
    "\n",
    "the equaiton of the second layer of shallow neural network:\n",
    "$$h^\\prime_1 = a[\\theta^\\prime_{10} + \\theta^\\prime_{11}y]$$\n",
    "$$h^\\prime_2 = a[\\theta^\\prime_{20} + \\theta^\\prime_{21}y]$$\n",
    "$$h^\\prime_3 = a[\\theta^\\prime_{30} + \\theta^\\prime_{31}y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then expand $y$ and put it's value from previous shallow neural nets:\n",
    "$$a[\\theta^\\prime_{10} + \\theta^\\prime{11} \\varphi_0 + \\theta^\\prime{11} \\varphi_1 h_1 + \\theta^\\prime{11} \\varphi_2 h_2 + \\theta^\\prime{11} \\varphi_3 h_3]$$\n",
    "$$a[\\theta^\\prime_{20} + \\theta^\\prime{21} \\varphi_0 + \\theta^\\prime{21} \\varphi_1 h_1 + \\theta^\\prime{21} \\varphi_2 h_2 + \\theta^\\prime{21} \\varphi_3 h_3]$$\n",
    "$$a[\\theta^\\prime_{30} + \\theta^\\prime{31} \\varphi_0 + \\theta^\\prime{31} \\varphi_1 h_1 + \\theta^\\prime{31} \\varphi_2 h_2 + \\theta^\\prime{31} \\varphi_3 h_3]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which now we can write this as:\n",
    "$$h^\\prime_1 = a[\\psi_{10} + \\psi_{11} h_1 + \\psi_{12} h_2 + \\psi_{12} h_3$$\n",
    "$$h^\\prime_2 = a[\\psi_{20} + \\psi_{21} h_1 + \\psi_{22} h_2 + \\psi_{22} h_3$$\n",
    "$$h^\\prime_3 = a[\\psi_{30} + \\psi_{31} h_1 + \\psi_{32} h_2 + \\psi_{32} h_3$$\n",
    "which is the genral case of a deep neural network with two hidden layers and each contains three hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering these equations leads to another way to think about how the network constructs an increasingly complicated functions:\n",
    "1. The three hidden units $h_1$,$h_2$, and $h_3$ in the first layer are computed as usual by\n",
    "forming linear functions of the input and passing these through $ReLU$ activation\n",
    "functions\n",
    "2. The pre-activations at the second layer are computed by taking three new linear\n",
    "functions of these hidden units (arguments of the activation functions. At this point, we effectively have a shallow network with three outputs;\n",
    "we have computed three piecewise linear functions with the “joints” between linear\n",
    "regions in the same places.\n",
    "3. At the second hidden layer, another ReLU function $a[\\bullet]$ is applied to each function, which clips them and adds new “joints” to each.\n",
    "4. The final output is a linear combination of these hidden units "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the deep network construction to more than two hidden layers; modern\n",
    "networks might have more than a hundred layers with thousands of hidden units at each\n",
    "layer. The number of hidden units in each layer is referred to as the width of the network,\n",
    "and the number of hidden layers as the depth. The total number of hidden units is a\n",
    "measure of the network’s capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*hyperparameters*:\n",
    "\n",
    "We denote the number of layers as $K$ and the number of hidden units in each layer\n",
    "as $D_1,D_2,...,D_K$ . These are examples of hyperparameters. They are quantities chosen\n",
    "*before* we learn the model parameters (i.e., the slope and intercept terms). For fixed\n",
    "hyperparameters (e.g., $K = 2$ layers with $D_k = 3$ hidden units in each), the model\n",
    "describes a family of functions, and the parameters determine the particular function.\n",
    "Hence, when we also consider the hyperparameters, we can think of neural networks as\n",
    "representing a family of families of functions relating input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in another terms we can have the equations above with matrices as so:\n",
    "$$\\begin{pmatrix}\n",
    "h^\\prime_1\\\\\n",
    "h^\\prime_2\\\\\n",
    "h^\\prime_3\n",
    "\\end{pmatrix} = a\\left[ \\begin{pmatrix}\n",
    "\\psi_{10}\\\\\n",
    "\\psi_{20}\\\\\n",
    "\\psi_{30}\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "\\psi_{11} & \\psi_{12} & \\psi_{13}\\\\\n",
    "\\psi_{21} & \\psi_{22} & \\psi_{23}\\\\\n",
    "\\psi_{31} & \\psi_{32} & \\psi_{33}\n",
    "\\end{pmatrix} \\begin{pmatrix} \n",
    "h_1\\\\\n",
    "h_2\\\\\n",
    "h_3\n",
    "\\end{pmatrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the final output looks like:\n",
    "$$y^\\prime = \\varphi^\\prime_0 + \\begin{pmatrix}\n",
    "\\varphi^\\prime_1 & \\varphi^\\prime_2 & \\varphi^\\prime_3 \\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "h^\\prime_1\\\\\n",
    "h^\\prime_2\\\\\n",
    "h^\\prime_3\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**term**: This notation becomes cumbersome for networks with many layers. Hence, from now\n",
    "on, we will describe the *vector* of hidden units at layer $k$ as $h_k$, the vector of biases\n",
    "(intercepts) that contribute to hidden layer $k + 1$ as $\\beta_k$ , and the weights (slopes) that\n",
    "are applied to the kth layer and contribute to the $(k+1)^{th}$ layer as $\\Omega_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so in a *general deep neural networks* $y = f[x, \\varphi]$ with $k$ hidden layer, we have:\n",
    "$$\\begin{gather}h_1 = a[\\beta_0 + \\Omega_0 x]\\\\\n",
    "h_2 = a[\\beta_1 + \\Omega_1 h_1]\\\\\n",
    "h_3 = a[\\beta_2 + \\Omega_2 h_2]\\\\\n",
    "\\dotsc \\\\\n",
    "h_k = a[\\beta_{k-1} + \\Omega_{k-1} h_{k-1}\\\\\n",
    "y = \\beta_k + \\Omega_k h_k\n",
    "\\end{gather}$$\n",
    "which the parameters $\\varphi$ of this matrices comprise all of these weight matrices and bias vectors: $\\varphi = \\lbrace\\beta_k, \\Omega_k \\rbrace$ which $k = 0, \\dotsc  ,k = K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

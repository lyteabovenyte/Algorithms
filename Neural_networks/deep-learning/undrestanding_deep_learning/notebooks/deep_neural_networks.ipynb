{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### deep neural networks\n",
    "\n",
    "deep neural networks have more than one hidden layer.\n",
    "With $ReLU$ activation functions, both shallow and deep networks describe piecewise\n",
    "linear mappings from input to output. Deep networks can produce many more linear regions than shallow networks for a given number of parameters. Hence, from a practical standpoint, they can be used to describe a broader family of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "think about the difference of two shallow neural networks with are composed and one deep neural network.\n",
    "\n",
    "the equaiton of the second layer of shallow neural network:\n",
    "$$h^\\prime_1 = a[\\theta^\\prime_{10} + \\theta^\\prime_{11}y]$$\n",
    "$$h^\\prime_2 = a[\\theta^\\prime_{20} + \\theta^\\prime_{21}y]$$\n",
    "$$h^\\prime_3 = a[\\theta^\\prime_{30} + \\theta^\\prime_{31}y]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then expand $y$ and put it's value from previous shallow neural nets:\n",
    "$$a[\\theta^\\prime_{10} + \\theta^\\prime{11} \\varphi_0 + \\theta^\\prime{11} \\varphi_1 h_1 + \\theta^\\prime{11} \\varphi_2 h_2 + \\theta^\\prime{11} \\varphi_3 h_3]$$\n",
    "$$a[\\theta^\\prime_{20} + \\theta^\\prime{21} \\varphi_0 + \\theta^\\prime{21} \\varphi_1 h_1 + \\theta^\\prime{21} \\varphi_2 h_2 + \\theta^\\prime{21} \\varphi_3 h_3]$$\n",
    "$$a[\\theta^\\prime_{30} + \\theta^\\prime{31} \\varphi_0 + \\theta^\\prime{31} \\varphi_1 h_1 + \\theta^\\prime{31} \\varphi_2 h_2 + \\theta^\\prime{31} \\varphi_3 h_3]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which now we can write this as:\n",
    "$$h^\\prime_1 = a[\\psi_{10} + \\psi_{11} h_1 + \\psi_{12} h_2 + \\psi_{12} h_3$$\n",
    "$$h^\\prime_2 = a[\\psi_{20} + \\psi_{21} h_1 + \\psi_{22} h_2 + \\psi_{22} h_3$$\n",
    "$$h^\\prime_3 = a[\\psi_{30} + \\psi_{31} h_1 + \\psi_{32} h_2 + \\psi_{32} h_3$$\n",
    "which is the genral case of a deep neural network with two hidden layers and each contains three hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering these equations leads to another way to think about how the network constructs an increasingly complicated functions:\n",
    "1. The three hidden units $h_1$,$h_2$, and $h_3$ in the first layer are computed as usual by\n",
    "forming linear functions of the input and passing these through $ReLU$ activation\n",
    "functions\n",
    "2. The pre-activations at the second layer are computed by taking three new linear\n",
    "functions of these hidden units (arguments of the activation functions. At this point, we effectively have a shallow network with three outputs;\n",
    "we have computed three piecewise linear functions with the “joints” between linear\n",
    "regions in the same places.\n",
    "3. At the second hidden layer, another ReLU function $a[\\bullet]$ is applied to each function, which clips them and adds new “joints” to each.\n",
    "4. The final output is a linear combination of these hidden units "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the deep network construction to more than two hidden layers; modern\n",
    "networks might have more than a hundred layers with thousands of hidden units at each\n",
    "layer. The number of hidden units in each layer is referred to as the width of the network,\n",
    "and the number of hidden layers as the depth. The total number of hidden units is a\n",
    "measure of the network’s capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*hyperparameters*:\n",
    "\n",
    "We denote the number of layers as $K$ and the number of hidden units in each layer\n",
    "as $D_1,D_2,...,D_K$ . These are examples of hyperparameters. They are quantities chosen\n",
    "*before* we learn the model parameters (i.e., the slope and intercept terms). For fixed\n",
    "hyperparameters (e.g., $K = 2$ layers with $D_k = 3$ hidden units in each), the model\n",
    "describes a family of functions, and the parameters determine the particular function.\n",
    "Hence, when we also consider the hyperparameters, we can think of neural networks as\n",
    "representing a family of families of functions relating input to output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in another terms we can have the equations above with matrices as so:\n",
    "$$\\begin{pmatrix}\n",
    "h^\\prime_1\\\\\n",
    "h^\\prime_2\\\\\n",
    "h^\\prime_3\n",
    "\\end{pmatrix} = a\\left[ \\begin{pmatrix}\n",
    "\\psi_{10}\\\\\n",
    "\\psi_{20}\\\\\n",
    "\\psi_{30}\n",
    "\\end{pmatrix} + \\begin{pmatrix}\n",
    "\\psi_{11} & \\psi_{12} & \\psi_{13}\\\\\n",
    "\\psi_{21} & \\psi_{22} & \\psi_{23}\\\\\n",
    "\\psi_{31} & \\psi_{32} & \\psi_{33}\n",
    "\\end{pmatrix} \\begin{pmatrix} \n",
    "h_1\\\\\n",
    "h_2\\\\\n",
    "h_3\n",
    "\\end{pmatrix} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the final output looks like:\n",
    "$$y^\\prime = \\varphi^\\prime_0 + \\begin{pmatrix}\n",
    "\\varphi^\\prime_1 & \\varphi^\\prime_2 & \\varphi^\\prime_3 \\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "h^\\prime_1\\\\\n",
    "h^\\prime_2\\\\\n",
    "h^\\prime_3\n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**term**: This notation becomes cumbersome for networks with many layers. Hence, from now\n",
    "on, we will describe the *vector* of hidden units at layer $k$ as $h_k$, the vector of biases\n",
    "(intercepts) that contribute to hidden layer $k + 1$ as $\\beta_k$ , and the weights (slopes) that\n",
    "are applied to the kth layer and contribute to the $(k+1)^{th}$ layer as $\\Omega_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so in a *general deep neural networks* $y = f[x, \\varphi]$ with $k$ hidden layer, we have:\n",
    "$$\\begin{gather}h_1 = a[\\beta_0 + \\Omega_0 x]\\\\\n",
    "h_2 = a[\\beta_1 + \\Omega_1 h_1]\\\\\n",
    "h_3 = a[\\beta_2 + \\Omega_2 h_2]\\\\\n",
    "\\dotsc \\\\\n",
    "h_k = a[\\beta_{k-1} + \\Omega_{k-1} h_{k-1}\\\\\n",
    "y = \\beta_k + \\Omega_k h_k\n",
    "\\end{gather}$$\n",
    "which the parameters $\\varphi$ of this matrices comprise all of these weight matrices and bias vectors: $\\varphi = \\lbrace\\beta_k, \\Omega_k \\rbrace$ which $k = 0, \\dotsc  ,k = K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "equivalently we can write this as such:\n",
    "$$\n",
    "\\mathbf{y} = \\beta_K + \\mathbf{\\Omega}_K \\mathbf{a} \\left[ \\beta_{K-1} + \\mathbf{\\Omega}_{K-1} \\mathbf{a} \\left[ \\dots \\beta_2 + \\mathbf{\\Omega}_2 \\mathbf{a} \\left[ \\beta_1 + \\mathbf{\\Omega}_1 \\mathbf{a} \\left[ \\beta_0 + \\mathbf{\\Omega}_0 \\mathbf{x} \\right] \\right] \\dots \\right] \\right]\n",
    "$$\n",
    "\n",
    "notice that If the $k_{th}$ layer has $D_k$ hidden units, then the bias vector $\\beta_k−1$ will be of size $D_k$.\n",
    "The last bias vector $\\beta_K$ has the size $D_0$ of the output. The first weight matrix $\\Omega_0$ has size $D_1 \\times D_i$ where $D_i$ is the size of the input. The last weight matrix $\\Omega_K$ is $D_0 \\times D_K$ ,\n",
    "and the remaining matrices $\\Omega$ are $D_k+1 \\times D_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shallow vs. deep neural networks:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A shallow network with one input, one output, and $D > 2$ hidden units can create up\n",
    "to $D + 1$ linear regions and is defined by $3D + 1$ parameters. A deep network with one\n",
    "input, one output, and $K$ layers of $D > 2$ hidden units can create a function with up to\n",
    "$(D+ 1)^K$ linear regions using $3D + 1 + (K−1)D(D+ 1)$ parameters.\n",
    "- Deep neural networks create much more complex functions for a fixed parameter budget, although This seems attractive, but the flexibility of the functions is still limited by the number of parameters. Deep networks can create extremely large numbers of linear regions, but\n",
    "these contain complex dependencies and symmetries.\n",
    "- Functions have been\n",
    "identified that require a shallow network with exponentially more hidden units to achieve\n",
    "an equivalent approximation to that of a deep network. This phenomenon is referred to\n",
    "as the *depth eﬀiciency* of neural networks. This property is also attractive, but it’s not\n",
    "clear that the real-world functions that we want to approximate fall into this category.\n",
    "- facing structured, large inputs: We have discussed fully connected networks where every element of each layer contributes\n",
    "to every element of the subsequent one. However, these are not practical for large, structured inputs like images, where the input might comprise $\\sim10^6$ pixels. The number of parameters would be prohibitive, and moreover, we want different parts of the image\n",
    "to be processed similarly; there is no point in independently learning to recognize the\n",
    "same object at every possible position in the image. The solution is to process local image regions in parallel and then gradually integrate\n",
    "information from increasingly large regions. This kind of *local-to-global* processing is\n",
    "diﬀicult to specify without using multiple layers\n",
    "- A further possible advantage of deep networks over shallow networks is their ease of fitting; it is usually easier to train moderately deep networks than to train shallow ones. It may be that over-parameterized deep models (i.e., those with more parameters than training examples) have a large family of roughly equivalent solutions that are easy to find. However, as we add more hidden layers, training becomes more diﬀicult\n",
    "again. Many methods have been developed to mitigate this problem\n",
    "- Deep neural networks also seem to generalize to new data better than shallow ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### summary: \n",
    "we first considered what happens when we compose two shallow networks.\n",
    "We argued that the first network “folds” the input space, and the second network then\n",
    "applies a piecewise linear function. The effects of the second network are duplicated\n",
    "where the input space is folded onto itself. We then showed that this composition of shallow networks is a special case of a deep\n",
    "network with two layers. We interpreted the ReLU functions in each layer as clipping\n",
    "the input functions in multiple places and creating more “joints” in the output function.\n",
    "We introduced the idea of hyperparameters, which for the networks we’ve seen so far,\n",
    "comprise the number of hidden layers and the number of hidden units in each. Finally, we compared shallow and deep networks. We saw that ($i$) both networks can approximate any function given enough capacity, ($ii$) deep networks produce many\n",
    "more linear regions per parameter, ($iii$) some functions can be approximated much more\n",
    "eﬀiciently by deep networks, ($iv$) large, structured inputs like images are best processed\n",
    "in multiple stages, and ($v$) in practice, the best results for most tasks are achieved using\n",
    "deep networks with many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for having a sense of computing the linear regions on a neural network with $D$ number of units per each $K$ layer and supposing D is an integer multiple of the input dimensionality $D_i$ then the maximum number of regions $N_r$ can be computed exactly and is:\n",
    "$$N_r = \\left( \\frac{D}{D_i} + 1 \\right)^{D_i (K-1)} \\cdot \\sum_{j=0}^{D_i} \\binom{D}{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### General knowledge:\n",
    "- The number of linear regions created by a neural network is a key measure of its expressive power—essentially, how complex the functions it can represent are. Understanding and counting these regions helps us analyze how well a neural network can approximate complicated decision boundaries and function mappings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*depth efficiency* : Several results show that there are functions that can be realized by deep\n",
    "networks but not by any shallow network whose capacity is bounded above exponentially. In\n",
    "other words, it would take an exponentially larger number of units in a shallow network to\n",
    "describe these functions accurately. This is known as the *depth eﬀiciency* of neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### from problems:\n",
    "using the homogeneity of the $ReLU$ function we can prove that:\n",
    "$$\n",
    "\\text{ReLU} \\left[ \\boldsymbol{\\beta}_1 + \\lambda_1 \\cdot \\boldsymbol{\\Omega}_1 \\text{ReLU} \\left[ \\boldsymbol{\\beta}_0 + \\lambda_0 \\cdot \\boldsymbol{\\Omega}_0 \\mathbf{x} \\right] \\right] = \\lambda_0 \\lambda_1 \\cdot \\text{ReLU} \\left[ \\frac{1}{\\lambda_0 \\lambda_1} \\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\text{ReLU} \\left[ \\frac{1}{\\lambda_0} \\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x} \\right] \\right]\n",
    "$$\n",
    "\n",
    "so we can see that the weight matrices can be\n",
    "re-scaled by any magnitude as long as the biases are also adjusted, and the scale factors can be\n",
    "re-applied at the end of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note from problem 4.9*: it is possible to create a function with five linear regions that oscillates in\n",
    "the same way using a shallow network with four hidden units, and generally, for $N \\ge 3$\n",
    "hidden units it’s possible to make a function that oscillates back and forth $N + 1$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a deep neural network with a *single* input, a single *output*, and $K$\n",
    "hidden layers, each of which contains $D$ hidden units. we can have a total of $3D+ 1 + (K−1)D(D+ 1)$ parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

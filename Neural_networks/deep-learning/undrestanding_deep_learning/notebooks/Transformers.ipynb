{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention mechanism: a model for processing text will (i) use parameter\n",
    "sharing to cope with long input passages of differing lengths and (ii) contain connections\n",
    "between word representations that depend on the words themselves. The transformer\n",
    "acquires both properties by using **dot-product self-attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dot product attention based mechanism:\n",
    "$$\\mathrm{sa}_n[X_1, \\dots, X_N] = \\sum_{m=1}^{N} a[X_m, X_n] V_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar weight $a[x_m,x_n]$ is the attention that the nth output pays to input $x_m$. The $N$\n",
    "weights $a[\\bullet,x_n]$ are non-negative and sum to one. Hence, self-attention can be thought\n",
    "of as routing the values in different proportions to create each output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention mechanism: a model for processing text will (i) use parameter\n",
    "sharing to cope with long input passages of differing lengths and (ii) contain connections\n",
    "between word representations that depend on the words themselves. The transformer\n",
    "acquires both properties by using **dot-product self-attention**.\n",
    "\n",
    "Transformers, capture connections between word representations through a mechanism called *self-attention*. This allows the model to weigh the importance of each word in a sequence relative to every other word, *depending on the words themselves*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the dot product attention based mechanism:\n",
    "$$\\mathrm{sa}_n[X_1, \\dots, X_N] = \\sum_{m=1}^{N} a[X_m, X_n] V_m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scalar weight $a[x_m,x_n]$ is the attention that the nth output pays to input $x_m$. The $N$\n",
    "weights $a[\\bullet,x_n]$ are non-negative and sum to one. Hence, self-attention can be thought\n",
    "of as routing the values in different proportions to create each output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### self attention mechanism:\n",
    "\n",
    "Transformers, a type of neural network architecture commonly used in natural language processing (NLP), capture connections between word representations through a mechanism called **self-attention**. This allows the model to weigh the importance of each word in a sequence relative to every other word, depending on the words themselves. Let’s break this down step-by-step to explain how it works:\n",
    "\n",
    "### 1. **Word Representations (Embeddings)**\n",
    "Each word in a sequence is first converted into a vector representation, typically through an embedding layer. These embeddings encode the meaning of the word in a high-dimensional space, but at this stage, they don’t yet account for context or relationships with other words.\n",
    "\n",
    "### 2. **Self-Attention Mechanism**\n",
    "The key innovation in transformers is the self-attention mechanism, which establishes connections between word representations dynamically. Here’s how it happens:\n",
    "- **Query, Key, and Value Vectors**: For each word embedding, the transformer computes three vectors: a **query (Q)**, a **key (K)**, and a **value (V)** vector. These are derived by multiplying the word embedding by learned weight matrices.\n",
    "- **Attention Scores**: The model calculates a score for how much each word should \"pay attention\" to every other word by taking the dot product of the query vector of one word with the key vectors of all words (including itself). This score reflects how relevant one word is to another based on their content.\n",
    "- **Softmax Normalization**: These scores are normalized using the softmax function, turning them into attention weights that sum to 1. This step ensures the model focuses more on highly relevant words.\n",
    "- **Weighted Sum**: The value vectors of all words are then combined into a single representation for each word, weighted by the attention scores. This means the final representation of a word incorporates information from other words, with the influence depending on their relevance (as determined by the attention weights).\n",
    "\n",
    "For example, in the sentence \"The cat sat on the mat,\" the word \"cat\" might have a strong connection to \"sat\" because they are grammatically related, and the attention mechanism assigns a higher weight to \"sat\" when computing the updated representation of \"cat.\"\n",
    "\n",
    "### 3. **Dependency on the Word Itself**\n",
    "The connections between word representations depend on the words themselves because the query, key, and value vectors are derived directly from the word embeddings. Different words produce different Q, K, and V vectors, leading to unique attention patterns. For instance:\n",
    "- If \"cat\" is replaced with \"dog,\" the attention scores and resulting connections shift based on the new embedding for \"dog,\" which might emphasize different relationships in the sentence.\n",
    "- This adaptability allows transformers to model context-sensitive relationships, unlike older models (e.g., RNNs) that process words sequentially without such flexible, content-based weighting.\n",
    "\n",
    "### 4. **Multi-Head Attention**\n",
    "Transformers use **multi-head attention**, where the self-attention process is repeated multiple times in parallel with different weight matrices. This allows the model to capture various types of relationships (e.g., syntactic, semantic) between words simultaneously, making the connections even richer and more nuanced.\n",
    "\n",
    "### 5. **Positional Encoding**\n",
    "Since transformers don’t process words sequentially (unlike RNNs), they add **positional encodings** to the embeddings to provide information about word order. This ensures that the connections also account for the relative positions of words, but the core dependency still stems from the word content via self-attention.\n",
    "\n",
    "### In Summary\n",
    "The connections between word representations in a transformer depend on the words themselves because the self-attention mechanism dynamically computes how much each word influences others based on their embeddings. This process creates contextualized representations where, for example, \"bank\" in \"I sat by the bank\" could connect strongly to \"sat\" (if it’s a bench) or \"river\" (if it’s a water body), depending on the surrounding words. This flexibility and context-awareness are what make transformers so powerful for tasks like translation, summarization, and more.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/self-attention.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, that the outputs results from two chained linear transformations; the value vectors $\\beta_{v} + \\Omega_{v} x_m$ are computed independently for each input $x_m$,\n",
    "and these vectors are combined linearly by the attention weights $a[x_m,x_n]$. However,\n",
    "the overall self-attention computation is *nonlinear*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "again, the self attention mechanism or ($QKV$) is: \n",
    "\n",
    "Each token (word or subword) in the input sequence is transformed into three vectors:\n",
    "1. Query ($Q$) – Represents what the token is looking for in other tokens.\n",
    "2. Key ($K$) – Represents how much information a token has that other tokens might find relevant.\n",
    "3. Value ($V$) – Contains the actual information that will be passed forward after the attention weights are applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism computes the attention scores A using the query and key vectors:\n",
    "$$A = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "- $QK^T$ computes similarity between queries and keys.\n",
    "- $\\sqrt{d_k}$ is a scaling factor to stabilize gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the attention scores $A$, the final representation for each token is computed as:\n",
    "\n",
    "$$\\text{Output} = AV$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means:\n",
    "- The attention scores tell us how much attention each token should pay to every other token.\n",
    "- The weighted sum of value vectors produces the output representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[0.16875885 0.07392469 0.75731646]\n",
      " [0.21372837 0.19409159 0.59218004]\n",
      " [0.07229842 0.02876049 0.89894109]]\n",
      "\n",
      "Final Output (Weighted Sum of Values):\n",
      " [[0.64157534 1.96921544 1.75396406 2.3889108 ]\n",
      " [0.59375779 1.76150136 1.58900757 2.13931733]\n",
      " [0.70437031 2.10776405 1.89519839 2.55911343]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define input (3 tokens, embedding size = 4)\n",
    "X = np.array([[1, 0, 1, 0],  # Token 1\n",
    "              [0, 1, 0, 1],  # Token 2\n",
    "              [1, 1, 1, 1]]) # Token 3\n",
    "\n",
    "# Define weight matrices (randomly initialized for simplicity)\n",
    "np.random.seed(42)\n",
    "W_Q = np.random.rand(4, 4)\n",
    "W_K = np.random.rand(4, 4)\n",
    "W_V = np.random.rand(4, 4)\n",
    "\n",
    "# Compute Query, Key, and Value matrices\n",
    "Q = X @ W_Q  # (3x4) @ (4x4) -> (3x4)\n",
    "K = X @ W_K  # (3x4) @ (4x4) -> (3x4)\n",
    "V = X @ W_V  # (3x4) @ (4x4) -> (3x4)\n",
    "\n",
    "# Compute attention scores (scaled dot-product)\n",
    "attention_scores = Q @ K.T  # (3x4) @ (4x3) -> (3x3)\n",
    "scaled_scores = attention_scores / np.sqrt(K.shape[1])  # Scale by sqrt(d_k)\n",
    "\n",
    "# Apply softmax to get attention weights\n",
    "attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=1, keepdims=True)\n",
    "\n",
    "# Compute final output by multiplying attention weights with Value matrix\n",
    "output = attention_weights @ V  # (3x3) @ (3x4) -> (3x4)\n",
    "\n",
    "# Print results\n",
    "print(\"Attention Weights:\\n\", attention_weights)\n",
    "print(\"\\nFinal Output (Weighted Sum of Values):\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-network:\n",
    "A Hyper-Network in the context of Transformers refers to a secondary neural network that generates the weights for the primary Transformer model, instead of learning them directly. This concept is particularly useful for improving parameter efficiency, adaptability, and generalization in large-scale models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***dot product self attention***:\n",
    "\n",
    "$$\n",
    "q_n = \\beta_q + \\Omega_q X_n, \\\\\n",
    "k_m = \\beta_k + \\Omega_k X_m,\n",
    "$$\n",
    "\n",
    "$$\n",
    "a[X_m, X_n] = \\text{softmax}_m \\left[ k_m^{\\top} \\cdot q_n \\right] = \\frac{\\exp \\left[ k_m^{\\top} q_n \\right]}{\\sum_{m'=1}^{N} \\exp \\left[ k_{m'}^{\\top} q_n \\right]},\n",
    "$$\n",
    "\n",
    "so in this way we compute the attention weights, and for each $x_n$, they are positive and sum to one. the dot product operation returns a measure of similarity between its inputs, so the weights $a[x_{\\bullet},x_n]$ depend on the relative similarities\n",
    "between the $n^{th}$ query and all of the keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so the self-attention summary:\n",
    "\n",
    "The $n_{th}$ output is a weighted sum of the same linear transformation $v_{\\bullet} = \\beta_v + \\Omega_v x_{\\bullet}$\n",
    "applied to all of the inputs, where these attention weights are positive and sum to one.\n",
    "The weights depend on a measure of similarity between input $x_n$ and the other inputs.\n",
    "There is no activation function, but the mechanism is nonlinear due to the dot-product\n",
    "and a softmax operation used to compute the attention weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions to dot-product self-attention\n",
    "\n",
    "- positional encoding\n",
    "  - Absolute positinal encodings\n",
    "  - Relative positional encodings\n",
    "- scaled dot-product self-attention: the $\\sqrt{D_q}$ part in $\\mathrm{Sa}[X] = V \\cdot \\mathrm{Softmax}\\left[\\frac{K^{\\top}Q}{\\sqrt{D_q}}\\right].$\n",
    "- multiple heads: $\\mathrm{MhSa}[X] = \\Omega_c \\left[ \\mathrm{Sa}_1[X]^{\\top}, \\mathrm{Sa}_2[X]^{\\top}, \\dots, \\mathrm{Sa}_H[X]^{\\top} \\right]^{\\top}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the complete set of operations in a transformer architecture:\n",
    "$$X \\gets X + \\mathrm{MhSa}[X]$$\n",
    "$$X \\gets \\mathrm{LayerNorm}[X]$$\n",
    "$$X_n \\gets X_n + \\mathrm{mlp}[X_n]$$\n",
    "$$X \\gets \\mathrm{LayerNorm}[X],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are three types of transformer models:\n",
    "\n",
    "An *encoder* transforms the text embeddings into a representation that can\n",
    "support a variety of tasks. A *decoder* predicts the next token to continue the input text. *Encoder-decoders* are used in sequence-to-sequence tasks, where one text string is\n",
    "converted into another (e.g., machine translation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT: (Bidirectional Encoder Representations from Transformers)\n",
    "- BERT is a pretrained deep learning model designed for contextual word embeddings. It is trained using unsupervised learning on large text corpora and fine-tuned for various downstream tasks like question answering, sentiment analysis, and named entity recognition.\n",
    "\n",
    "- BERT consists of:\n",
    "    - Input Embeddings (Token, Segment, and Positional Embeddings)\n",
    "    - Multiple Transformer Encoder Layers\n",
    "    - Self-Attention Mechanism (Multi-Head Attention)\n",
    "    - Feedforward Networks & Layer Normalization\n",
    "    - Final Output Representations for NLP Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$?$  the difference between encoder and decoder models:\n",
    "\n",
    "The encoder aimed to build a representation of the text that could be fine-tuned to solve a variety of more specific NLP tasks. Conversely, the decoder has one\n",
    "purpose: to generate the next token in a sequence. It can generate a coherent text\n",
    "passage by feeding the extended sequence back into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***autoregressive*** formulation demonstrates the connection between maximizing the\n",
    "joint probability of the tokens and the next token prediction task.\n",
    "\n",
    "$$\n",
    "\\Pr(t_1, t_2, \\dots, t_N) = \\Pr(t_1) \\prod_{n=2}^{N} \\Pr(t_n | t_1, \\dots, t_{n-1}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$?$ The entire decoder network using masked self-attention mechanism operates as follows:\n",
    "\n",
    "The input text is tokenized, and the\n",
    "tokens are converted to embeddings. The embeddings are passed into the transformer\n",
    "network, but now the transformer layers use masked self-attention so that they can\n",
    "only attend to the current and previous tokens. Each of the output embeddings can be\n",
    "thought of as representing a partial sentence, and for each, the goal is to predict the next\n",
    "token in the sequence. Consequently, after the transformer layers, a single linear layer\n",
    "maps each output embedding to the size of the vocabulary, followed by a softmax[•]\n",
    "function that converts these values to probabilities. During training, we aim to maximize\n",
    "the sum of the log probabilities of the next token in the ground truth sequence at every\n",
    "position using a standard multiclass cross-entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in text generation task, many strategies can make the output text more coherent. For example,\n",
    "beam search keeps track of multiple possible sentence completions to find the overall most\n",
    "likely sequence of words (which is not necessarily found by greedily choosing the most\n",
    "likely word at each step). Top-k sampling randomly draws the next word from only the\n",
    "top-K most likely possibilities to prevent the system from accidentally choosing from the\n",
    "long tail of low-probability tokens and leading to an unnecessary linguistic dead end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation between languages is an example of a sequence-to-sequence task. One common approach uses both an encoder (to compute a good representation of the source\n",
    "sentence) and a decoder (to generate the sentence in the target language). This is aptly\n",
    "called an *encoder-decoder* model.\n",
    "\n",
    "This is achieved by modifying the transformer layers in the decoder. Originally,\n",
    "these consisted of a masked self-attention layer followed by a neural network applied\n",
    "individually to each embedding. A new self-attention layer is added between these two components, in which the decoder embeddings attend to the encoder\n",
    "embeddings. This uses a version of self-attention known as encoder-decoder attention or\n",
    "**cross-attention**, where the queries are computed from the decoder embeddings and the\n",
    "keys and values from the encoder embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$??$ open research case: ***Interaction matrices for self-attention*** $??$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

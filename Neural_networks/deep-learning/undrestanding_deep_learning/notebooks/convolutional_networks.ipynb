{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional networks\n",
    "more specialized network components with sparser connections, shared weights, and parallel processing paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for 1D convolutional network:\n",
    "Convolutional layers are network layers based on the convolution operation. In 1D, a\n",
    "convolution transforms an input vector $x$ into an output vector $z$ so that each output $z_i$\n",
    "is a weighted sum of nearby inputs. The same weights are used at every position and\n",
    "are collectively called the *convolution kernel* or *filter*. The size of the region over which\n",
    "inputs are combined is termed the *kernel size*.\n",
    "\n",
    "$$z_i = w_1 x_{i-1} + w_2 x_i + w_3 x_{i+1}$$\n",
    "where: $\\mathbf{w} = [w_1, w_2, w_3]^T$. Notice that the convolution oper- ation is equivariant with respect to translation. If we translate the input $x$, then the corresponding output $z$ is translated in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the members of convolution operations are distinguished by their *stride*, *kernel size*, and *dilation rate*. When we evaluate the output\n",
    "at every position, we term this a stride of one. However, it is also possible to shift the\n",
    "kernel by a stride greater than one. If we have a stride of two, we create roughly half\n",
    "the number of outputs.\n",
    "The kernel size can be increased to integrate over a larger area. However, it typically remains an odd number so that it can be centered around the current\n",
    "position. Increasing the kernel size has the disadvantage of requiring more weights. This\n",
    "leads to the idea of dilated or atrous convolutions, in which the kernel values are interspersed with zeros. For example, we can turn a kernel of size five into a dilated kernel of\n",
    "size three by setting the second and fourth elements to zero. We still integrate information from a larger input region but only require three weights to do this.\n",
    "The number of zeros we intersperse between the weights determines the *dilation rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_i = a \\left[ \\beta + w_1 x_{i-1} + w_2 x_i + w_3 x_{i+1} \\right] = a \\left[ \\beta + \\sum_{j=1}^{3} w_j x_{i+j-2} \\right]$$\n",
    "where the bias $\\beta$ and kernel weights $ω_1,ω_2,ω_3$ are trainable parameters, and (with zero-\n",
    "padding) we treat the input xas zero when it is out of the valid range. This is a special\n",
    "case of a fully connected layer that computes the ith hidden unit as:\n",
    "$$h_i = a \\left[ \\beta_i + \\sum_{j=1}^{D} w_j x_j \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we only apply a single convolution, information will likely be lost; we are averaging\n",
    "nearby inputs, and the $ReLU$ activation function clips results that are less than zero.\n",
    "Hence, it is usual to compute several convolutions in *parallel*. Each convolution produces\n",
    "a new set of hidden variables, termed a *feature map* or *channel*.\n",
    "\n",
    "Typically, multiple convolutions are applied to the input $x$\n",
    "and stored in channels. so channels in terms of the sizes: In general, the input and the hidden layers all have multiple channels. If\n",
    "the incoming layer has $C_i$ channels and we select a kernel size $K$ per channel, the hidden units in each output channel are computed as a weighted sum over all $C_i$ channels and $K$\n",
    "kernel entries using a weight matrix $\\Omega \\in R^{C_i \\times K}$ and one bias. Hence, if there are $C_o$\n",
    "channels in the next layer, then we need $\\Omega \\in R^{C_i \\times C_o \\times K}$ weights and $\\beta \\in R^{C_o}$ biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *receptive field* of a hidden unit in the network is the region of the original input that\n",
    "feeds into it. Consider a convolutional network where each convolutional layer has kernel\n",
    "size three. The hidden units in the first layer take a weighted sum of the three closest\n",
    "inputs, so have receptive fields of size three. The units in the second layer take a weighted\n",
    "sum of the three closest positions in the first layer, which are themselves weighted sums\n",
    "of three inputs. Hence, the hidden units in the second layer have a receptive field of size\n",
    "five. In this way, the receptive field of units in successive layers increases, and information\n",
    "from across the input is gradually integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weighted sum**: In the context of deep neural networks, a weighted sum is a fundamental mathematical operation used to compute the input to a neuron (or node) in a network layer. It combines the inputs from the previous layer (or input data) with a set of weights, which represent the strength or importance of each input, and often includes a bias term. This operation is central to how neural networks process and transform data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This discrepancy between a convolutional neural network and a fully connected deep neural network is probably not due to the difference in the number of parameters;\n",
    "we know overparameterization usually improves performance. The likely\n",
    "explanation is that the convolutional architecture has a *superior inductive bias* (i.e.,\n",
    "interpolates between the training data better) because we have embodied some prior\n",
    "knowledge in the architecture; we have forced the network to process each position in\n",
    "the input in the same way. We know that the data were created by starting with a\n",
    "template that is (among other operations) randomly translated, so this is sensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fully connected network has to learn what each digit template looks like at every\n",
    "position. In contrast, the convolutional network shares information across positions and\n",
    "hence learns to identify each category more accurately. Another way of thinking about\n",
    "this is that when we train the convolutional network, we search through a smaller family\n",
    "of input/output mappings, all of which are plausible. Alternatively, the convolutional\n",
    "structure can be considered a regularizer that applies an infinite penalty to most of the\n",
    "solutions that a fully connected network can describe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolutional neural networks for 2D images:\n",
    "$$h_{ij} = a \\left[ \\beta + \\sum_{m=1}^{3} \\sum_{n=1}^{3} w_{mn} x_{i+m-2, j+n-2} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further research: \n",
    "- **max_pooling, mean_pooling and average_pooling**\n",
    "- downsampling and upsampling\n",
    "- transposed convolutions\n",
    "- *cutout* (dropout equvalent for CNN)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary:\n",
    "In convolutional layers, each hidden unit is computed by taking a weighted sum of the\n",
    "nearby inputs, adding a bias, and applying an activation function. The weights and\n",
    "the bias are the same at every spatial position, so there are far fewer parameters than\n",
    "in a fully connected network, and the number of parameters doesn’t increase with the\n",
    "input image size. To ensure that information is not lost, this operation is repeated with different weights and biases to create multiple channels at each spatial position.\n",
    "Typical convolutional networks consist of convolutional layers interspersed with layers\n",
    "that downsample by a factor of two. As a data example passes through the network, the\n",
    "spatial dimensions usually decrease by factors of two, and the channels increase by factors\n",
    "of two. At the end of the network, there are typically one or more fully connected layers\n",
    "that integrate information from across the entire input and create the desired output. If\n",
    "the output is an image, a mirrored *“decoder”* upsamples back to the original size.\n",
    "The translational equivariance of convolutional layers imposes a useful inductive bias\n",
    "that increases performance for image-based tasks relative to fully connected networks.\n",
    "We described image classification, object detection, and semantic segmentation networks.\n",
    "Image classification performance was shown to improve as the network became deeper.\n",
    "However, subsequent experiments showed that increasing the network depth indefinitely\n",
    "doesn’t continue to help; after a certain depth, the system becomes difficult to train.\n",
    "This is the motivation for residual connections, which are the topic of the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "1. Convolutional networks are typically initialized using\n",
    "Xavier initialization (Glorot & Bengio, 2010) or He initialization (He et al., 2015), as described\n",
    "in section 7.5. However, the ConvolutionOrthogonal initializer (Xiao et al., 2018a) is special- ized for convolutional networks. Networks of up to 10,000 layers can be trained using this\n",
    "initialization without the need for residual connections.\n",
    "2. Dropout is effective for fully connected networks but less so for convolutional layers (Park &\n",
    "Kwak, 2016). This may be because neighboring image pixels are highly correlated, so if a hidden\n",
    "unit drops out, the same information is passed on via adjacent positions. This is the motivation\n",
    "for spatial dropout and cutout. In spatial dropout (Tompson et al., 2015), entire feature maps\n",
    "are discarded instead of individual pixels. This circumvents the problem of neighboring pixels\n",
    "carrying the same information. Similarly, DeVries & Taylor (2017b) propose cutout, in which a\n",
    "square patch of each input image is masked at training time. Wu & Gu (2015) modified max\n",
    "pooling for dropout layers using a method that involves sampling from a probability distribution\n",
    "over the constituent elements rather than always taking the maximum."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

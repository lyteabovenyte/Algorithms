{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d219bc9",
   "metadata": {},
   "source": [
    "### Probability\n",
    "\n",
    "one note in reading: ðŸ˜… the vertical line '|' in $Pr(x|y)$ is read in english as \"**Given**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93fe4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    body {\n",
       "        --vscode-font-family: \"Arial\";\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "    body {\n",
    "        --vscode-font-family: \"Arial\";\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc7fcc9",
   "metadata": {},
   "source": [
    "Probability is critical to deep learning. In supervised learning, deep networks implicitly rely on a probabilistic formulation of the loss function. In unsupervised learning,\n",
    "generative models aim to produce samples that are drawn from the same probability\n",
    "distribution as the training data. Reinforcement learning occurs within Markov decision\n",
    "processes, and these are defined in terms of probability distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4c505",
   "metadata": {},
   "source": [
    "**Random variable and probability distribution**:\n",
    "\n",
    "A random variable xdenotes a quantity that is uncertain. It may be discrete (take only\n",
    "certain values, for example integers) or continuous (take any value on a continuum, for\n",
    "example real numbers). If we observe several instances of a random variable x, it will\n",
    "take different values, and the relative propensity to take different values is described by\n",
    "a probability distribution Pr(x).\n",
    "For a discrete variable, this distribution associates a probability Pr(x= k) âˆˆ[0,1] with\n",
    "each potential outcome k, and the sum of these probabilities is one. For a continuous\n",
    "variable, there is a non-negative probability density Pr(x= a) â‰¥0 associated with each\n",
    "value a in the domain of x, and the **integral** of this **probability density function (PDF)** over this domain must be one. This density can be greater than one for any point a. From here on, we assume that the random variables are continuous. The ideas are exactly the same for discrete distributions but with sums replacing integrals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413e1d6",
   "metadata": {},
   "source": [
    "**Joint probability**:\n",
    "\n",
    "Consider the case where we have two random variables $x$ and $y$. The joint distribution $Pr(x,y)$ tells us about the propensity that $x$ and $y$ take particular combinations of values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687fda8f",
   "metadata": {},
   "source": [
    "<img src=./images/joint_probability.png width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1190069",
   "metadata": {},
   "source": [
    "**marginization**:\n",
    "\n",
    "If we know the joint distribution $Pr(x,y)$ over two variables, we can recover the marginal\n",
    "distributions $Pr(x)$ and $Pr(y)$ by integrating over the other variable:\n",
    "\n",
    "$$\\int \\text{Pr}(x, y) \\cdot dx = \\text{Pr}(y)$$\n",
    "\n",
    "$$\\int \\text{Pr}(x, y) \\cdot dy = \\text{Pr}(x)$$\n",
    "\n",
    "This process is called marginalization and has the interpretation that we are computing the distribution of one variable regardless of the value the other one took. The\n",
    "idea of marginalization extends to higher dimensions, so if we have a joint distribution $Pr(x,y,z)$, we can recover the joint distribution $Pr(x,z)$ by integrating over $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cf693",
   "metadata": {},
   "source": [
    "$?$ what is **Probability Density Function**:\n",
    "\n",
    "A Probability Density Function (PDF) describes the relative likelihood of a continuous random variable taking on a particular value. in simple terms, The PDF tells you how dense the probability is around a value\n",
    "\n",
    "the core idea:\n",
    "\n",
    "For a continuous variable $X$ with a PDF $f(x)$, the probability that $X$ lies within an interval $[a, b]$ is:\n",
    "\n",
    "$$P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx$$\n",
    "\n",
    "Key Properties:\n",
    "- $f(x) \\geq 0$ for all $x$\n",
    "- Total area under the curve = 1: $\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd0b94f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "**Conditional Probability**:\n",
    "\n",
    "$Pr(x|y)$ is the probability of $x$ given $y$. The conditional probability $Pr(x|y)$ can be\n",
    "found by taking a slice through the joint distribution $Pr(x,y)$ for a fixed $y$. This slice is\n",
    "then divided by the probability of that value $y$ occurring (the total area under the slice)\n",
    "so that the conditional distribution sums to one:\n",
    "\n",
    "$$\\Pr(x|y) = \\frac{\\Pr(x, y)}{\\Pr(y)}$$  \n",
    "\n",
    "this is a function of $x$ when $y$ is fixed.\n",
    "\n",
    "$$\\Pr(y|x) = \\frac{\\Pr(x, y)}{\\Pr(x)}$$\n",
    "\n",
    "this is a function of $y$ when $x$ is fixed\n",
    "\n",
    "Note that, When we consider the conditional probability $Pr(x|y)$ as a function of $x$, it *must* sum\n",
    "to one. When we consider the same quantity $Pr(x|y)$ as a function of $y$, it is termed the\n",
    "**likelihood** of $x$ given $y$ and does *not* have to sum to one.\n",
    "\n",
    "more on **Likelihood**:\n",
    "\n",
    "Interpreting $\\Pr(x \\mid y)$ as a Function of $y$:\n",
    "\n",
    "If we fix $x$, and view $\\Pr(x \\mid y)$ as a function of $y$ instead of $x$, it is no longer a probability distribution. Instead, we call it the **likelihood**: $$\\mathcal{L}(y) = \\Pr(x \\mid y) \\quad \\text{(as a function of } y \\text{, with } x \\text{ fixed)}$$\n",
    "\n",
    "$?$ This likelihood function does not need to sum to 1 over y. Why?\n",
    "\n",
    "$!$ Because itâ€™s not a probability distribution over y.\n",
    "\n",
    "Instead, itâ€™s used as a measure of how likely the fixed data x is under different hypothetical values of y. In statistics and machine learning, this comes up in parameter estimation â€” especially in **maximum likelihood estimation (MLE)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427fd6d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Bayes' Rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d2929",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "from the equation above we can rearrange $Pr(x|y)$ and $Pr(y|x)$ so that:\n",
    "\n",
    "$$\\Pr(x|y) = \\frac{\\Pr(y|x) \\Pr(x)}{\\Pr(y)}.$$\n",
    "\n",
    "This expression relates the conditional probability $Pr(x|y)$ of $x$ given $y$ to the conditional\n",
    "probability $Pr(y|x)$ of $y$ given $x$ and is known as **Bayesâ€™ rule**.\n",
    "\n",
    "so, the Key Concept of Bayes' Rule:\n",
    "\n",
    "Each term in this Bayesâ€™ rule has a name. The term $Pr(y|x)$ is the likelihood of $y$\n",
    "given $x$, and the term $Pr(x)$ is the *prior probability* of $x$. The denominator $Pr(y)$ is\n",
    "known as the *evidence*, and the left-hand side $Pr(x|y)$ is termed the *posterior probability*\n",
    "of $x$ given $y$. **The equation maps from the prior $Pr(x)$ (what we know about $x$ before observing y) to the posterior $Pr(x|y)$ (what we know about $x$ after observing $y$).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37585c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Expectation:\n",
    "\n",
    "Formally, the expectation of a random variable $X$, denoted as $\\mathbb{E}[X]$, is a weighted average of all possible values that $X$ can take, weighted by their probabilities.\n",
    "\n",
    "If $X$ is continuous, then we integrate over all values of $X$:\n",
    "\n",
    "$$\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f_X(x) \\, dx$$\n",
    "\n",
    "Where $f_X(x)$ is the probability density function (PDF) of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a4611",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Consider a function $f[x]$ and a probability distribution $Pr(x)$ defined over $x$. The expected value of a function $f[\\bullet]$ of a random variable $x$ with respect to the probability distribution $Pr(x)$ is defined as:\n",
    "\n",
    "$$\\mathbb{E}_x[f(x)] = \\int f(x) \\Pr(x) \\, dx$$\n",
    "\n",
    "As the name suggests, this is the expected or average value of $f[x]$ after taking into account\n",
    "the probability of seeing different values of $x$. This idea generalizes to functions $f[\\bullet,\\bullet]$ of\n",
    "more than one random variable:\n",
    "\n",
    "$$\\mathbb{E}_{x,y}[f(x,y)] = \\iint f(x,y) \\Pr(x,y) \\, dx \\, dy$$\n",
    "\n",
    "**An expectation is always taken with respect to a distribution over one or more variables.**\n",
    "\n",
    "If we drew a large number I of samples $\\{x_i\\}_{i=1}^I$ from $Pr(x)$, calculated $f[x_i]$ for\n",
    "each sample and took the average of these values, the result would approximate the\n",
    "expectation $E[f[x]]$ of the function:\n",
    "\n",
    "$$\\mathbb{E}_x[f(x)] \\approx \\frac{1}{I} \\sum_{i=1}^I f(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3cea30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Mean, Variance and Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d3e9d",
   "metadata": {},
   "source": [
    "For some choices of function $f[\\bullet]$, the expectation is given a special name. These quantities are often used to summarize the properties of complex distributions. For example,\n",
    "when $f[x] = x$, the resulting expectation $E[x]$ is termed the mean, $\\mu$. It is a measure of the center of a distribution. Similarly, the expected squared deviation from the\n",
    "mean $E[(xâˆ’\\mu)2]$ is termed the variance, $\\sigma^2$. This is a measure of the spread of the\n",
    "distribution. The standard deviation $\\sigma$ is the positive square root of the variance. It\n",
    "also measures the spread of the distribution but has the merit that it is expressed in the\n",
    "same units as the variable $x$.\n",
    "\n",
    "As the name suggests, the **covariance** $E[(xâˆ’\\mu x)(yâˆ’ \\mu y )]$ of two variables x and y\n",
    "measures the degree to which they co-vary. Here Âµx and Âµy represent the mean of the\n",
    "variables x and y, respectively. The covariance will be large when the variance of both\n",
    "variables is large and when the value of x tends to increase when the value of y increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdfb719",
   "metadata": {},
   "source": [
    "important note on **Covariance**:\n",
    "\n",
    "The covariances of multiple random variables stored in a column vector $x \\in R^D$ can be\n",
    "represented by the DÃ—D covariance matrix $E[(xâˆ’Âµx)(xâˆ’\\mu x)^T ]$, where the vector Âµx\n",
    "contains the means E[x]. The element at position (i,j) of this matrix represents the\n",
    "covariance between variables xi and xj."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642d5c5",
   "metadata": {},
   "source": [
    "**Variance Identity**:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[(x - \\mu)^2] &= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2, \\\\\n",
    "\\mathbb{E}[(x - \\mu)^2] &= \\mathbb{E}[x^2 - 2\\mu x + \\mu^2] \\\\\n",
    "&= \\mathbb{E}[x^2] - \\mathbb{E}[2\\mu x] + \\mathbb{E}[\\mu^2] \\\\\n",
    "&= \\mathbb{E}[x^2] - 2\\mu \\cdot \\mathbb{E}[x] + \\mu^2 \\\\\n",
    "&= \\mathbb{E}[x^2] - 2\\mu^2 + \\mu^2 \\\\\n",
    "&= \\mathbb{E}[x^2] - \\mu^2 \\\\\n",
    "&= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2,\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8c97b",
   "metadata": {},
   "source": [
    "#### Standardization:\n",
    "\n",
    "Setting the mean of a random variable to zero and the variance to one is known as\n",
    "*standardization*. This is achieved using the transformation:\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "This transformation has the following properties:\n",
    "- $\\mathbb{E}[Z] = 0$\n",
    "- $\\text{Var}(Z) = 1$\n",
    "\n",
    "So $Z$ is a new variable that represents how many standard deviations a particular value of $X$ is from its mean. Thatâ€™s why $Z$ is often called a z-score in statistics.\n",
    "\n",
    "this is useful in:\n",
    "1. comparison across distributions: If you have two variables (say height and weight), they may be measured on completely different scales. Standardizing them allows you to compare them directly.\n",
    "2. Normalization for Gaussian Distributions: If $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then $Z = \\frac{X - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)$. This is called the **standard normal distribution**.\n",
    "\n",
    "The mean of the new distribution over z is given by:\n",
    "\n",
    "$$\\mathbb{E}[z] = \\mathbb{E}\\left[\\frac{x - \\mu}{\\sigma}\\right] \\\\\n",
    "= \\frac{1}{\\sigma} \\mathbb{E}[x - \\mu] \\\\\n",
    "= \\frac{1}{\\sigma} (\\mathbb{E}[x] - \\mathbb{E}[\\mu]) \\\\\n",
    "= \\frac{1}{\\sigma} (\\mu - \\mu) = 0$$\n",
    "\n",
    "The variance of the new distribution is given by:\n",
    "\n",
    "$$\\mathbb{E}[(z - \\mu_z)^2] = \\mathbb{E}\\left[(z - \\mathbb{E}[z])^2\\right] \\\\\n",
    "= \\mathbb{E}[z^2] \\\\\n",
    "= \\mathbb{E}\\left[\\left(\\frac{x - \\mu}{\\sigma}\\right)^2\\right] \\\\\n",
    "= \\frac{1}{\\sigma^2} \\mathbb{E}[(x - \\mu)^2] \\\\\n",
    "= \\frac{1}{\\sigma^2} \\cdot \\sigma^2 = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe170d",
   "metadata": {},
   "source": [
    "In the multivariate case, we can standardize a variable $x$ with mean $\\mu$ and covariance\n",
    "matrix $\\Sigma$ using:\n",
    "\n",
    "$$z = \\Sigma^{-1/2} (x - \\mu)$$\n",
    "\n",
    "So the vector z is a transformed version of x that has been:\n",
    "1.\tCentered (zero mean),\n",
    "2.\tScaled and rotated (via $\\Sigma^{-1/2}$) so that its covariance becomes the identity matrix.\n",
    "\n",
    "The result will have a mean E[z] = 0 and an identity covariance matrix $E[(zâˆ’E[z])(zâˆ’E[z])^T ] = I$. To reverse this process, we use:\n",
    "\n",
    "$$x = \\mu + \\Sigma^{1/2} z$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f156fa5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

##### Introduction and motivation:

- Machine learning builds upon the language of mathematics to express
concepts that seem intuitively obvious but that are surprisingly difficult
to formalize.

- inner product of two vectors: $\langle \mathbf{u}, \mathbf{v} \rangle = \sum_{i=1}^{n} u_i v_i$ and two matrices: $\langle \mathbf{u}, \mathbf{v} \rangle = \mathbf{u}^T \mathbf{v}$

- The *gradient* is a generalization of the derivative for multivariable functions. It points in the direction of the steepest increase of a function and gives the rate of change in that direction: $\nabla f = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)$ or equivalently: $\nabla f = \text{grad} \, f = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}$. The gradient is widely used in ML, particularly in Gradient Descent, an optimization algorithm to minimize loss functions: $\theta = \theta - \alpha \nabla f(\theta)$; where  $\theta$  are model parameters,  $\alpha$  is the learning rate,  $\nabla f(\theta)$  is the gradient of the loss function.

- Machine learning is about designing algorithms that *automatically* extract
valuable information from data.

- A model is said to learn from data if its performance on a given task improves after the data is taken into account.
The goal is to find good models that generalize well to yet unseen data,
which we may care about in the future. *Learning* can be understood as a way to automatically find patterns and structure in data by optimizing the parameters of the model.

- we assume that data has already been appropriately converted into a numerical representation suitable for reading into a computer program. Therefore, we think of data as *vectors*.

- three different ways to think about vectors: a vector as an array of numbers (a computer science view), a vector as an arrow with a direction and magnitude (a physics view), and a vector as an object that obeys addition and scaling (a mathematical view).

- Assume we are given a dataset and a suitable model. *Training* the model means to use the data available to optimize some parameters of the model with respect to a utility function that evaluates how well the model predicts the training data.

- summary:
    - We represent data as vectors.
    - We choose an appropriate model, either using the probabilistic or optimization view.
    - We learn from available data by using numerical optimization methods with the aim that the model performs well on data not used for training.

- Given two vectors representing two objects in the real world, we want to make statements about their similarity. The idea is that vectors that are similar should be predicted to have similar outputs by our machine learning algorithm (our predictor). To formalize the idea of similarity between vectors, we need to introduce operations that take two vectors as input and return a numerical value representing their similarity. The construction of similarity and distances is central to *analytic geometry*.

- Quantification of uncertainty is the realm of *probability theory*

- To train machine learning models, we typically find parameters that
maximize some performance measure. Many optimization techniques re-
quire the concept of a gradient, which tells us the *direction* in which to
search for a solution.

- The objective of density estimation is to find a probability distribution that describes a given dataset.
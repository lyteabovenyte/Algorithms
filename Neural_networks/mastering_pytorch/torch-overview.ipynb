{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch overview and warming-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overview of Adam optimization schedule:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "SG_i^t = \\frac{SG_i^t}{1 - \\gamma^t}\n",
    "\\end{equation}\n",
    "$$\n",
    "$$\n",
    "\\begin{equation}\n",
    "SSG_i^t = \\frac{SSG_i^t}{1 - \\gamma}\n",
    "\\end{equation}\n",
    "$$\n",
    "Once they are defined, the parameter update is expressed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\beta_i^{t+1} = \\beta_i^t - \\frac{\\alpha}{\\sqrt{SSG_i^t + \\epsilon}} \\times SG_i^t\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "so Adam optimization involves three hyperparameters â€“ the base\n",
    "learning rate, and the two decaying rates for the gradients and squared gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the overall difference: *RMSprop*, *Adadelta*, and *Adam* are quite close in terms of their performance on various deep learning\n",
    "tasks. RMSprop is largely similar to Adadelta, except for the use of the base learning rate in RMSprop\n",
    "versus the use of the decaying average of previous parameter updates in Adadelta. Adam is slightly\n",
    "different in that it also includes the first-moment calculation of gradients and accounts for bias correction. Overall, Adam could be the optimizer to go with, all else being equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in terms of the difference between torch and tensorflow: *Eager execution* is basically an imperative programming mode where mathematical operations are\n",
    "computed immediately. A *deferred execution* mode would have all the operations stored in a computational graph without immediate calculations and then the entire graph would be evaluated later.\n",
    "Eager execution is considered advantageous for reasons such as intuitive flow, easy debugging, and\n",
    "less scaffolding code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is a tensor? *Tensors*\n",
    "are computational units, very similar to NumPy arrays, except that they can also be used on GPUs to\n",
    "accelerate computing. tensors are conceptually similar to NumPy arrays. A tensor is an n-dimensional\n",
    "array on which we can operate mathematical functions, accelerate computations via GPUs, and *can also keep track of a computational graph and gradients*, which prove vital for deep learning. To run\n",
    "a tensor on a GPU, all we need is to cast the tensor into a certain data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__and__',\n",
       " '__annotations__',\n",
       " '__array__',\n",
       " '__array_priority__',\n",
       " '__array_wrap__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__complex__',\n",
       " '__contains__',\n",
       " '__deepcopy__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__dlpack__',\n",
       " '__dlpack_device__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__firstlineno__',\n",
       " '__float__',\n",
       " '__floordiv__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__iand__',\n",
       " '__idiv__',\n",
       " '__ifloordiv__',\n",
       " '__ilshift__',\n",
       " '__imod__',\n",
       " '__imul__',\n",
       " '__index__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__int__',\n",
       " '__invert__',\n",
       " '__ior__',\n",
       " '__ipow__',\n",
       " '__irshift__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__ixor__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__long__',\n",
       " '__lshift__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__mod__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__or__',\n",
       " '__pos__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rand__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__rfloordiv__',\n",
       " '__rlshift__',\n",
       " '__rmatmul__',\n",
       " '__rmod__',\n",
       " '__rmul__',\n",
       " '__ror__',\n",
       " '__rpow__',\n",
       " '__rrshift__',\n",
       " '__rshift__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__rxor__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__static_attributes__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__torch_dispatch__',\n",
       " '__torch_function__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '__xor__',\n",
       " '_addmm_activation',\n",
       " '_autocast_to_full_precision',\n",
       " '_autocast_to_reduced_precision',\n",
       " '_backward_hooks',\n",
       " '_base',\n",
       " '_cdata',\n",
       " '_clear_non_serializable_cached_data',\n",
       " '_coalesced_',\n",
       " '_conj',\n",
       " '_conj_physical',\n",
       " '_dimI',\n",
       " '_dimV',\n",
       " '_fix_weakref',\n",
       " '_grad',\n",
       " '_grad_fn',\n",
       " '_has_symbolic_sizes_strides',\n",
       " '_indices',\n",
       " '_is_all_true',\n",
       " '_is_any_true',\n",
       " '_is_view',\n",
       " '_is_zerotensor',\n",
       " '_lazy_clone',\n",
       " '_make_subclass',\n",
       " '_make_wrapper_subclass',\n",
       " '_neg_view',\n",
       " '_nested_tensor_size',\n",
       " '_nested_tensor_storage_offsets',\n",
       " '_nested_tensor_strides',\n",
       " '_nnz',\n",
       " '_post_accumulate_grad_hooks',\n",
       " '_python_dispatch',\n",
       " '_reduce_ex_internal',\n",
       " '_rev_view_func_unsafe',\n",
       " '_sparse_mask_projection',\n",
       " '_to_dense',\n",
       " '_to_sparse',\n",
       " '_to_sparse_bsc',\n",
       " '_to_sparse_bsr',\n",
       " '_to_sparse_csc',\n",
       " '_to_sparse_csr',\n",
       " '_typed_storage',\n",
       " '_update_names',\n",
       " '_use_count',\n",
       " '_values',\n",
       " '_version',\n",
       " '_view_func',\n",
       " '_view_func_unsafe',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'absolute',\n",
       " 'absolute_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'acosh',\n",
       " 'acosh_',\n",
       " 'add',\n",
       " 'add_',\n",
       " 'addbmm',\n",
       " 'addbmm_',\n",
       " 'addcdiv',\n",
       " 'addcdiv_',\n",
       " 'addcmul',\n",
       " 'addcmul_',\n",
       " 'addmm',\n",
       " 'addmm_',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'addr_',\n",
       " 'adjoint',\n",
       " 'align_as',\n",
       " 'align_to',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'amax',\n",
       " 'amin',\n",
       " 'aminmax',\n",
       " 'angle',\n",
       " 'any',\n",
       " 'apply_',\n",
       " 'arccos',\n",
       " 'arccos_',\n",
       " 'arccosh',\n",
       " 'arccosh_',\n",
       " 'arcsin',\n",
       " 'arcsin_',\n",
       " 'arcsinh',\n",
       " 'arcsinh_',\n",
       " 'arctan',\n",
       " 'arctan2',\n",
       " 'arctan2_',\n",
       " 'arctan_',\n",
       " 'arctanh',\n",
       " 'arctanh_',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'argwhere',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_strided_scatter',\n",
       " 'as_subclass',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'asinh',\n",
       " 'asinh_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan2_',\n",
       " 'atan_',\n",
       " 'atanh',\n",
       " 'atanh_',\n",
       " 'backward',\n",
       " 'baddbmm',\n",
       " 'baddbmm_',\n",
       " 'bernoulli',\n",
       " 'bernoulli_',\n",
       " 'bfloat16',\n",
       " 'bincount',\n",
       " 'bitwise_and',\n",
       " 'bitwise_and_',\n",
       " 'bitwise_left_shift',\n",
       " 'bitwise_left_shift_',\n",
       " 'bitwise_not',\n",
       " 'bitwise_not_',\n",
       " 'bitwise_or',\n",
       " 'bitwise_or_',\n",
       " 'bitwise_right_shift',\n",
       " 'bitwise_right_shift_',\n",
       " 'bitwise_xor',\n",
       " 'bitwise_xor_',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_to',\n",
       " 'byte',\n",
       " 'cauchy_',\n",
       " 'ccol_indices',\n",
       " 'cdouble',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'cfloat',\n",
       " 'chalf',\n",
       " 'char',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clip',\n",
       " 'clip_',\n",
       " 'clone',\n",
       " 'coalesce',\n",
       " 'col_indices',\n",
       " 'conj',\n",
       " 'conj_physical',\n",
       " 'conj_physical_',\n",
       " 'contiguous',\n",
       " 'copy_',\n",
       " 'copysign',\n",
       " 'copysign_',\n",
       " 'corrcoef',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'count_nonzero',\n",
       " 'cov',\n",
       " 'cpu',\n",
       " 'cross',\n",
       " 'crow_indices',\n",
       " 'cuda',\n",
       " 'cummax',\n",
       " 'cummin',\n",
       " 'cumprod',\n",
       " 'cumprod_',\n",
       " 'cumsum',\n",
       " 'cumsum_',\n",
       " 'data',\n",
       " 'data_ptr',\n",
       " 'deg2rad',\n",
       " 'deg2rad_',\n",
       " 'dense_dim',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'diagonal_scatter',\n",
       " 'diff',\n",
       " 'digamma',\n",
       " 'digamma_',\n",
       " 'dim',\n",
       " 'dim_order',\n",
       " 'dist',\n",
       " 'div',\n",
       " 'div_',\n",
       " 'divide',\n",
       " 'divide_',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dsplit',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'element_size',\n",
       " 'eq',\n",
       " 'eq_',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'erfinv_',\n",
       " 'exp',\n",
       " 'exp2',\n",
       " 'exp2_',\n",
       " 'exp_',\n",
       " 'expand',\n",
       " 'expand_as',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'exponential_',\n",
       " 'fill_',\n",
       " 'fill_diagonal_',\n",
       " 'fix',\n",
       " 'fix_',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'fliplr',\n",
       " 'flipud',\n",
       " 'float',\n",
       " 'float_power',\n",
       " 'float_power_',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'floor_divide',\n",
       " 'floor_divide_',\n",
       " 'fmax',\n",
       " 'fmin',\n",
       " 'fmod',\n",
       " 'fmod_',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frexp',\n",
       " 'gather',\n",
       " 'gcd',\n",
       " 'gcd_',\n",
       " 'ge',\n",
       " 'ge_',\n",
       " 'geometric_',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'get_device',\n",
       " 'grad',\n",
       " 'grad_fn',\n",
       " 'greater',\n",
       " 'greater_',\n",
       " 'greater_equal',\n",
       " 'greater_equal_',\n",
       " 'gt',\n",
       " 'gt_',\n",
       " 'half',\n",
       " 'hardshrink',\n",
       " 'has_names',\n",
       " 'heaviside',\n",
       " 'heaviside_',\n",
       " 'histc',\n",
       " 'histogram',\n",
       " 'hsplit',\n",
       " 'hypot',\n",
       " 'hypot_',\n",
       " 'i0',\n",
       " 'i0_',\n",
       " 'igamma',\n",
       " 'igamma_',\n",
       " 'igammac',\n",
       " 'igammac_',\n",
       " 'imag',\n",
       " 'index_add',\n",
       " 'index_add_',\n",
       " 'index_copy',\n",
       " 'index_copy_',\n",
       " 'index_fill',\n",
       " 'index_fill_',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_reduce',\n",
       " 'index_reduce_',\n",
       " 'index_select',\n",
       " 'indices',\n",
       " 'inner',\n",
       " 'int',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'ipu',\n",
       " 'is_coalesced',\n",
       " 'is_complex',\n",
       " 'is_conj',\n",
       " 'is_contiguous',\n",
       " 'is_cpu',\n",
       " 'is_cuda',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_inference',\n",
       " 'is_ipu',\n",
       " 'is_leaf',\n",
       " 'is_maia',\n",
       " 'is_meta',\n",
       " 'is_mkldnn',\n",
       " 'is_mps',\n",
       " 'is_mtia',\n",
       " 'is_neg',\n",
       " 'is_nested',\n",
       " 'is_nonzero',\n",
       " 'is_pinned',\n",
       " 'is_quantized',\n",
       " 'is_same_size',\n",
       " 'is_set_to',\n",
       " 'is_shared',\n",
       " 'is_signed',\n",
       " 'is_sparse',\n",
       " 'is_sparse_csr',\n",
       " 'is_vulkan',\n",
       " 'is_xla',\n",
       " 'is_xpu',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'isneginf',\n",
       " 'isposinf',\n",
       " 'isreal',\n",
       " 'istft',\n",
       " 'item',\n",
       " 'itemsize',\n",
       " 'kron',\n",
       " 'kthvalue',\n",
       " 'layout',\n",
       " 'lcm',\n",
       " 'lcm_',\n",
       " 'ldexp',\n",
       " 'ldexp_',\n",
       " 'le',\n",
       " 'le_',\n",
       " 'lerp',\n",
       " 'lerp_',\n",
       " 'less',\n",
       " 'less_',\n",
       " 'less_equal',\n",
       " 'less_equal_',\n",
       " 'lgamma',\n",
       " 'lgamma_',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_normal_',\n",
       " 'log_softmax',\n",
       " 'logaddexp',\n",
       " 'logaddexp2',\n",
       " 'logcumsumexp',\n",
       " 'logdet',\n",
       " 'logical_and',\n",
       " 'logical_and_',\n",
       " 'logical_not',\n",
       " 'logical_not_',\n",
       " 'logical_or',\n",
       " 'logical_or_',\n",
       " 'logical_xor',\n",
       " 'logical_xor_',\n",
       " 'logit',\n",
       " 'logit_',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstsq',\n",
       " 'lt',\n",
       " 'lt_',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'mH',\n",
       " 'mT',\n",
       " 'map2_',\n",
       " 'map_',\n",
       " 'masked_fill',\n",
       " 'masked_fill_',\n",
       " 'masked_scatter',\n",
       " 'masked_scatter_',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_exp',\n",
       " 'matrix_power',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'module_load',\n",
       " 'moveaxis',\n",
       " 'movedim',\n",
       " 'msort',\n",
       " 'mtia',\n",
       " 'mul',\n",
       " 'mul_',\n",
       " 'multinomial',\n",
       " 'multiply',\n",
       " 'multiply_',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'mvlgamma_',\n",
       " 'name',\n",
       " 'names',\n",
       " 'nan_to_num',\n",
       " 'nan_to_num_',\n",
       " 'nanmean',\n",
       " 'nanmedian',\n",
       " 'nanquantile',\n",
       " 'nansum',\n",
       " 'narrow',\n",
       " 'narrow_copy',\n",
       " 'nbytes',\n",
       " 'ndim',\n",
       " 'ndimension',\n",
       " 'ne',\n",
       " 'ne_',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'negative',\n",
       " 'negative_',\n",
       " 'nelement',\n",
       " 'new',\n",
       " 'new_empty',\n",
       " 'new_empty_strided',\n",
       " 'new_full',\n",
       " 'new_ones',\n",
       " 'new_tensor',\n",
       " 'new_zeros',\n",
       " 'nextafter',\n",
       " 'nextafter_',\n",
       " 'nonzero',\n",
       " 'nonzero_static',\n",
       " 'norm',\n",
       " 'normal_',\n",
       " 'not_equal',\n",
       " 'not_equal_',\n",
       " 'numel',\n",
       " 'numpy',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'outer',\n",
       " 'output_nr',\n",
       " 'permute',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'polygamma',\n",
       " 'polygamma_',\n",
       " 'positive',\n",
       " 'pow',\n",
       " 'pow_',\n",
       " 'prelu',\n",
       " 'prod',\n",
       " 'put',\n",
       " 'put_',\n",
       " 'q_per_channel_axis',\n",
       " 'q_per_channel_scales',\n",
       " 'q_per_channel_zero_points',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qr',\n",
       " 'qscheme',\n",
       " 'quantile',\n",
       " 'rad2deg',\n",
       " 'rad2deg_',\n",
       " 'random_',\n",
       " 'ravel',\n",
       " 'real',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'record_stream',\n",
       " 'refine_names',\n",
       " 'register_hook',\n",
       " 'register_post_accumulate_grad_hook',\n",
       " 'reinforce',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'remainder_',\n",
       " 'rename',\n",
       " 'rename_',\n",
       " 'renorm',\n",
       " 'renorm_',\n",
       " 'repeat',\n",
       " 'repeat_interleave',\n",
       " 'requires_grad',\n",
       " 'requires_grad_',\n",
       " 'reshape',\n",
       " 'reshape_as',\n",
       " 'resize',\n",
       " 'resize_',\n",
       " 'resize_as',\n",
       " 'resize_as_',\n",
       " 'resize_as_sparse_',\n",
       " 'resolve_conj',\n",
       " 'resolve_neg',\n",
       " 'retain_grad',\n",
       " 'retains_grad',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'row_indices',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'scatter',\n",
       " 'scatter_',\n",
       " 'scatter_add',\n",
       " 'scatter_add_',\n",
       " 'scatter_reduce',\n",
       " 'scatter_reduce_',\n",
       " 'select',\n",
       " 'select_scatter',\n",
       " 'set_',\n",
       " 'sgn',\n",
       " 'sgn_',\n",
       " 'shape',\n",
       " 'share_memory_',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sign_',\n",
       " 'signbit',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinc',\n",
       " 'sinc_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'size',\n",
       " 'slice_inverse',\n",
       " 'slice_scatter',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse_dim',\n",
       " 'sparse_mask',\n",
       " 'sparse_resize_',\n",
       " 'sparse_resize_and_clear_',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'square',\n",
       " 'square_',\n",
       " 'squeeze',\n",
       " 'squeeze_',\n",
       " 'sspaddmm',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'storage_offset',\n",
       " 'storage_type',\n",
       " 'stride',\n",
       " 'sub',\n",
       " 'sub_',\n",
       " 'subtract',\n",
       " 'subtract_',\n",
       " 'sum',\n",
       " 'sum_to_size',\n",
       " 'svd',\n",
       " 'swapaxes',\n",
       " 'swapaxes_',\n",
       " 'swapdims',\n",
       " 'swapdims_',\n",
       " 'symeig',\n",
       " 't',\n",
       " 't_',\n",
       " 'take',\n",
       " 'take_along_dim',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor_split',\n",
       " 'tile',\n",
       " 'to',\n",
       " 'to_dense',\n",
       " 'to_mkldnn',\n",
       " 'to_padded_tensor',\n",
       " 'to_sparse',\n",
       " 'to_sparse_bsc',\n",
       " 'to_sparse_bsr',\n",
       " 'to_sparse_coo',\n",
       " 'to_sparse_csc',\n",
       " 'to_sparse_csr',\n",
       " 'tolist',\n",
       " 'topk',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'transpose_',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_',\n",
       " 'triu',\n",
       " 'triu_',\n",
       " 'true_divide',\n",
       " 'true_divide_',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'type',\n",
       " 'type_as',\n",
       " 'unbind',\n",
       " 'unflatten',\n",
       " 'unfold',\n",
       " 'uniform_',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsafe_chunk',\n",
       " 'unsafe_split',\n",
       " 'unsafe_split_with_sizes',\n",
       " 'unsqueeze',\n",
       " 'unsqueeze_',\n",
       " 'untyped_storage',\n",
       " 'values',\n",
       " 'var',\n",
       " 'vdot',\n",
       " 'view',\n",
       " 'view_as',\n",
       " 'vsplit',\n",
       " 'where',\n",
       " 'xlogy',\n",
       " 'xlogy_',\n",
       " 'xpu',\n",
       " 'zero_']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "points = torch.tensor([1.0, 1.1, 1.2, 1.3, 1.4])\n",
    "dir(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, tensors are implemented as views over a one-dimensional array of numerical data stored\n",
    "in contiguous chunks of memory. These arrays are called storage instances. Every PyTorch tensor has\n",
    "a storage attribute that can be called to output the underlying storage instance for a tensor, as shown\n",
    "in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/my/_m65k3j97z30x5yp5bmzr44h0000gp/T/ipykernel_8421/767904295.py:2: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  points.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 1.0\n",
       " 1.100000023841858\n",
       " 1.2000000476837158\n",
       " 1.2999999523162842\n",
       " 1.399999976158142\n",
       " 1.5\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[1.0, 1.1], [1.2, 1.3], [1.4, 1.5]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the tensor uses the following information to implement the view:\n",
    "- storage\n",
    "- size\n",
    "- offset\n",
    "- stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# The offset here represents the index of the first element of the tensor in the storage array. Because\n",
    "# the output is 0, it means that the first element of the tensor is the first element in the storage array.\n",
    "points.storage_offset()\n",
    "print(points[1].storage_offset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stride contains, for each dimension, the number of elements to be skipped in order\n",
    "# to access the next element of the tensor.\n",
    "points.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "weights = torch.randn(256, 4) / math.sqrt(256)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(4, requires_grad=True)\n",
    "\n",
    "# or\n",
    "\n",
    "# weights = nn.Linear(256, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.4257, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# very simple neural network with 3 Layer\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 4)\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for param in model.parameters():\n",
    "#         param -= param.grad * 0.01\n",
    "#         model.zero_grad()\n",
    "\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "\n",
    "input = torch.randn(128, 2)\n",
    "target = torch.randn(128, 4)\n",
    "\n",
    "loss = loss_func(model(input), target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1416f6f90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import (TensorDataset, DataLoader)\n",
    "train_dataset = TensorDataset(input, target)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: -0.8342684507369995\n",
      "Loss: -1.210632562637329\n",
      "Loss: -0.13056328892707825\n",
      "Loss: 0.4806225001811981\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in train_dataloader:\n",
    "    pred = model(x_batch)\n",
    "    loss = loss_func(pred, y_batch)\n",
    "    print('Loss:', loss.item())\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a sample Neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: torch==2.6.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final desired architecture:\n",
    "<img src=images/Mnist_arch.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output size of a conv2d: $$\\text{Output size} = \\frac{\\text{Input size} - \\text{Kernel size}}{\\text{Stride}} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer                 | Type                   | Output Shape    |\n",
    "|-----------------------|----------------------|----------------|\n",
    "| **Input**            | -                    | `(1, 28, 28)`  |\n",
    "| **Conv1 (`cn1`)**    | `Conv2d(1, 16, 3, 1)` | `(16, 26, 26)` |\n",
    "| **ReLU**             | Activation           | `(16, 26, 26)` |\n",
    "| **Conv2 (`cn2`)**    | `Conv2d(16, 32, 3, 1)`| `(32, 24, 24)` |\n",
    "| **ReLU**             | Activation           | `(32, 24, 24)` |\n",
    "| **Max Pooling (2x2)**| `F.max_pool2d(x, 2)`  | `(32, 12, 12)` |\n",
    "| **Dropout1 (`dp1`)** | Dropout (10%)        | `(32, 12, 12)` |\n",
    "| **Flatten**          | `torch.flatten(x, 1)` | `(4608,)`      |\n",
    "| **Fully Connected (`fc1`)** | `Linear(4608, 64)` | `(64,)`  |\n",
    "| **ReLU**             | Activation           | `(64,)`        |\n",
    "| **Dropout2 (`dp2`)** | Dropout (25%)        | `(64,)`        |\n",
    "| **Fully Connected (`fc2`)** | `Linear(64, 10)` | `(10,)`  |\n",
    "| **Log Softmax**      | Activation           | `(10,)`        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of parameters in conv layers are determined by:\n",
    "$$Parameters = (kernel_{height} \\times kernel_{width} \\times input_{channels}+1 ) \\times output_{channels}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Dropout2d(0.10),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(4608, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout2d(0.25),\n",
    "    nn.Linear(64, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whatâ€™s the difference between a `Sequential` and a `torch.nn.ModuleList`? A ModuleList is exactly what it sounds likeâ€“a list for storing Module s! On the other hand, the layers in a Sequential are connected in a cascading way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # each batch starts with fresh gradients\n",
    "        optim.zero_grad()\n",
    "        output_prob = model(data)\n",
    "        loss = F.nll_loss(output_prob, target)\n",
    "        loss.backward()\n",
    "        # Update model parameters using the optimizer\n",
    "        optim.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} \\\n",
    "                  [{batch_idx * len(data)}/{len(train_dataloader.dataset)} \\\n",
    "                    ({100. * batch_idx / len(train_dataloader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    # dropout and batchnorm layers are disabled\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            pred_prob = model(data)\n",
    "            test_loss += F.nll_loss(pred_prob, target, reduction='sum').item()\n",
    "            # getting the highest probability prediction\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            # compare predictions to actual target\n",
    "            success += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {success}/{len(test_dataloader.dataset)} \\\n",
    "          ({100. * success / len(test_dataloader.dataset):.0f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "```\n",
    ":\n",
    "we also normalize the dataset to a normal distribution with a specified mean and\n",
    "standard deviation. This mean and standard deviation comes from the training dataset if we\n",
    "are training a model from scratch. However, if we are transfer-learning from a pre-trained\n",
    "model, then the mean and standard deviation values are obtained from the original training\n",
    "dataset of the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device('cpu')\n",
    "model = ConvNet()\n",
    "# Adadelta could be a good choice if we are dealing with sparse data.\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyteatnyte/Dev/github/Algorithms/venv/lib/python3.13/site-packages/torch/nn/functional.py:1538: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1                   [0/60000                     (0%)]\tLoss: 2.301478\n",
      "Train Epoch: 1                   [320/60000                     (1%)]\tLoss: 1.744151\n",
      "Train Epoch: 1                   [640/60000                     (1%)]\tLoss: 1.234419\n",
      "Train Epoch: 1                   [960/60000                     (2%)]\tLoss: 0.831864\n",
      "Train Epoch: 1                   [1280/60000                     (2%)]\tLoss: 0.969944\n",
      "Train Epoch: 1                   [1600/60000                     (3%)]\tLoss: 0.553209\n",
      "Train Epoch: 1                   [1920/60000                     (3%)]\tLoss: 0.627510\n",
      "Train Epoch: 1                   [2240/60000                     (4%)]\tLoss: 0.576939\n",
      "Train Epoch: 1                   [2560/60000                     (4%)]\tLoss: 0.446714\n",
      "Train Epoch: 1                   [2880/60000                     (5%)]\tLoss: 0.378821\n",
      "Train Epoch: 1                   [3200/60000                     (5%)]\tLoss: 0.337835\n",
      "Train Epoch: 1                   [3520/60000                     (6%)]\tLoss: 0.362680\n",
      "Train Epoch: 1                   [3840/60000                     (6%)]\tLoss: 0.318202\n",
      "Train Epoch: 1                   [4160/60000                     (7%)]\tLoss: 0.424434\n",
      "Train Epoch: 1                   [4480/60000                     (7%)]\tLoss: 0.384365\n",
      "Train Epoch: 1                   [4800/60000                     (8%)]\tLoss: 0.470336\n",
      "Train Epoch: 1                   [5120/60000                     (9%)]\tLoss: 0.186225\n",
      "Train Epoch: 1                   [5440/60000                     (9%)]\tLoss: 0.289098\n",
      "Train Epoch: 1                   [5760/60000                     (10%)]\tLoss: 0.130551\n",
      "Train Epoch: 1                   [6080/60000                     (10%)]\tLoss: 0.373286\n",
      "Train Epoch: 1                   [6400/60000                     (11%)]\tLoss: 0.269454\n",
      "Train Epoch: 1                   [6720/60000                     (11%)]\tLoss: 0.079203\n",
      "Train Epoch: 1                   [7040/60000                     (12%)]\tLoss: 0.288895\n",
      "Train Epoch: 1                   [7360/60000                     (12%)]\tLoss: 0.080621\n",
      "Train Epoch: 1                   [7680/60000                     (13%)]\tLoss: 0.266042\n",
      "Train Epoch: 1                   [8000/60000                     (13%)]\tLoss: 0.186528\n",
      "Train Epoch: 1                   [8320/60000                     (14%)]\tLoss: 0.395197\n",
      "Train Epoch: 1                   [8640/60000                     (14%)]\tLoss: 0.090320\n",
      "Train Epoch: 1                   [8960/60000                     (15%)]\tLoss: 0.068758\n",
      "Train Epoch: 1                   [9280/60000                     (15%)]\tLoss: 0.246774\n",
      "Train Epoch: 1                   [9600/60000                     (16%)]\tLoss: 0.224801\n",
      "Train Epoch: 1                   [9920/60000                     (17%)]\tLoss: 0.247637\n",
      "Train Epoch: 1                   [10240/60000                     (17%)]\tLoss: 0.179032\n",
      "Train Epoch: 1                   [10560/60000                     (18%)]\tLoss: 0.500616\n",
      "Train Epoch: 1                   [10880/60000                     (18%)]\tLoss: 0.098835\n",
      "Train Epoch: 1                   [11200/60000                     (19%)]\tLoss: 0.172090\n",
      "Train Epoch: 1                   [11520/60000                     (19%)]\tLoss: 0.314219\n",
      "Train Epoch: 1                   [11840/60000                     (20%)]\tLoss: 0.044964\n",
      "Train Epoch: 1                   [12160/60000                     (20%)]\tLoss: 0.139055\n",
      "Train Epoch: 1                   [12480/60000                     (21%)]\tLoss: 0.048559\n",
      "Train Epoch: 1                   [12800/60000                     (21%)]\tLoss: 0.148498\n",
      "Train Epoch: 1                   [13120/60000                     (22%)]\tLoss: 0.113620\n",
      "Train Epoch: 1                   [13440/60000                     (22%)]\tLoss: 0.093684\n",
      "Train Epoch: 1                   [13760/60000                     (23%)]\tLoss: 0.041878\n",
      "Train Epoch: 1                   [14080/60000                     (23%)]\tLoss: 0.037125\n",
      "Train Epoch: 1                   [14400/60000                     (24%)]\tLoss: 0.117824\n",
      "Train Epoch: 1                   [14720/60000                     (25%)]\tLoss: 0.156898\n",
      "Train Epoch: 1                   [15040/60000                     (25%)]\tLoss: 0.104644\n",
      "Train Epoch: 1                   [15360/60000                     (26%)]\tLoss: 0.180832\n",
      "Train Epoch: 1                   [15680/60000                     (26%)]\tLoss: 0.309479\n",
      "Train Epoch: 1                   [16000/60000                     (27%)]\tLoss: 0.160072\n",
      "Train Epoch: 1                   [16320/60000                     (27%)]\tLoss: 0.063483\n",
      "Train Epoch: 1                   [16640/60000                     (28%)]\tLoss: 0.030786\n",
      "Train Epoch: 1                   [16960/60000                     (28%)]\tLoss: 0.088329\n",
      "Train Epoch: 1                   [17280/60000                     (29%)]\tLoss: 0.055597\n",
      "Train Epoch: 1                   [17600/60000                     (29%)]\tLoss: 0.083759\n",
      "Train Epoch: 1                   [17920/60000                     (30%)]\tLoss: 0.046107\n",
      "Train Epoch: 1                   [18240/60000                     (30%)]\tLoss: 0.042055\n",
      "Train Epoch: 1                   [18560/60000                     (31%)]\tLoss: 0.075683\n",
      "Train Epoch: 1                   [18880/60000                     (31%)]\tLoss: 0.350804\n",
      "Train Epoch: 1                   [19200/60000                     (32%)]\tLoss: 0.121279\n",
      "Train Epoch: 1                   [19520/60000                     (33%)]\tLoss: 0.105879\n",
      "Train Epoch: 1                   [19840/60000                     (33%)]\tLoss: 0.107398\n",
      "Train Epoch: 1                   [20160/60000                     (34%)]\tLoss: 0.067641\n",
      "Train Epoch: 1                   [20480/60000                     (34%)]\tLoss: 0.090563\n",
      "Train Epoch: 1                   [20800/60000                     (35%)]\tLoss: 0.065467\n",
      "Train Epoch: 1                   [21120/60000                     (35%)]\tLoss: 0.379715\n",
      "Train Epoch: 1                   [21440/60000                     (36%)]\tLoss: 0.101872\n",
      "Train Epoch: 1                   [21760/60000                     (36%)]\tLoss: 0.139810\n",
      "Train Epoch: 1                   [22080/60000                     (37%)]\tLoss: 0.065695\n",
      "Train Epoch: 1                   [22400/60000                     (37%)]\tLoss: 0.230191\n",
      "Train Epoch: 1                   [22720/60000                     (38%)]\tLoss: 0.236581\n",
      "Train Epoch: 1                   [23040/60000                     (38%)]\tLoss: 0.027067\n",
      "Train Epoch: 1                   [23360/60000                     (39%)]\tLoss: 0.203361\n",
      "Train Epoch: 1                   [23680/60000                     (39%)]\tLoss: 0.343837\n",
      "Train Epoch: 1                   [24000/60000                     (40%)]\tLoss: 0.084029\n",
      "Train Epoch: 1                   [24320/60000                     (41%)]\tLoss: 0.053355\n",
      "Train Epoch: 1                   [24640/60000                     (41%)]\tLoss: 0.217042\n",
      "Train Epoch: 1                   [24960/60000                     (42%)]\tLoss: 0.056232\n",
      "Train Epoch: 1                   [25280/60000                     (42%)]\tLoss: 0.031154\n",
      "Train Epoch: 1                   [25600/60000                     (43%)]\tLoss: 0.316281\n",
      "Train Epoch: 1                   [25920/60000                     (43%)]\tLoss: 0.518665\n",
      "Train Epoch: 1                   [26240/60000                     (44%)]\tLoss: 0.035091\n",
      "Train Epoch: 1                   [26560/60000                     (44%)]\tLoss: 0.076205\n",
      "Train Epoch: 1                   [26880/60000                     (45%)]\tLoss: 0.152732\n",
      "Train Epoch: 1                   [27200/60000                     (45%)]\tLoss: 0.053142\n",
      "Train Epoch: 1                   [27520/60000                     (46%)]\tLoss: 0.015807\n",
      "Train Epoch: 1                   [27840/60000                     (46%)]\tLoss: 0.109646\n",
      "Train Epoch: 1                   [28160/60000                     (47%)]\tLoss: 0.005179\n",
      "Train Epoch: 1                   [28480/60000                     (47%)]\tLoss: 0.202223\n",
      "Train Epoch: 1                   [28800/60000                     (48%)]\tLoss: 0.108081\n",
      "Train Epoch: 1                   [29120/60000                     (49%)]\tLoss: 0.479661\n",
      "Train Epoch: 1                   [29440/60000                     (49%)]\tLoss: 0.018398\n",
      "Train Epoch: 1                   [29760/60000                     (50%)]\tLoss: 0.092437\n",
      "Train Epoch: 1                   [30080/60000                     (50%)]\tLoss: 0.369978\n",
      "Train Epoch: 1                   [30400/60000                     (51%)]\tLoss: 0.029465\n",
      "Train Epoch: 1                   [30720/60000                     (51%)]\tLoss: 0.057008\n",
      "Train Epoch: 1                   [31040/60000                     (52%)]\tLoss: 0.065981\n",
      "Train Epoch: 1                   [31360/60000                     (52%)]\tLoss: 0.049769\n",
      "Train Epoch: 1                   [31680/60000                     (53%)]\tLoss: 0.024130\n",
      "Train Epoch: 1                   [32000/60000                     (53%)]\tLoss: 0.003086\n",
      "Train Epoch: 1                   [32320/60000                     (54%)]\tLoss: 0.058487\n",
      "Train Epoch: 1                   [32640/60000                     (54%)]\tLoss: 0.232613\n",
      "Train Epoch: 1                   [32960/60000                     (55%)]\tLoss: 0.006225\n",
      "Train Epoch: 1                   [33280/60000                     (55%)]\tLoss: 0.421996\n",
      "Train Epoch: 1                   [33600/60000                     (56%)]\tLoss: 0.128025\n",
      "Train Epoch: 1                   [33920/60000                     (57%)]\tLoss: 0.078332\n",
      "Train Epoch: 1                   [34240/60000                     (57%)]\tLoss: 0.038251\n",
      "Train Epoch: 1                   [34560/60000                     (58%)]\tLoss: 0.010043\n",
      "Train Epoch: 1                   [34880/60000                     (58%)]\tLoss: 0.090680\n",
      "Train Epoch: 1                   [35200/60000                     (59%)]\tLoss: 0.091606\n",
      "Train Epoch: 1                   [35520/60000                     (59%)]\tLoss: 0.083667\n",
      "Train Epoch: 1                   [35840/60000                     (60%)]\tLoss: 0.024100\n",
      "Train Epoch: 1                   [36160/60000                     (60%)]\tLoss: 0.017545\n",
      "Train Epoch: 1                   [36480/60000                     (61%)]\tLoss: 0.063018\n",
      "Train Epoch: 1                   [36800/60000                     (61%)]\tLoss: 0.154319\n",
      "Train Epoch: 1                   [37120/60000                     (62%)]\tLoss: 0.185728\n",
      "Train Epoch: 1                   [37440/60000                     (62%)]\tLoss: 0.152526\n",
      "Train Epoch: 1                   [37760/60000                     (63%)]\tLoss: 0.012324\n",
      "Train Epoch: 1                   [38080/60000                     (63%)]\tLoss: 0.100100\n",
      "Train Epoch: 1                   [38400/60000                     (64%)]\tLoss: 0.114238\n",
      "Train Epoch: 1                   [38720/60000                     (65%)]\tLoss: 0.239765\n",
      "Train Epoch: 1                   [39040/60000                     (65%)]\tLoss: 0.022144\n",
      "Train Epoch: 1                   [39360/60000                     (66%)]\tLoss: 0.316773\n",
      "Train Epoch: 1                   [39680/60000                     (66%)]\tLoss: 0.031453\n",
      "Train Epoch: 1                   [40000/60000                     (67%)]\tLoss: 0.103417\n",
      "Train Epoch: 1                   [40320/60000                     (67%)]\tLoss: 0.058461\n",
      "Train Epoch: 1                   [40640/60000                     (68%)]\tLoss: 0.191510\n",
      "Train Epoch: 1                   [40960/60000                     (68%)]\tLoss: 0.157104\n",
      "Train Epoch: 1                   [41280/60000                     (69%)]\tLoss: 0.304523\n",
      "Train Epoch: 1                   [41600/60000                     (69%)]\tLoss: 0.160950\n",
      "Train Epoch: 1                   [41920/60000                     (70%)]\tLoss: 0.043875\n",
      "Train Epoch: 1                   [42240/60000                     (70%)]\tLoss: 0.008179\n",
      "Train Epoch: 1                   [42560/60000                     (71%)]\tLoss: 0.016439\n",
      "Train Epoch: 1                   [42880/60000                     (71%)]\tLoss: 0.125389\n",
      "Train Epoch: 1                   [43200/60000                     (72%)]\tLoss: 0.042035\n",
      "Train Epoch: 1                   [43520/60000                     (73%)]\tLoss: 0.156890\n",
      "Train Epoch: 1                   [43840/60000                     (73%)]\tLoss: 0.112813\n",
      "Train Epoch: 1                   [44160/60000                     (74%)]\tLoss: 0.135779\n",
      "Train Epoch: 1                   [44480/60000                     (74%)]\tLoss: 0.073559\n",
      "Train Epoch: 1                   [44800/60000                     (75%)]\tLoss: 0.267357\n",
      "Train Epoch: 1                   [45120/60000                     (75%)]\tLoss: 0.077661\n",
      "Train Epoch: 1                   [45440/60000                     (76%)]\tLoss: 0.154593\n",
      "Train Epoch: 1                   [45760/60000                     (76%)]\tLoss: 0.009740\n",
      "Train Epoch: 1                   [46080/60000                     (77%)]\tLoss: 0.478159\n",
      "Train Epoch: 1                   [46400/60000                     (77%)]\tLoss: 0.066206\n",
      "Train Epoch: 1                   [46720/60000                     (78%)]\tLoss: 0.209078\n",
      "Train Epoch: 1                   [47040/60000                     (78%)]\tLoss: 0.062023\n",
      "Train Epoch: 1                   [47360/60000                     (79%)]\tLoss: 0.135709\n",
      "Train Epoch: 1                   [47680/60000                     (79%)]\tLoss: 0.113449\n",
      "Train Epoch: 1                   [48000/60000                     (80%)]\tLoss: 0.159229\n",
      "Train Epoch: 1                   [48320/60000                     (81%)]\tLoss: 0.168876\n",
      "Train Epoch: 1                   [48640/60000                     (81%)]\tLoss: 0.019429\n",
      "Train Epoch: 1                   [48960/60000                     (82%)]\tLoss: 0.010621\n",
      "Train Epoch: 1                   [49280/60000                     (82%)]\tLoss: 0.201216\n",
      "Train Epoch: 1                   [49600/60000                     (83%)]\tLoss: 0.078317\n",
      "Train Epoch: 1                   [49920/60000                     (83%)]\tLoss: 0.008731\n",
      "Train Epoch: 1                   [50240/60000                     (84%)]\tLoss: 0.110615\n",
      "Train Epoch: 1                   [50560/60000                     (84%)]\tLoss: 0.003093\n",
      "Train Epoch: 1                   [50880/60000                     (85%)]\tLoss: 0.011122\n",
      "Train Epoch: 1                   [51200/60000                     (85%)]\tLoss: 0.301765\n",
      "Train Epoch: 1                   [51520/60000                     (86%)]\tLoss: 0.006589\n",
      "Train Epoch: 1                   [51840/60000                     (86%)]\tLoss: 0.030179\n",
      "Train Epoch: 1                   [52160/60000                     (87%)]\tLoss: 0.015812\n",
      "Train Epoch: 1                   [52480/60000                     (87%)]\tLoss: 0.026616\n",
      "Train Epoch: 1                   [52800/60000                     (88%)]\tLoss: 0.050278\n",
      "Train Epoch: 1                   [53120/60000                     (89%)]\tLoss: 0.012629\n",
      "Train Epoch: 1                   [53440/60000                     (89%)]\tLoss: 0.063350\n",
      "Train Epoch: 1                   [53760/60000                     (90%)]\tLoss: 0.095081\n",
      "Train Epoch: 1                   [54080/60000                     (90%)]\tLoss: 0.020215\n",
      "Train Epoch: 1                   [54400/60000                     (91%)]\tLoss: 0.107523\n",
      "Train Epoch: 1                   [54720/60000                     (91%)]\tLoss: 0.066372\n",
      "Train Epoch: 1                   [55040/60000                     (92%)]\tLoss: 0.003276\n",
      "Train Epoch: 1                   [55360/60000                     (92%)]\tLoss: 0.075881\n",
      "Train Epoch: 1                   [55680/60000                     (93%)]\tLoss: 0.015081\n",
      "Train Epoch: 1                   [56000/60000                     (93%)]\tLoss: 0.172304\n",
      "Train Epoch: 1                   [56320/60000                     (94%)]\tLoss: 0.047717\n",
      "Train Epoch: 1                   [56640/60000                     (94%)]\tLoss: 0.082685\n",
      "Train Epoch: 1                   [56960/60000                     (95%)]\tLoss: 0.029626\n",
      "Train Epoch: 1                   [57280/60000                     (95%)]\tLoss: 0.063506\n",
      "Train Epoch: 1                   [57600/60000                     (96%)]\tLoss: 0.033575\n",
      "Train Epoch: 1                   [57920/60000                     (97%)]\tLoss: 0.063062\n",
      "Train Epoch: 1                   [58240/60000                     (97%)]\tLoss: 0.043786\n",
      "Train Epoch: 1                   [58560/60000                     (98%)]\tLoss: 0.018516\n",
      "Train Epoch: 1                   [58880/60000                     (98%)]\tLoss: 0.044078\n",
      "Train Epoch: 1                   [59200/60000                     (99%)]\tLoss: 0.005494\n",
      "Train Epoch: 1                   [59520/60000                     (99%)]\tLoss: 0.026589\n",
      "Train Epoch: 1                   [59840/60000                     (100%)]\tLoss: 0.048349\n",
      "\n",
      "Test set: Average loss: 0.0491, Accuracy: 9835/10000           (98%)\n",
      "Train Epoch: 2                   [0/60000                     (0%)]\tLoss: 0.063484\n",
      "Train Epoch: 2                   [320/60000                     (1%)]\tLoss: 0.039784\n",
      "Train Epoch: 2                   [640/60000                     (1%)]\tLoss: 0.195712\n",
      "Train Epoch: 2                   [960/60000                     (2%)]\tLoss: 0.117099\n",
      "Train Epoch: 2                   [1280/60000                     (2%)]\tLoss: 0.047198\n",
      "Train Epoch: 2                   [1600/60000                     (3%)]\tLoss: 0.014882\n",
      "Train Epoch: 2                   [1920/60000                     (3%)]\tLoss: 0.040512\n",
      "Train Epoch: 2                   [2240/60000                     (4%)]\tLoss: 0.001546\n",
      "Train Epoch: 2                   [2560/60000                     (4%)]\tLoss: 0.009847\n",
      "Train Epoch: 2                   [2880/60000                     (5%)]\tLoss: 0.078043\n",
      "Train Epoch: 2                   [3200/60000                     (5%)]\tLoss: 0.013498\n",
      "Train Epoch: 2                   [3520/60000                     (6%)]\tLoss: 0.164202\n",
      "Train Epoch: 2                   [3840/60000                     (6%)]\tLoss: 0.089391\n",
      "Train Epoch: 2                   [4160/60000                     (7%)]\tLoss: 0.181953\n",
      "Train Epoch: 2                   [4480/60000                     (7%)]\tLoss: 0.000422\n",
      "Train Epoch: 2                   [4800/60000                     (8%)]\tLoss: 0.025706\n",
      "Train Epoch: 2                   [5120/60000                     (9%)]\tLoss: 0.276707\n",
      "Train Epoch: 2                   [5440/60000                     (9%)]\tLoss: 0.171957\n",
      "Train Epoch: 2                   [5760/60000                     (10%)]\tLoss: 0.006266\n",
      "Train Epoch: 2                   [6080/60000                     (10%)]\tLoss: 0.208084\n",
      "Train Epoch: 2                   [6400/60000                     (11%)]\tLoss: 0.082103\n",
      "Train Epoch: 2                   [6720/60000                     (11%)]\tLoss: 0.136626\n",
      "Train Epoch: 2                   [7040/60000                     (12%)]\tLoss: 0.119682\n",
      "Train Epoch: 2                   [7360/60000                     (12%)]\tLoss: 0.065843\n",
      "Train Epoch: 2                   [7680/60000                     (13%)]\tLoss: 0.124651\n",
      "Train Epoch: 2                   [8000/60000                     (13%)]\tLoss: 0.051093\n",
      "Train Epoch: 2                   [8320/60000                     (14%)]\tLoss: 0.021817\n",
      "Train Epoch: 2                   [8640/60000                     (14%)]\tLoss: 0.073141\n",
      "Train Epoch: 2                   [8960/60000                     (15%)]\tLoss: 0.441978\n",
      "Train Epoch: 2                   [9280/60000                     (15%)]\tLoss: 0.111521\n",
      "Train Epoch: 2                   [9600/60000                     (16%)]\tLoss: 0.007829\n",
      "Train Epoch: 2                   [9920/60000                     (17%)]\tLoss: 0.004737\n",
      "Train Epoch: 2                   [10240/60000                     (17%)]\tLoss: 0.007602\n",
      "Train Epoch: 2                   [10560/60000                     (18%)]\tLoss: 0.071552\n",
      "Train Epoch: 2                   [10880/60000                     (18%)]\tLoss: 0.020272\n",
      "Train Epoch: 2                   [11200/60000                     (19%)]\tLoss: 0.018194\n",
      "Train Epoch: 2                   [11520/60000                     (19%)]\tLoss: 0.060438\n",
      "Train Epoch: 2                   [11840/60000                     (20%)]\tLoss: 0.021399\n",
      "Train Epoch: 2                   [12160/60000                     (20%)]\tLoss: 0.024211\n",
      "Train Epoch: 2                   [12480/60000                     (21%)]\tLoss: 0.045710\n",
      "Train Epoch: 2                   [12800/60000                     (21%)]\tLoss: 0.052033\n",
      "Train Epoch: 2                   [13120/60000                     (22%)]\tLoss: 0.182636\n",
      "Train Epoch: 2                   [13440/60000                     (22%)]\tLoss: 0.031412\n",
      "Train Epoch: 2                   [13760/60000                     (23%)]\tLoss: 0.019119\n",
      "Train Epoch: 2                   [14080/60000                     (23%)]\tLoss: 0.039841\n",
      "Train Epoch: 2                   [14400/60000                     (24%)]\tLoss: 0.030542\n",
      "Train Epoch: 2                   [14720/60000                     (25%)]\tLoss: 0.032932\n",
      "Train Epoch: 2                   [15040/60000                     (25%)]\tLoss: 0.044408\n",
      "Train Epoch: 2                   [15360/60000                     (26%)]\tLoss: 0.177658\n",
      "Train Epoch: 2                   [15680/60000                     (26%)]\tLoss: 0.013357\n",
      "Train Epoch: 2                   [16000/60000                     (27%)]\tLoss: 0.048883\n",
      "Train Epoch: 2                   [16320/60000                     (27%)]\tLoss: 0.024317\n",
      "Train Epoch: 2                   [16640/60000                     (28%)]\tLoss: 0.046076\n",
      "Train Epoch: 2                   [16960/60000                     (28%)]\tLoss: 0.008777\n",
      "Train Epoch: 2                   [17280/60000                     (29%)]\tLoss: 0.044368\n",
      "Train Epoch: 2                   [17600/60000                     (29%)]\tLoss: 0.134910\n",
      "Train Epoch: 2                   [17920/60000                     (30%)]\tLoss: 0.001354\n",
      "Train Epoch: 2                   [18240/60000                     (30%)]\tLoss: 0.040698\n",
      "Train Epoch: 2                   [18560/60000                     (31%)]\tLoss: 0.012824\n",
      "Train Epoch: 2                   [18880/60000                     (31%)]\tLoss: 0.018079\n",
      "Train Epoch: 2                   [19200/60000                     (32%)]\tLoss: 0.017808\n",
      "Train Epoch: 2                   [19520/60000                     (33%)]\tLoss: 0.010137\n",
      "Train Epoch: 2                   [19840/60000                     (33%)]\tLoss: 0.189845\n",
      "Train Epoch: 2                   [20160/60000                     (34%)]\tLoss: 0.013035\n",
      "Train Epoch: 2                   [20480/60000                     (34%)]\tLoss: 0.005195\n",
      "Train Epoch: 2                   [20800/60000                     (35%)]\tLoss: 0.012008\n",
      "Train Epoch: 2                   [21120/60000                     (35%)]\tLoss: 0.048538\n",
      "Train Epoch: 2                   [21440/60000                     (36%)]\tLoss: 0.018056\n",
      "Train Epoch: 2                   [21760/60000                     (36%)]\tLoss: 0.032908\n",
      "Train Epoch: 2                   [22080/60000                     (37%)]\tLoss: 0.061946\n",
      "Train Epoch: 2                   [22400/60000                     (37%)]\tLoss: 0.001856\n",
      "Train Epoch: 2                   [22720/60000                     (38%)]\tLoss: 0.007859\n",
      "Train Epoch: 2                   [23040/60000                     (38%)]\tLoss: 0.025621\n",
      "Train Epoch: 2                   [23360/60000                     (39%)]\tLoss: 0.007869\n",
      "Train Epoch: 2                   [23680/60000                     (39%)]\tLoss: 0.010558\n",
      "Train Epoch: 2                   [24000/60000                     (40%)]\tLoss: 0.089654\n",
      "Train Epoch: 2                   [24320/60000                     (41%)]\tLoss: 0.001356\n",
      "Train Epoch: 2                   [24640/60000                     (41%)]\tLoss: 0.039122\n",
      "Train Epoch: 2                   [24960/60000                     (42%)]\tLoss: 0.110788\n",
      "Train Epoch: 2                   [25280/60000                     (42%)]\tLoss: 0.005459\n",
      "Train Epoch: 2                   [25600/60000                     (43%)]\tLoss: 0.006762\n",
      "Train Epoch: 2                   [25920/60000                     (43%)]\tLoss: 0.003032\n",
      "Train Epoch: 2                   [26240/60000                     (44%)]\tLoss: 0.059276\n",
      "Train Epoch: 2                   [26560/60000                     (44%)]\tLoss: 0.035565\n",
      "Train Epoch: 2                   [26880/60000                     (45%)]\tLoss: 0.023390\n",
      "Train Epoch: 2                   [27200/60000                     (45%)]\tLoss: 0.008466\n",
      "Train Epoch: 2                   [27520/60000                     (46%)]\tLoss: 0.006617\n",
      "Train Epoch: 2                   [27840/60000                     (46%)]\tLoss: 0.043703\n",
      "Train Epoch: 2                   [28160/60000                     (47%)]\tLoss: 0.112623\n",
      "Train Epoch: 2                   [28480/60000                     (47%)]\tLoss: 0.064886\n",
      "Train Epoch: 2                   [28800/60000                     (48%)]\tLoss: 0.010229\n",
      "Train Epoch: 2                   [29120/60000                     (49%)]\tLoss: 0.041330\n",
      "Train Epoch: 2                   [29440/60000                     (49%)]\tLoss: 0.097618\n",
      "Train Epoch: 2                   [29760/60000                     (50%)]\tLoss: 0.019152\n",
      "Train Epoch: 2                   [30080/60000                     (50%)]\tLoss: 0.003601\n",
      "Train Epoch: 2                   [30400/60000                     (51%)]\tLoss: 0.063127\n",
      "Train Epoch: 2                   [30720/60000                     (51%)]\tLoss: 0.384330\n",
      "Train Epoch: 2                   [31040/60000                     (52%)]\tLoss: 0.004449\n",
      "Train Epoch: 2                   [31360/60000                     (52%)]\tLoss: 0.023555\n",
      "Train Epoch: 2                   [31680/60000                     (53%)]\tLoss: 0.036049\n",
      "Train Epoch: 2                   [32000/60000                     (53%)]\tLoss: 0.212626\n",
      "Train Epoch: 2                   [32320/60000                     (54%)]\tLoss: 0.042870\n",
      "Train Epoch: 2                   [32640/60000                     (54%)]\tLoss: 0.132386\n",
      "Train Epoch: 2                   [32960/60000                     (55%)]\tLoss: 0.022134\n",
      "Train Epoch: 2                   [33280/60000                     (55%)]\tLoss: 0.020621\n",
      "Train Epoch: 2                   [33600/60000                     (56%)]\tLoss: 0.395675\n",
      "Train Epoch: 2                   [33920/60000                     (57%)]\tLoss: 0.069880\n",
      "Train Epoch: 2                   [34240/60000                     (57%)]\tLoss: 0.050026\n",
      "Train Epoch: 2                   [34560/60000                     (58%)]\tLoss: 0.019411\n",
      "Train Epoch: 2                   [34880/60000                     (58%)]\tLoss: 0.019894\n",
      "Train Epoch: 2                   [35200/60000                     (59%)]\tLoss: 0.031352\n",
      "Train Epoch: 2                   [35520/60000                     (59%)]\tLoss: 0.010547\n",
      "Train Epoch: 2                   [35840/60000                     (60%)]\tLoss: 0.006944\n",
      "Train Epoch: 2                   [36160/60000                     (60%)]\tLoss: 0.142385\n",
      "Train Epoch: 2                   [36480/60000                     (61%)]\tLoss: 0.088084\n",
      "Train Epoch: 2                   [36800/60000                     (61%)]\tLoss: 0.030451\n",
      "Train Epoch: 2                   [37120/60000                     (62%)]\tLoss: 0.362079\n",
      "Train Epoch: 2                   [37440/60000                     (62%)]\tLoss: 0.062161\n",
      "Train Epoch: 2                   [37760/60000                     (63%)]\tLoss: 0.191499\n",
      "Train Epoch: 2                   [38080/60000                     (63%)]\tLoss: 0.074916\n",
      "Train Epoch: 2                   [38400/60000                     (64%)]\tLoss: 0.097933\n",
      "Train Epoch: 2                   [38720/60000                     (65%)]\tLoss: 0.019903\n",
      "Train Epoch: 2                   [39040/60000                     (65%)]\tLoss: 0.010658\n",
      "Train Epoch: 2                   [39360/60000                     (66%)]\tLoss: 0.033019\n",
      "Train Epoch: 2                   [39680/60000                     (66%)]\tLoss: 0.001407\n",
      "Train Epoch: 2                   [40000/60000                     (67%)]\tLoss: 0.133222\n",
      "Train Epoch: 2                   [40320/60000                     (67%)]\tLoss: 0.050011\n",
      "Train Epoch: 2                   [40640/60000                     (68%)]\tLoss: 0.007024\n",
      "Train Epoch: 2                   [40960/60000                     (68%)]\tLoss: 0.002925\n",
      "Train Epoch: 2                   [41280/60000                     (69%)]\tLoss: 0.027031\n",
      "Train Epoch: 2                   [41600/60000                     (69%)]\tLoss: 0.023323\n",
      "Train Epoch: 2                   [41920/60000                     (70%)]\tLoss: 0.001528\n",
      "Train Epoch: 2                   [42240/60000                     (70%)]\tLoss: 0.000246\n",
      "Train Epoch: 2                   [42560/60000                     (71%)]\tLoss: 0.008346\n",
      "Train Epoch: 2                   [42880/60000                     (71%)]\tLoss: 0.042852\n",
      "Train Epoch: 2                   [43200/60000                     (72%)]\tLoss: 0.026238\n",
      "Train Epoch: 2                   [43520/60000                     (73%)]\tLoss: 0.086446\n",
      "Train Epoch: 2                   [43840/60000                     (73%)]\tLoss: 0.009028\n",
      "Train Epoch: 2                   [44160/60000                     (74%)]\tLoss: 0.141427\n",
      "Train Epoch: 2                   [44480/60000                     (74%)]\tLoss: 0.013132\n",
      "Train Epoch: 2                   [44800/60000                     (75%)]\tLoss: 0.023973\n",
      "Train Epoch: 2                   [45120/60000                     (75%)]\tLoss: 0.002288\n",
      "Train Epoch: 2                   [45440/60000                     (76%)]\tLoss: 0.084714\n",
      "Train Epoch: 2                   [45760/60000                     (76%)]\tLoss: 0.071507\n",
      "Train Epoch: 2                   [46080/60000                     (77%)]\tLoss: 0.141065\n",
      "Train Epoch: 2                   [46400/60000                     (77%)]\tLoss: 0.267571\n",
      "Train Epoch: 2                   [46720/60000                     (78%)]\tLoss: 0.072223\n",
      "Train Epoch: 2                   [47040/60000                     (78%)]\tLoss: 0.002962\n",
      "Train Epoch: 2                   [47360/60000                     (79%)]\tLoss: 0.008598\n",
      "Train Epoch: 2                   [47680/60000                     (79%)]\tLoss: 0.131633\n",
      "Train Epoch: 2                   [48000/60000                     (80%)]\tLoss: 0.285689\n",
      "Train Epoch: 2                   [48320/60000                     (81%)]\tLoss: 0.025189\n",
      "Train Epoch: 2                   [48640/60000                     (81%)]\tLoss: 0.006479\n",
      "Train Epoch: 2                   [48960/60000                     (82%)]\tLoss: 0.111887\n",
      "Train Epoch: 2                   [49280/60000                     (82%)]\tLoss: 0.075296\n",
      "Train Epoch: 2                   [49600/60000                     (83%)]\tLoss: 0.087534\n",
      "Train Epoch: 2                   [49920/60000                     (83%)]\tLoss: 0.036029\n",
      "Train Epoch: 2                   [50240/60000                     (84%)]\tLoss: 0.090669\n",
      "Train Epoch: 2                   [50560/60000                     (84%)]\tLoss: 0.004787\n",
      "Train Epoch: 2                   [50880/60000                     (85%)]\tLoss: 0.406628\n",
      "Train Epoch: 2                   [51200/60000                     (85%)]\tLoss: 0.002603\n",
      "Train Epoch: 2                   [51520/60000                     (86%)]\tLoss: 0.013166\n",
      "Train Epoch: 2                   [51840/60000                     (86%)]\tLoss: 0.490425\n",
      "Train Epoch: 2                   [52160/60000                     (87%)]\tLoss: 0.026260\n",
      "Train Epoch: 2                   [52480/60000                     (87%)]\tLoss: 0.073466\n",
      "Train Epoch: 2                   [52800/60000                     (88%)]\tLoss: 0.312811\n",
      "Train Epoch: 2                   [53120/60000                     (89%)]\tLoss: 0.018275\n",
      "Train Epoch: 2                   [53440/60000                     (89%)]\tLoss: 0.143099\n",
      "Train Epoch: 2                   [53760/60000                     (90%)]\tLoss: 0.043657\n",
      "Train Epoch: 2                   [54080/60000                     (90%)]\tLoss: 0.029120\n",
      "Train Epoch: 2                   [54400/60000                     (91%)]\tLoss: 0.143278\n",
      "Train Epoch: 2                   [54720/60000                     (91%)]\tLoss: 0.315735\n",
      "Train Epoch: 2                   [55040/60000                     (92%)]\tLoss: 0.024043\n",
      "Train Epoch: 2                   [55360/60000                     (92%)]\tLoss: 0.004260\n",
      "Train Epoch: 2                   [55680/60000                     (93%)]\tLoss: 0.172337\n",
      "Train Epoch: 2                   [56000/60000                     (93%)]\tLoss: 0.041465\n",
      "Train Epoch: 2                   [56320/60000                     (94%)]\tLoss: 0.164921\n",
      "Train Epoch: 2                   [56640/60000                     (94%)]\tLoss: 0.378410\n",
      "Train Epoch: 2                   [56960/60000                     (95%)]\tLoss: 0.002229\n",
      "Train Epoch: 2                   [57280/60000                     (95%)]\tLoss: 0.008381\n",
      "Train Epoch: 2                   [57600/60000                     (96%)]\tLoss: 0.103596\n",
      "Train Epoch: 2                   [57920/60000                     (97%)]\tLoss: 0.010686\n",
      "Train Epoch: 2                   [58240/60000                     (97%)]\tLoss: 0.069567\n",
      "Train Epoch: 2                   [58560/60000                     (98%)]\tLoss: 0.012537\n",
      "Train Epoch: 2                   [58880/60000                     (98%)]\tLoss: 0.004968\n",
      "Train Epoch: 2                   [59200/60000                     (99%)]\tLoss: 0.251686\n",
      "Train Epoch: 2                   [59520/60000                     (99%)]\tLoss: 0.090526\n",
      "Train Epoch: 2                   [59840/60000                     (100%)]\tLoss: 0.006218\n",
      "\n",
      "Test set: Average loss: 0.0398, Accuracy: 9860/10000           (99%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 3):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14718e510>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "plt.imshow(sample_data[0][0],\n",
    "    cmap='gray', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 7\n",
      "Ground truth is : 7\n"
     ]
    }
   ],
   "source": [
    "# The forward pass of the neural network done using\n",
    "# model() produces probabilities. Hence, we use the max() function to output the class with the\n",
    "# maximum probability.\n",
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, for predictions, we first calculate the class with maximum probability using the max()\n",
    "function on axis=1. The max() function outputs two lists â€“ a list of probabilities of classes for\n",
    "every sample in sample_data and a list of class labels for each sample. Hence, we choose the\n",
    "second list using index [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([ 0.0000e+00, -5.7220e-06, -8.7022e-06, -1.4305e-05, -4.2915e-06,\n",
      "        -5.8412e-06, -7.7483e-05, -4.0676e-03, -1.1091e-02, -4.5031e-03,\n",
      "        -2.3842e-07, -5.0068e-06, -1.4304e-04, -5.9605e-07, -3.9339e-06,\n",
      "        -1.8482e-03, -2.1934e-05, -3.5763e-07, -9.6553e-03, -7.1526e-07,\n",
      "        -4.0265e-03, -1.4186e-05, -2.4199e-05, -7.1526e-07, -3.3378e-05,\n",
      "        -5.0068e-06, -7.3909e-06, -1.1921e-07, -1.7881e-06, -8.4638e-06,\n",
      "        -9.5367e-07, -8.0696e-04, -3.5763e-07, -1.5491e-02, -5.9604e-06,\n",
      "        -3.9339e-06, -1.7153e-04, -6.3181e-06, -1.1206e-05, -1.5497e-06,\n",
      "        -3.2544e-05, -1.6366e-04, -2.8463e-04, -1.7207e-03, -1.8333e-04,\n",
      "        -1.7285e-05, -1.3714e-03, -7.1526e-07, -3.9219e-05, -3.5763e-07,\n",
      "        -5.9604e-06, -1.5616e-05, -3.5763e-07, -9.6674e-05, -5.1260e-06,\n",
      "        -2.3842e-07,  0.0000e+00, -1.1802e-05, -2.7418e-05, -8.1351e-04,\n",
      "        -2.7418e-06, -5.0543e-05, -8.4364e-02, -1.5560e-03, -2.1458e-06,\n",
      "        -1.1719e-03, -2.6703e-05, -1.0490e-05, -5.9605e-07, -5.6028e-06,\n",
      "        -1.3113e-06,  0.0000e+00, -3.8187e-04, -2.2598e-02, -9.5367e-06,\n",
      "        -1.6689e-06, -8.3446e-07, -1.1921e-05, -2.9003e-02, -7.8678e-06,\n",
      "        -1.5937e-04, -3.7908e-05, -1.1921e-07, -1.1921e-07, -3.0994e-06,\n",
      "        -4.1723e-06, -1.0967e-05, -3.5231e-03, -4.0531e-06, -1.7047e-05,\n",
      "        -2.3842e-07, -3.5763e-07, -6.0014e-01, -5.4835e-05, -2.1100e-05,\n",
      "        -2.2974e-03, -2.0405e-02, -1.3169e-03, -1.7129e-04, -3.9339e-06,\n",
      "        -1.7881e-06, -1.1921e-07, -1.0729e-05,  0.0000e+00, -5.5488e-04,\n",
      "        -1.9193e-05, -1.7285e-05, -1.7439e-03, -3.8557e-04,  0.0000e+00,\n",
      "        -3.9339e-06, -2.0982e-02, -1.0729e-06, -5.7220e-06, -8.1893e-05,\n",
      "        -2.5441e-01, -1.5091e-04, -2.8610e-06, -1.5303e-03, -1.2831e-02,\n",
      "        -1.4376e-04, -7.2489e-04, -3.5763e-06, -3.5763e-07, -7.8135e-04,\n",
      "        -2.5666e-02, -2.2514e-03, -8.3446e-07, -1.1921e-07, -2.3842e-07,\n",
      "        -1.9073e-05, -1.0014e-05, -6.9973e-05, -1.0001e-04, -1.1921e-07,\n",
      "        -5.9605e-07, -4.7684e-07, -6.3181e-06, -2.3842e-07, -2.2866e-03,\n",
      "        -3.3616e-05, -1.1921e-07, -2.8118e-04, -2.0266e-06, -8.5830e-06,\n",
      "        -1.9669e-05, -8.3446e-07, -1.0729e-06,  0.0000e+00, -5.0306e-03,\n",
      "        -1.5139e-05, -1.1886e-01, -1.0729e-06, -8.2254e-06, -1.2695e-04,\n",
      "        -1.9073e-06, -7.1525e-06, -2.9802e-06, -3.6894e-03, -1.0573e-04,\n",
      "        -6.3745e-04, -9.5367e-07,  0.0000e+00, -1.9073e-06, -2.7895e-05,\n",
      "        -9.4175e-06,  0.0000e+00, -3.1327e-03, -4.7684e-07, -1.7881e-06,\n",
      "        -5.9605e-07, -6.5543e-03, -8.4638e-06, -8.7022e-06, -1.7881e-06,\n",
      "        -4.2466e-02, -1.2517e-05, -4.2915e-06, -1.5497e-06, -4.3323e-04,\n",
      "        -6.5565e-06, -7.0333e-06, -3.0875e-05,  0.0000e+00, -1.4548e-01,\n",
      "        -1.1803e-03, -5.4836e-06, -4.4941e-05, -1.1921e-07, -5.8419e-04,\n",
      "        -3.3087e-04, -8.9772e-04, -4.7684e-07, -3.1189e-03, -1.1921e-06,\n",
      "        -2.3988e-03, -1.6928e-05, -1.6332e-05, -1.0729e-06, -1.7070e-03,\n",
      "        -4.1961e-05, -6.6757e-06, -4.7684e-06, -1.8357e-04, -8.5830e-06,\n",
      "        -3.5763e-07, -6.0504e-04, -1.1777e-04, -2.5034e-06, -3.6795e-02,\n",
      "        -2.0266e-06, -6.2909e-03, -3.1113e-05, -7.0333e-06, -2.5736e-03,\n",
      "        -9.1791e-06, -1.1921e-07, -7.6849e-02, -1.6689e-06, -2.3842e-06,\n",
      "        -3.1471e-05, -1.9896e-03, -1.1444e-05, -4.7684e-06, -3.4755e-04,\n",
      "        -2.5868e-05, -1.4221e-04, -4.5179e-05, -3.4571e-06, -4.4226e-05,\n",
      "        -5.7299e-04, -1.1444e-05, -7.3585e-04, -1.2994e-05, -5.8172e-05,\n",
      "        -1.8202e-04, -2.0266e-06, -1.3232e-05, -1.1921e-07, -4.8876e-06,\n",
      "        -5.7696e-05, -9.4095e-04, -1.3970e-04, -4.5895e-05, -9.6430e-04,\n",
      "        -1.3453e-03, -3.5763e-07, -5.9739e-01, -1.0252e-05, -4.8875e-05,\n",
      "        -3.9069e-04, -1.8308e-03, -1.7093e-04, -2.3842e-06, -3.8147e-06,\n",
      "        -1.4185e-04, -6.8543e-05, -7.2238e-03, -1.7881e-06, -5.7824e-02,\n",
      "         0.0000e+00, -4.0531e-06,  0.0000e+00, -8.3446e-07, -7.1076e-02,\n",
      "        -1.4305e-06, -6.1751e-02, -2.1458e-06, -5.6739e-04, -3.9339e-06,\n",
      "        -1.6689e-06, -4.2915e-06, -7.0333e-06, -2.4661e-04, -9.4409e-05,\n",
      "        -1.0729e-06, -1.1921e-07, -1.0133e-05, -5.9604e-06, -1.9908e-05,\n",
      "        -5.1735e-05, -5.4836e-06, -9.7886e-02, -9.5367e-07, -4.2676e-05,\n",
      "         0.0000e+00, -1.9703e-04, -1.2437e-03, -5.7220e-06, -3.0339e-03,\n",
      "        -3.3682e-01, -9.3217e-05, -4.2594e-03, -1.3113e-06, -2.3842e-07,\n",
      "        -2.9683e-05,  0.0000e+00, -2.2530e-05, -4.4107e-06, -2.3842e-07,\n",
      "        -5.5781e-03, -1.1647e-03, -1.4305e-06, -1.1610e-04, -2.3842e-07,\n",
      "        -4.8040e-05, -2.3842e-07, -3.2424e-05, -1.7404e-05, -5.9604e-06,\n",
      "        -8.3446e-07, -4.2915e-06, -3.8147e-06, -1.1682e-05, -1.9073e-06,\n",
      "        -1.1921e-05, -7.1046e-05, -1.9431e-05, -3.8115e-03, -5.2570e-05,\n",
      "        -6.1210e-01, -3.6687e-01, -4.1005e-03,  0.0000e+00, -8.9665e-04,\n",
      "        -1.9431e-05, -2.0051e-02, -1.1921e-06, -7.6529e-05, -1.6689e-06,\n",
      "        -7.6294e-06, -7.1526e-07, -2.3842e-06, -2.7248e-04,  0.0000e+00,\n",
      "        -1.6689e-06, -3.0733e-03, -7.9779e-04, -7.3907e-05, -4.8077e-04,\n",
      "        -4.7790e-01, -7.3379e-03, -2.0266e-06, -1.6570e-05, -1.4663e-05,\n",
      "        -1.0490e-05, -3.4571e-06, -1.0848e-05, -2.6226e-06, -3.1145e-04,\n",
      "        -2.4685e-04, -5.0068e-06, -5.5908e-05, -3.9450e-04, -4.7684e-06,\n",
      "        -1.1086e-04, -2.3842e-07, -2.3456e-03, -1.8640e-03, -5.3479e-01,\n",
      "        -1.3172e-04, -2.8610e-06, -2.9403e-03, -1.0847e-04, -8.2254e-06,\n",
      "        -8.3446e-07, -7.4427e-03, -1.0610e-05, -2.5877e-04, -5.1260e-06,\n",
      "        -3.5763e-06, -1.0729e-06, -3.7073e-05, -5.0068e-06, -8.1062e-06,\n",
      "         0.0000e+00, -1.4113e-04, -8.9407e-06, -1.4305e-06, -1.4305e-06,\n",
      "        -3.7073e-05, -5.1778e-02, -8.3446e-07, -1.6689e-05, -3.5763e-07,\n",
      "        -1.5497e-06, -3.0628e-03, -5.1260e-06, -4.2915e-06, -1.0218e-03,\n",
      "        -5.8412e-06, -3.9696e-05, -3.5763e-07, -2.2769e-05, -2.1725e-02,\n",
      "        -7.7486e-06, -4.2915e-06, -6.0194e-04, -1.3113e-06, -3.8147e-06,\n",
      "        -1.6994e-02, -2.6226e-05, -3.2186e-06, -1.0049e-04, -1.4531e-04,\n",
      "        -7.2238e-05, -2.5229e-03, -2.3842e-06, -6.6757e-06, -4.3573e-04,\n",
      "         0.0000e+00, -5.5596e-03, -4.2779e-02, -1.3113e-06, -2.0033e-03,\n",
      "        -1.6451e-05, -7.1526e-07, -2.3561e-03, -5.9605e-07, -6.7949e-06,\n",
      "        -2.6672e-03, -1.6130e-02, -2.3744e-04, -1.0729e-06, -1.3113e-06,\n",
      "        -9.5367e-07, -2.1100e-05, -1.4543e-05, -3.2186e-06,  0.0000e+00,\n",
      "        -1.0729e-06, -7.1691e-04, -1.6259e-04, -3.5763e-06, -3.6716e-05,\n",
      "        -2.4273e-02, -1.0133e-05, -8.3446e-07, -3.4928e-05, -3.8623e-05,\n",
      "        -2.3842e-07, -8.9407e-06, -4.7684e-07, -1.8881e-04, -2.6154e-02,\n",
      "        -2.4927e-02, -2.5329e-04, -6.4797e-02, -1.5794e-01, -1.1443e-01,\n",
      "        -3.9201e-03, -2.7418e-06, -2.4769e-04, -2.3842e-07, -2.3126e-05,\n",
      "        -1.1325e-05, -4.5921e-04, -1.0812e-03,  0.0000e+00, -8.0940e-05,\n",
      "        -3.0960e-03, -2.2530e-05, -9.8788e-04, -1.4424e-05, -2.8701e-03,\n",
      "        -4.0411e-05,  0.0000e+00, -7.1525e-06, -6.9868e-04, -1.9849e-03,\n",
      "        -4.9113e-05, -8.1506e-04, -3.2186e-06, -2.4438e-05, -1.6212e-05,\n",
      "        -8.1655e-05, -9.5367e-07, -8.3446e-07, -4.1628e-03, -1.5242e-02,\n",
      "        -4.4106e-05, -2.0623e-05, -9.5367e-07, -3.0040e-05, -8.3446e-07,\n",
      "        -4.4107e-06, -6.9497e-05, -1.5528e-03, -7.7885e-04, -5.8412e-06,\n",
      "        -4.5800e-03, -3.8616e-04, -2.5306e-03,  0.0000e+00, -1.1921e-07,\n",
      "        -1.3737e-01, -7.8678e-06, -2.8557e-03, -5.3166e-05, -6.0795e-05]),\n",
      "indices=tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3, 7, 4, 6, 4, 3, 0, 7, 0,\n",
      "        2, 9, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 9, 3, 1, 4,\n",
      "        1, 7, 6, 9, 6, 0, 5, 4, 9, 9, 2, 1, 9, 4, 8, 7, 3, 9, 7, 4, 4, 4, 9, 2,\n",
      "        5, 4, 7, 6, 7, 9, 0, 5, 8, 5, 6, 6, 5, 7, 8, 1, 0, 1, 6, 4, 6, 7, 3, 1,\n",
      "        7, 1, 8, 2, 0, 2, 9, 9, 5, 5, 1, 5, 6, 0, 3, 4, 4, 6, 5, 4, 6, 5, 4, 5,\n",
      "        1, 4, 4, 7, 2, 3, 2, 7, 1, 8, 1, 8, 1, 8, 5, 0, 8, 9, 2, 5, 0, 1, 1, 1,\n",
      "        0, 9, 0, 3, 1, 6, 4, 2, 3, 6, 1, 1, 1, 3, 9, 5, 2, 9, 4, 5, 9, 3, 9, 0,\n",
      "        3, 6, 5, 5, 7, 2, 2, 7, 1, 2, 8, 4, 1, 7, 3, 3, 8, 8, 7, 9, 2, 2, 4, 1,\n",
      "        5, 9, 8, 7, 2, 3, 0, 4, 4, 2, 4, 1, 9, 5, 7, 7, 2, 8, 2, 0, 8, 5, 7, 7,\n",
      "        9, 1, 8, 1, 8, 0, 3, 0, 1, 9, 9, 4, 1, 8, 2, 1, 2, 9, 7, 5, 9, 2, 6, 4,\n",
      "        1, 5, 8, 2, 9, 2, 0, 4, 0, 0, 2, 8, 4, 7, 1, 2, 4, 0, 2, 7, 4, 3, 3, 0,\n",
      "        0, 3, 1, 9, 6, 5, 2, 5, 9, 7, 9, 3, 0, 4, 2, 0, 7, 1, 1, 2, 1, 5, 3, 3,\n",
      "        9, 7, 8, 6, 5, 6, 1, 3, 8, 1, 0, 5, 1, 3, 1, 5, 5, 6, 1, 8, 5, 1, 7, 9,\n",
      "        4, 6, 2, 2, 5, 0, 6, 5, 6, 3, 7, 2, 0, 8, 8, 5, 4, 1, 1, 4, 0, 3, 3, 7,\n",
      "        6, 1, 6, 2, 1, 9, 2, 8, 6, 1, 9, 5, 2, 5, 4, 4, 2, 8, 3, 8, 2, 4, 5, 0,\n",
      "        3, 1, 7, 7, 5, 7, 9, 7, 1, 9, 2, 1, 4, 2, 9, 2, 0, 4, 9, 1, 4, 8, 1, 8,\n",
      "        4, 5, 9, 8, 8, 3, 7, 6, 0, 0, 3, 0, 2, 0, 6, 4, 9, 3, 3, 3, 2, 3, 9, 1,\n",
      "        2, 6, 8, 0, 5, 6, 6, 6, 3, 8, 8, 2, 7, 5, 8, 9, 6, 1, 8, 4, 1, 2, 5, 9,\n",
      "        1, 9, 7, 5, 4, 0, 8, 9, 9, 1, 0, 5, 2, 3, 7, 8, 9, 4, 0, 6]))\n"
     ]
    }
   ],
   "source": [
    "print(model(sample_data).data.max(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa81303",
   "metadata": {},
   "source": [
    "#### Graph Neural Networks (a class of deep learning models that can natively learn patterns from graph structures.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0610808",
   "metadata": {},
   "source": [
    "in following exercise we'll cover two variant of GNN:\n",
    "1. graph convolutional network (GCN)\n",
    "2. graph attention network (GAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e49ec18",
   "metadata": {},
   "source": [
    "Capturing the graph structure besides the node-specific information is what gave birth to the new class of deep learning models in the form of GNNs, or else regular NNs would have been sufficient.\n",
    "\n",
    "so, in the process of embedding nodes or edges, our embedding function uses the graph structure and information to embed nodes with similar properties close to each other and embed the edges(relationship between them) in a (roughly) similar direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04e679",
   "metadata": {},
   "source": [
    "Using adjacency matrix and node(edge)-level features we can train a regular NN model or more specifically train a variant of GNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75a35ed",
   "metadata": {},
   "source": [
    "for GNN we need more than just the adjacency matrix and node(edge)-level information:\n",
    "- due to neighboring constraint (for addressing more than just *one* edge further layer) -> using computational graphs: ach node in the graph is represented by the respective node-level features and if a node has\n",
    "multiple neighbouring nodes, these neighbour nodes’ features are **aggregated** before being propagated\n",
    "to the given node. For each node in the graph, we create a **computational graph** where we represent the node’s neighborhood *layer-wise* –each layer representing a different degree of connection. **It is important to note that weights are shared between all NNs of a given layer and between all computational graphs** . This weight sharing prevents the number of overall model parameters from exploding as the number of graph nodes or the depth of the computation graph increases. Weight sharing also makes the system robust against different ordering of nodes or the addition of a new node to the original graph.\n",
    "\n",
    "- with the **aggregation** mechanism we address the limitation of permutation and rotation of the nodes in the computational graph because the ordering of nodes in the graph doesn’t matter anymore. This implicitly means that the aggregation function needs to be order independent, such as *sum, average, maximum, and minimum*. We cannot use aggregation functions like concatenation.\n",
    "\n",
    "- by using computational graph we don't need ro retrain the whole model from scratch, we only need to create one more computational graph for this newly added node.\n",
    "\n",
    "- also with this method (computational graph) we address the sparsity of adjacency matrix,  because we do not use the adjacency matrix information with computational graphs and rely only on the *intrinsic node features*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e4b15c",
   "metadata": {},
   "source": [
    "we have three types of graph learning tasks:\n",
    "1. For **Node-level tasks**, the **latent feature representations** of each node (final output of the node’s computational graph) are used to train a downstream task.\n",
    "2. Typically, an **edge-level task** uses the accompanying *node features* as well as the *edge features* to train a downstream classification task such as predicting the type of relationship -> To find equivalence in the world of computer vision, **image scene understanding** is an appropriate example of an edge-level task equivalent where different objects in an image are the nodes and the relationships between these objects are the edges, and the goal is to predict the type of relationship between these objects in the image.\n",
    "3. In **graph-level tasks**, we predict a class or a numerical value for the entire graph, by using the (permutation invariant) aggregation of the latent features of all nodes in the graph -> In the world of images, image classification is the graph-level task equivalent because all pixels (nodes) of the image (graph) are used to attribute a single value to the entire image (graph)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f2f9f1",
   "metadata": {},
   "source": [
    "**more on computational graph**:\n",
    "\n",
    "The concept of a computational graph in Graph Neural Networks (GNNs) refers to the structured representation of how computations are organized and executed during the training and inference processes. This graph is not the same as the input graph data; instead, it is a tool used to describe the flow of data and operations within the neural network itself.\n",
    "\n",
    "- **Forward** and **Backward** Passes:\n",
    "    - Forward Pass: This involves propagating input data through the network, applying each operation in sequence to compute the output. In GNNs, this includes **message passing** between nodes to update their representations.\n",
    "\t- Backward Pass: This is used during training to compute gradients of the loss function with respect to model parameters, facilitating optimization via backpropagation\n",
    "  \n",
    "**message passing mechanism**:\n",
    "\n",
    "$$h_v^{(l+1)} = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)} W^{(l)} h_u^{(l)} \\right)$$\n",
    "where:\n",
    "- $h_v^{(l)}$: embedding of node $v$ at layer $l$\n",
    "- $W^{(l)}$: learnable weight matrix (parameters)\n",
    "- $\\sigma$: activation function (e.g., ReLU)\n",
    "\n",
    "it also is written using **Kipf & Welling (2016) GCN layer implementation**:\n",
    "$$H^{(l+1)} = \\sigma\\left( \\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)} \\right)$$\n",
    "\n",
    "lets separate the process to undrestand how GCN's leverage the graph topology under message passing mechanism:\n",
    "1. Aggregation (structure-driven): $\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)}$:\n",
    "   - This part averages (or sums) over a node’s neighbors.\n",
    "   - It is structure-aware — it considers the graph topology.\n",
    "   - No trainable parameters here.\n",
    "   - You’re collecting messages — **“raw influence” from neighbors**.\n",
    "  \n",
    "2. Transformation (feature-driven): $\\left( \\text{Aggregated Messages} \\right) \\cdot W^{(l)}$:\n",
    "   - Now, you apply the *same* linear transformation to all nodes.\n",
    "   - $W^{(l)}$ acts like an MLP layer: it projects node features from $\\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{d_{out}}$\n",
    "   - This is the **trainable part**.\n",
    "   - The weight matrix learns what to “look for” in the aggregated messages. It’s not learning the importance of each neighbor per se — it’s learning how to interpret the combined signal from the neighbors.\n",
    "  \n",
    "so, $W$ acts after neighbors are combined. It says “*when a signal arrives from the neighborhood, how do I process it*?”\n",
    "\n",
    "and one important charasteristic of GNN and it's variants is:\n",
    "- Unlike CNNs (spatial locality) or RNNs (temporal locality), **GNNs leverage the graph topology**. The same parameters $W$ are shared across all nodes, enabling generalization across arbitrary graph structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cd591",
   "metadata": {},
   "source": [
    "### Reviewing prominent GNN models:\n",
    "- GCN\n",
    "- GAT\n",
    "- GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98592962",
   "metadata": {},
   "source": [
    "##### GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb2c4fc",
   "metadata": {},
   "source": [
    "first, lets remind ourselves that the **convolution** term in GCN architecture comes from **shared weights** using computational graphs\n",
    "\n",
    "<img src=../images/GCN-computation_graph.png width=700>\n",
    "\n",
    "the computational graph above is the computational graph of Node A with feature embedding of it's neighbors' neighbors as:\n",
    "\n",
    "Two-layer GCN-based node classification model, demonstrating the computational graph\n",
    "for node A – in the first layer, the features from the second-level neighbors (neighbor of neighbors)\n",
    "of node A are aggregated to produce a latent representation of node A’s neighbors; in the second\n",
    "layer, these latent representations are aggregated to produce final features for node A, which are\n",
    "then used for node classification\n",
    "\n",
    "While we are using node classification as an example to discuss GCNs, GCNs can\n",
    "also perform *graph classification, wherein the aggregation is performed across all nodes of the graph.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88158dc",
   "metadata": {},
   "source": [
    "$?$ brief note about the **projection layer**:\n",
    "\n",
    "a projection layer refers to a linear transformation that maps input features into a new space. in GCN it correspond to:\n",
    "$$Z = \\hat{A}_{norm} \\cdot X \\cdot W$$\n",
    "\n",
    "where:\n",
    "- $X$: input node feature matrix $\\in \\mathbb{R}^{N \\times F}$\n",
    "- $W$: learnable weight matrix $\\in \\mathbb{R}^{F \\times F{\\prime}}$\n",
    "- $\\hat{A}_{norm} = \\hat{D}^{-1/2} \\hat{A} \\hat{D}^{-1/2}$: normalized adjacency matrix\n",
    "- $Z$: output features $\\in \\mathbb{R}^{N \\times F{\\prime}}$\n",
    "\n",
    "It projects node features from their original space (e.g., 5-dim) to a new embedding space (e.g., 64-dim). This is equivalent to a Linear Layer (also known as fully connected / dense)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb2c61",
   "metadata": {},
   "source": [
    "so the difference between fully-connected(Feed_Forwarded layer) and GCN is that, The GCN layer itself consists of a fully connected layer **but also contains the neighborhood feature aggregation component**, which is the secret sauce that makes GCN work well on graph datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4ebc6",
   "metadata": {},
   "source": [
    "##### GAT\n",
    "\n",
    "While GCNs use the averaging of information from neighbors, which already extracts valuable graph\n",
    "information, a lot of work has been done on *finding better ways of aggregating information from neighbors*. An important milestone in this aspect of GNN research is GAT.\n",
    "\n",
    "as we said, GCN uses averaging as a mechanism to aggregate information from neighboring node features. This\n",
    "has an inherent limitation as it assumes that all neighbors are to be treated equally, which might not\n",
    "necessarily be the case. For example, if two nodes, $X$ and $Y$, have the same initial feature values and\n",
    "the same set of neighbors, a GCN model would identify them under the same class or cluster. But this\n",
    "might not be true. To capture this level of nuanced information in graphs, we can replace the simple\n",
    "averaging mechanism with the attention mechanism. This is where GATs come into play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967de741",
   "metadata": {},
   "source": [
    "In the context of GATs, attention allows the model to place different weights on different\n",
    "neighbors of a node while classifying the node type, thereby enabling a more complex and powerful\n",
    "model. With the attention mechanism, we learn attention coefficients for each neighbor that add more\n",
    "trainable parameters to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634af900",
   "metadata": {},
   "source": [
    "the image below demonstrate the difference between GCN and GAT architecture:\n",
    "\n",
    "<img src=../images/GCN-vs-GAT.png width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fde977",
   "metadata": {},
   "source": [
    "in GAT, scalibility can become a bottleneck over a large graph data structure. this is where the **GraphSAGE** comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37047126",
   "metadata": {},
   "source": [
    "##### GraphSAGE\n",
    "\n",
    "the source paper of introducing [GraphSAGE](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf)\n",
    "\n",
    "introduction in the paper of the proposed method:\n",
    "\n",
    "The key idea behind our approach is that we learn how to aggregate feature information from a\n",
    "node’s local neighborhood (e.g., the degrees or text attributes of nearby nodes). We first describe\n",
    "the GraphSAGE embedding generation (i.e., forward propagation) algorithm, which generates\n",
    "embeddings for nodes assuming that the GraphSAGE model parameters are already learned . We then describe how the GraphSAGE model parameters can be learned using standard stochastic gradient descent and backpropagation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63bb27a",
   "metadata": {},
   "source": [
    "**GraphSAGE** , short for graph sample and aggregate, randomly and uniformly samples neighbors\n",
    "for a given node, and uses only these selected neighbors to extract graph information, as opposed to\n",
    "GCN and GAT, which uses all neighbors. This algorithm is hence useful for large and dense graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef1046",
   "metadata": {},
   "source": [
    "<img src=../images/GraphSAGE.png width=750>\n",
    "\n",
    "Node Embedding algorithm provided in the paper:\n",
    "\n",
    "<img src=../images/Node_embedding_GraphSAGE.png width=850>\n",
    "\n",
    "The intuition behind Algorithm is that at each iteration, or search depth, nodes aggregate information\n",
    "from their local neighbors, and as this process iterates, nodes incrementally gain more and more\n",
    "information from further reaches of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed695d3",
   "metadata": {},
   "source": [
    "so we've seen that the difference is the in averaging techniques which GraphSAGE approach provides, there are listed as below:\n",
    "- Mean aggregator, (We (as in the paper) call this modified mean-based aggregator convolutional since it is a rough, linear approximation of\n",
    "a localized spectral convolution):\n",
    "$$\\mathbf{h}_v^k \\leftarrow \\sigma\\left(\\mathbf{W} \\cdot \\text{MEAN}\\left(\\left\\{\\mathbf{h}_v^{k-1}\\right\\} \\cup \\left\\{\\mathbf{h}_u^{k-1}, \\forall u \\in \\mathcal{N}(v)\\right\\}\\right)\\right).$$\n",
    "\n",
    "- LSTM aggregator: We also examined a more complex aggregator based on an LSTM architecture. Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not inherently symmetric (i.e., they are not permutation invariant), since they process their inputs in a sequential manner. We adapt LSTMs to operate on an unordered set by simply applying the LSTMs to a random permutation of the node’s neighbors.\n",
    "\n",
    "- Pooling aggregator: The final aggregator we examine is both symmetric and trainable. In this\n",
    "pooling approach, each neighbor’s vector is independently fed through a fully-connected neural\n",
    "network; following this transformation, an elementwise max-pooling operation is applied to aggregate\n",
    "information across the neighbor set:\n",
    "$$\\text{AGGREGATE}_k^{\\text{pool}} = \\max\\left(\\left\\{\\sigma\\left(\\mathbf{W}_{\\text{pool}}\\mathbf{h}_{u_i}^k + \\mathbf{b}\\right), \\forall u_i \\in \\mathcal{N}(v)\\right\\}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896a1482",
   "metadata": {},
   "source": [
    "### Building a GCN model using PyTorch Geometric\n",
    "***Lets get hands-on***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.2\n",
    "%pip install torch_geometric==2.4.0\n",
    "%pip install seaborn==0.12.2\n",
    "%pip install networkx==2.8.5\n",
    "%pip install scikit-learn==1.3.2\n",
    "%pip install matplotlib==3.5.2\n",
    "%pip install pandas==1.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea839f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# setting up the notebook width to 100% of the screen\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f90480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install decorator==5.0.9\n",
    "# %pip install torch-geometric==2.3.1\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971445c",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d971f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function transforms the node embeddings of size 6 to size 2 using the t-SNE algorithm.\n",
    "def visualize(data, labels):\n",
    "    tsne = TSNE(n_components=2, init='pca', random_state=7)\n",
    "    tsne_res = tsne.fit_transform(data)\n",
    "    v = pd.DataFrame(data,columns=[str(i) for i in range(data.shape[1])])\n",
    "    v['color'] = labels\n",
    "    v['label'] = v['color'].apply(lambda i: str(i))\n",
    "    v[\"dim1\"] = tsne_res[:,0]\n",
    "    v[\"dim2\"] = tsne_res[:,1]\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "\n",
    "    sns.scatterplot(\n",
    "        x=\"dim1\", y=\"dim2\",\n",
    "        hue=\"color\",\n",
    "        palette=sns.color_palette([\"#52D1DC\", \"#8D0004\", \"#845218\",\"#563EAA\", \"#E44658\", \"#63C100\", \"#FF7800\"]),\n",
    "        legend=False,\n",
    "        data=v,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d237ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(G, color):\n",
    "    plt.figure(figsize=(75,75))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    nx.draw_networkx(G, pos=nx.spring_layout(G), with_labels=False,\n",
    "                     node_color=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3dbcde",
   "metadata": {},
   "source": [
    "#### Load graph data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd48b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='data/Planetoid', name='CiteSeer')\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90698444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describing the dataset\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print(data)\n",
    "print('==============================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a8aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = to_networkx(data)\n",
    "visualize_graph(G, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b4b78",
   "metadata": {},
   "source": [
    "#### First lets build a baseline model NN based node classifier\n",
    "\n",
    "we’ll build and train a multilayer perceptron (MLP) model on the CiteSeer graph dataset\n",
    "using PyTorch. First, we instantiate, with random weights, an MLP model that takes in the features\n",
    "of a node as input and produces the node class as output. Then, we visualize the output node embed-\n",
    "dings (of size 6) produced by the randomly initialized MLP model. Next, we train the MLP model on\n",
    "the multi-class classification task. We finally evaluate the trained MLP model and visualize the node\n",
    "embeddings produced from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(dataset.num_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "model = MLP(hidden_channels=16)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f7b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize initial MLP embedding for different graph nodes\n",
    "model.eval()\n",
    "out = model(data.x)\n",
    "visualize(out.detach().cpu().numpy(), data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d253343",
   "metadata": {},
   "source": [
    "More on **t-Distributed stochastic neighbor embedding (t-SNE)**:\n",
    "\n",
    "[read more about t-SNE](https://distill.pub/2016/misread-tsne/)\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique used primarily for visualizing high-dimensional data in a lower-dimensional space, typically two or three dimensions. Developed by Laurens van der Maaten and Geoffrey Hinton, t-SNE is based on Stochastic Neighbor Embedding (SNE) but uses a t-distribution in the low-dimensional space to alleviate the “crowding problem” and better separate dissimilar points\n",
    "\n",
    "it aims to map high-dimensional points to low-dimensional space while preserving local structure (i.e it keeps similar points close together)\n",
    "\n",
    "the Goal:\n",
    "\n",
    "Given high-dimensional data $X = \\{x_1, x_2, \\dots, x_n\\}$, t-SNE finds a low-dimensional embedding $Y = \\{y_1, y_2, \\dots, y_n\\}$ such that the similarity between the points in the high-dimensional space is preserved in the low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843299b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(hidden_channels=16)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)  # Define optimizer.\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(mask):\n",
    "      \"\"\"\n",
    "        mask is a list of length equal to the total number of nodes in the graph. \n",
    "        The mask list contains 0s and 1s, indicating which node\n",
    "        in the graph is to be ignored and to be chosen, respectively.\n",
    "      \"\"\"\n",
    "      model.eval()\n",
    "      out = model(data.x)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
    "      return acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078c38d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(data.test_mask)\n",
    "print(f'Test: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b25fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the final MLP embedding for different graph nodes\n",
    "out = model(data.x)\n",
    "visualize(out.detach().cpu().numpy(), data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44faac6a",
   "metadata": {},
   "source": [
    "So far, we have just used the 3703 node-level features to classify nodes only based on their features, without any graph-level knowledge using MLP\n",
    "\n",
    "Next, we try **building GCN model for node classification** and levarage the neighborhood context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76167e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed36f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the initial node embedding produced by GCN node classifier\n",
    "# before training\n",
    "model.eval()\n",
    "out = model(data.x, data.edge_index)\n",
    "visualize(out.detach().cpu().numpy(), data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04d516",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, data.edge_index)  # Perform a single forward pass. (also providing the Adjacency matrix)\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(mask):\n",
    "      model.eval()\n",
    "      out = model(data.x, data.edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      correct = pred[mask] == data.y[mask]  # Check against ground-truth labels.\n",
    "      acc = int(correct.sum()) / int(mask.sum())  # Derive ratio of correct predictions.\n",
    "      return acc\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(data.test_mask)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ee79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running on whole graph and visualizing the final classifier output in 2D\n",
    "out = model(data.x, data.edge_index)\n",
    "visualize(out.detach().cpu().numpy(), data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107ca70",
   "metadata": {},
   "source": [
    "Now let's improve the node classification task using **Attentions (GAT)**\n",
    "\n",
    "the important difference between GAT and GCN is the aggregation mechanism which is replaced by attention in GAT instead of neighborhood aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9e74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, hidden_channels, heads):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GATConv(dataset.num_features, hidden_channels, heads)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, dataset.num_classes, heads=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "    \n",
    "model = GAT(hidden_channels=16, heads=8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500d08b",
   "metadata": {},
   "source": [
    "Note that In the first layer, we keep eight attention heads. This means that we are going to derive eight parallel aggregations of neighboring node features using eight parallel and independent trainable attention coefficients. and at the end the resulting feature vectors of all of these head (of size 16) are concatenated and the the output of size 128 goes to second GATConv layer.\n",
    "\n",
    "<img src=../images/GAT_arch_implementation.png width=850>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model(data.x, data.edge_index)\n",
    "visualize(out.detach().cpu().numpy(), data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=0.0005, weight_decay=1e-1\n",
    ")\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(mask):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_idnex)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[mask] == data.y[mask]\n",
    "    acc = int(correct.sum()) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_acc = test(data.val_mask)\n",
    "    test_acc = test(data.test_mask)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b33760",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = test(data.test_mask)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fc680",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data.x, data.edge_index)\n",
    "visualize(out.detach().cpu().numpy(), data.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97772fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ebebe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11986b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a neural network with CNN and LSTMs to build an image caption generator using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN-LSTM network architecture consists of a convolutional layer(s) for extracting features from\n",
    "the input data (image), followed by an LSTM layer(s) to perform sequential predictions. This kind of\n",
    "model is deep both spatially – thanks to the CNN component – as well as temporally – thanks to the\n",
    "LSTM network. The convolutional part of the model is often used as an encoder that takes in an input\n",
    "image and outputs high-dimensional features or embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/CNN-LSTM.png width=850>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we will implement an image captioning system in PyTorch that includes building a hybrid model architecture as well as data loading, preprocessing, model training, and model evaluation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "from pycocotools.coco import COCO\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linux\n",
    "# !apt-get install wget\n",
    "# mac\n",
    "!brew install wget\n",
    " \n",
    "# create a data directory\n",
    "!mkdir data_dir\n",
    " \n",
    "# download images and annotations to the data directory\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip -P ./data_dir/\n",
    "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data_dir/\n",
    "!wget http://images.cocodataset.org/zips/val2014.zip -P ./data_dir/\n",
    "    \n",
    "# extract zipped images and annotations and remove the zip files\n",
    "!unzip ./data_dir/annotations_trainval2014.zip -d ./data_dir/\n",
    "!rm ./data_dir/annotations_trainval2014.zip\n",
    "!unzip ./data_dir/train2014.zip -d ./data_dir/\n",
    "!rm ./data_dir/train2014.zip \n",
    "!unzip ./data_dir/val2014.zip -d ./data_dir/ \n",
    "!rm ./data_dir/val2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        self.index = 0\n",
    " \n",
    "    def __call__(self, token):\n",
    "        if not token in self.w2i:\n",
    "            return self.w2i['<unk>']\n",
    "        return self.w2i[token]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.w2i)\n",
    "    def add_token(self, token):\n",
    "        if not token in self.w2i:\n",
    "            self.w2i[token] = self.index\n",
    "            self.i2w[self.index] = token\n",
    "            self.index += 1\n",
    "\n",
    "def build_vocabulary(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    " \n",
    "        if (i+1) % 1000 == 0:\n",
    "            print(\"[{}/{}] Tokenized the captions.\".format(i+1, len(ids)))\n",
    " \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    tokens = [token for token, cnt in counter.items() if cnt >= threshold]\n",
    " \n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocab()\n",
    "    vocab.add_token('<pad>')\n",
    "    vocab.add_token('<start>')\n",
    "    vocab.add_token('<end>')\n",
    "    vocab.add_token('<unk>')\n",
    " \n",
    "    # Add the words to the vocabulary.\n",
    "    for i, token in enumerate(tokens):\n",
    "        vocab.add_token(token)\n",
    "    return vocab\n",
    " \n",
    "vocab = build_vocabulary(json='data_dir/annotations/captions_train2014.json', threshold=4)\n",
    "vocab_path = './data_dir/vocabulary.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary('./data_dir/annotations/captions_train2014.json', 4)\n",
    "vocab_path = './data_dir/vocabulary.pk1'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(f'Total vocabulary size: {len(vocab)}')\n",
    "print(f'Saved the vocabulary wrapper to {vocab_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_image(image, shape):\n",
    "    \"\"\"Resize an image to the given shape.\"\"\"\n",
    "    return image.resize(shape, Image.LANCZOS)\n",
    " \n",
    "def reshape_images(image_path, output_path, shape):\n",
    "    \"\"\"Reshape the images in 'image_path' and save into 'output_path'.\"\"\"\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    " \n",
    "    images = os.listdir(image_path)\n",
    "    num_im = len(images)\n",
    "    for i, im in enumerate(images):\n",
    "        with open(os.path.join(image_path, im), 'r+b') as f:\n",
    "            with Image.open(f) as image:\n",
    "                image = reshape_image(image, shape)\n",
    "                image.save(os.path.join(output_path, im), image.format)\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"[{}/{}] Resized the images and saved into '{}'.\"\n",
    "                   .format(i+1, num_im, output_path))\n",
    "\n",
    "image_path = './data_dir/train2014/'\n",
    "output_path = './data_dir/resized_images/'\n",
    "image_shape = [256, 256]\n",
    "reshape_images(image_path, output_path, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, data_path, coco_json_path, vocabulary, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = data_path\n",
    "        self.coco_data = COCO(coco_json_path)\n",
    "        self.indices = list(self.coco_data.anns.keys())\n",
    "        self.vocabulary = vocabulary\n",
    "        self.transform = transform\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco_data = self.coco_data\n",
    "        vocabulary = self.vocabulary\n",
    "        annotation_id = self.indices[idx]\n",
    "        caption = coco_data.anns[annotation_id]['caption']\n",
    "        image_id = coco_data.anns[annotation_id]['image_id']\n",
    "        image_path = coco_data.loadImgs(image_id)[0]['file_name']\n",
    " \n",
    "        image = Image.open(os.path.join(self.root, image_path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    " \n",
    "        # Convert caption (string) to word ids.\n",
    "        word_tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocabulary('<start>'))\n",
    "        caption.extend([vocabulary(token) for token in word_tokens])\n",
    "        caption.append(vocabulary('<end>'))\n",
    "        ground_truth = torch.Tensor(caption)\n",
    "        return image, ground_truth\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    " \n",
    " \n",
    "def collate_function(data_batch):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data_batch.sort(key=lambda d: len(d[1]), reverse=True)\n",
    "    imgs, caps = zip(*data_batch)\n",
    " \n",
    "    # Merge images (from list of 3D tensors to 4D tensor).\n",
    "    # Originally, imgs is a list of <batch_size> number of RGB images with dimensions (3, 256, 256)\n",
    "    # This line of code turns it into a single tensor of dimensions (<batch_size>, 3, 256, 256)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    " \n",
    "    # Merge captions (from list of 1D tensors to 2D tensor), similar to merging of images donw above.\n",
    "    cap_lens = [len(cap) for cap in caps]\n",
    "    tgts = torch.zeros(len(caps), max(cap_lens)).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        end = cap_lens[i]\n",
    "        tgts[i, :end] = cap[:end]        \n",
    "    return imgs, tgts, cap_lens\n",
    " \n",
    "def get_loader(data_path, coco_json_path, vocabulary, transform, batch_size, shuffle):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco_dataser = CustomCocoDataset(data_path=data_path,\n",
    "                       coco_json_path=coco_json_path,\n",
    "                       vocabulary=vocabulary,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    custom_data_loader = torch.utils.data.DataLoader(dataset=coco_dataser, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              collate_fn=collate_function)\n",
    "    return custom_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data pipeline is setup.\n",
    "\n",
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(CNNModel, self).__init__()\n",
    "        resnet = models.resnet152(weights=True)\n",
    "        module_list = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet_module = nn.Sequential(*module_list)\n",
    "        self.linear_layer = nn.Linear(resnet.fc.in_features, embedding_size)\n",
    "        self.batch_norm = nn.BatchNorm1d(embedding_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, input_images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            resnet_features = self.resnet_module(input_images)\n",
    "        resnet_features = resnet_features.reshape(resnet_features.size(0), -1)\n",
    "        final_features = self.batch_norm(self.linear_layer(resnet_features))\n",
    "        return final_features\n",
    " \n",
    " \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_layer_size, vocabulary_size, num_layers, max_seq_len=20):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm_layer = nn.LSTM(embedding_size, hidden_layer_size, num_layers, batch_first=True)\n",
    "        self.linear_layer = nn.Linear(hidden_layer_size, vocabulary_size)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def forward(self, input_features, capts, lens):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embedding_layer(caps)\n",
    "        embeddings = torch.cat((input_features.unsqueeze(1), embeddings), 1)\n",
    "        lstm_input = pack_padded_sequence(embeddings, lens, batch_first=True) \n",
    "        hidden_variables, _ = self.lstm_layer(lstm_input)\n",
    "        model_outputs = self.linear_layer(hidden_variables[0])\n",
    "        return model_outputs\n",
    "    \n",
    "    def sample(self, input_features, lstm_states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_indices = []\n",
    "        lstm_inputs = input_features.unsqueeze(1)\n",
    "        for i in range(self.max_seq_len):\n",
    "            hidden_variables, lstm_states = self.lstm_layer(lstm_inputs, lstm_states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            model_outputs = self.linear_layer(hidden_variables.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted_outputs = model_outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_indices.append(predicted_outputs)\n",
    "            lstm_inputs = self.embedding_layer(predicted_outputs)                       # inputs: (batch_size, embed_size)\n",
    "            lstm_inputs = lstm_inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_indices = torch.stack(sampled_indices, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

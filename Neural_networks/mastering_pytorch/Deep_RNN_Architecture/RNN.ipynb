{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Recurrent neural networks architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are capable of modeling the temporal aspect of data by including additional weights in the model that create cycles in the network. This helps maintain a *state*. The concept of cycles explains the term *recurrence*, and this recurrence helps establish the concept of *memory* in these networks. Essentially, such networks facilitate the use of intermediate outputs at time step $t$ as inputs for time step $t+1$, while maintaining a hidden internal state. **These connections across time steps are called recurrent connections**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while RNNs can remember the sequential information across short sequences, they tend to struggle with\n",
    "long sequences due to the larger number of multiplications. *LSTM*s resolve this issue by controlling\n",
    "their input and output using *gates*.\n",
    "\n",
    "An LSTM layer essentially consists of various time-unfolded LSTM cells. Information passes from\n",
    "one cell to another in the form of cell states. These cell states are controlled or manipulated using\n",
    "multiplications and additions using the mechanism of gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/LSTM_gates.png width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "##### difference between vanilla RNNs and Gated ones:\n",
    "\n",
    "a) in vanilla we have:\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b)$$\n",
    "   1. This formulation blindly updates the hidden state at each time step, with no control over what to keep or what to forget.\n",
    "   2. It leads to vanishing gradients in long sequences, making it hard to learn long-range dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Gates in LSTM:\n",
    "LSTM maintain two things at each time step:\n",
    "- a **hidden state** $h_t$ (what gets output)\n",
    "- a **cell state** $c_t$ (long-term memory)\n",
    "there are three gates in LSTM:\n",
    "1. **Forget Gate**: Decides what to forget from the previous cell state $c_{t-1}$.\n",
    "$$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "2. **Input Gate**: Decides how much new information to write to the cell.\n",
    "$$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "and also update the cell state:\n",
    "$$c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$$\n",
    "3. **Output Gate**: Decides what part of the memory should be output.\n",
    "$$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "and finally:\n",
    "$$h_t = o_t \\odot \\tanh(c_t)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in *Stacked LSTMs*:\n",
    "\n",
    "LSTM cells are, by their very nature, stacked in the time dimension of an LSTM layer. Stacking several\n",
    "such layers in the space dimension provides them with the additional depth in space they need. The\n",
    "downside of these models is that they are significantly slower to train due to the extra depth and extra\n",
    "recurrent connections they have. Furthermore, the additional LSTM layers need to be unrolled (in\n",
    "the time dimension) at every training iteration. Hence, training stacked recurrent models, in general,\n",
    "is not parallelizable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRUs:\n",
    "\n",
    "The LSTM cell has two states – internal and external – as well as three different gates – input gate,\n",
    "forget gate, and output gate. A similar type of cell, named a gated recurrent unit (GRU), was invented\n",
    "in 2014 with the goal of learning long-term dependencies while effectively dealing with the exploding\n",
    "and vanishing gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/GRU-net.png width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training an RNN model using PyTorch for text classification task -sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$?$ Tokenization, Embedding, Feature vectors:\n",
    "\n",
    "$!$ Tokenization is the process of converting words into numerical tokens or integers, as we will see in\n",
    "this exercise. Sentences are then equivalent to an array of numbers, each number in the ordered\n",
    "array representing a word. While tokenization provides us with integer indices for each word, we\n",
    "still want to represent each word as a vector of numbers – as a feature – in the feature space of words.\n",
    "Why? Because the information contained in a word cannot just be represented by a single number.\n",
    "*This process of representing words as vectors is called **embedding***, which we will also use later in\n",
    "this exercise. An embedding matrix, which can be learned during model training, acts as a lookup\n",
    "for word vectors. If a word has a token index of 123, then the embedding for that word is the vector\n",
    "contained in row 123 of the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12500/12500 [00:02<00:00, 4857.18it/s]\n",
      "100%|██████████| 12500/12500 [00:02<00:00, 4950.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews : 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# read sentiments and reviews data from the text files\n",
    "review_list = []\n",
    "label_list = []\n",
    "for label in ['pos', 'neg']:\n",
    "    for fname in tqdm(os.listdir(f'./aclImdb/train/{label}/')):\n",
    "        if 'txt' not in fname:\n",
    "            continue\n",
    "        with open(os.path.join(f'./aclImdb/train/{label}/', fname), encoding=\"utf8\") as f:\n",
    "            review_list += [f.read()]\n",
    "            label_list += [label]\n",
    "print ('Number of reviews :', len(review_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:01<00:00, 18183.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 334691), ('and', 162228), ('a', 161940), ('of', 145326), ('to', 135042), ('is', 106855), ('in', 93028), ('it', 77099), ('i', 75719), ('this', 75190)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pre-processing review text\n",
    "review_list = [review.lower() for review in review_list]\n",
    "review_list = [''.join([letter for letter in review if letter not in punctuation]) for review in tqdm(review_list)]\n",
    "\n",
    "# accumulate all review texts together\n",
    "review_blob = ' '.join(review_list)\n",
    "\n",
    "# generate a list of all words of all reviews\n",
    "review_words = review_blob.split()\n",
    "\n",
    "# get the word counts\n",
    "count_words = Counter(review_words)\n",
    "\n",
    "# sort words as per counts (decreasing order)\n",
    "total_review_words = len(review_words)\n",
    "sorted_review_words = count_words.most_common(total_review_words)\n",
    "\n",
    "print(sorted_review_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, these non-nouns, also referred to as *stop words*, would be removed from the corpus\n",
    "as they do not carry a lot of meaning. However, we will skip those advanced text-processing\n",
    "steps to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('a', 3), ('of', 4), ('to', 5), ('is', 6), ('in', 7), ('it', 8), ('i', 9), ('this', 10)]\n"
     ]
    }
   ],
   "source": [
    "# create word to integer (tokens) dictionary in order to encode text as numbers\n",
    "vocab_to_token = {word:idx+1 for idx, (word, count) in enumerate(sorted_review_words)}\n",
    "print(list(vocab_to_token.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem imagine a movie where joe piscopo is actually funny maureen stapleton is a scene stealer the moroni character is an absolute scream watch for alan the skipper hale jr as a police sgt\n",
      "\n",
      "[15, 3, 17, 11, 201, 56, 1165, 47, 242, 23, 3, 168, 4, 891, 4325, 3513, 15, 10, 1514, 822, 3, 17, 112, 884, 14623, 6, 155, 161, 7307, 15816, 6, 3, 134, 20049, 1, 32064, 108, 6, 33, 1492, 1943, 103, 15, 1550, 1, 18993, 9055, 1809, 14, 3, 549, 6906]\n"
     ]
    }
   ],
   "source": [
    "review_tokenized = []\n",
    "for review in review_list:\n",
    "    review_tokenized.append([vocab_to_token[word] for word in review.split() if word in vocab_to_token])\n",
    "print(review_list[0])\n",
    "print()\n",
    "print(review_tokenized[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode setiments as 0 and 1\n",
    "encoded_label_list = [1 if label == 'pos' else 0 for label in label_list]\n",
    "review_len = [len(review) for review in review_tokenized]\n",
    "\n",
    "review_tokenized = [review_tokenized[i] for i, l in enumerate(review_len) if l > 0]\n",
    "encoded_label_list = np.array([encoded_label_list[i] for i, l in enumerate(review_len)], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pad_sequence(review_tokenized, sequence_length):\n",
    "    \"\"\"\n",
    "        returns the tokenized review sequences padded with 0's\n",
    "        or truncated to the sequence_lenght\n",
    "    \"\"\"\n",
    "    padded_reviews = np.zeros((len(review_tokenized), sequence_length), dtype=np.int64)\n",
    "    for idx, review in enumerate(review_tokenized):\n",
    "        review_len = len(review)\n",
    "\n",
    "        if review_len <= sequence_length:\n",
    "            zeroes = list(np.zeros(sequence_length - review_len))\n",
    "            new_sequence = zeroes+review\n",
    "        elif review_len > sequence_length:\n",
    "            new_sequence = review[0:sequence_length]\n",
    "\n",
    "        padded_reviews[idx, :] = new_sequence\n",
    "\n",
    "    return padded_reviews\n",
    "\n",
    "sequence_length = 512\n",
    "padded_reviews = pad_sequence(review_tokenized, sequence_length)\n",
    "plt.hist(review_len);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.75\n",
    "train_X = padded_reviews[:int(train_val_split*len(padded_reviews))]\n",
    "train_y = encoded_label_list[:int(train_val_split*len(padded_reviews))]\n",
    "validation_X = padded_reviews[int(train_val_split*len(padded_reviews)):]\n",
    "validation_y = encoded_label_list[int(train_val_split*len(padded_reviews)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate torch datasets\n",
    "train_dataset = TensorDataset(torch.from_numpy(train_X).to(device), torch.from_numpy(train_y).to(device))\n",
    "validation_dataset = TensorDataset(torch.from_numpy(validation_X).to(device), torch.from_numpy(validation_y).to(device))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "# torch dataloaders (shuffle data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Input size:  torch.Size([32, 512])\n",
      "Example Input:\n",
      " tensor([[   0,    0,    0,  ...,   80,   15, 4116],\n",
      "        [   0,    0,    0,  ...,   20,   10,   28],\n",
      "        [   0,    0,    0,  ...,   44,   22,   68],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    2,   37, 2632],\n",
      "        [   0,    0,    0,  ...,   35, 1033,  476],\n",
      "        [   0,    0,    0,  ..., 1493,    5,   81]])\n",
      "\n",
      "Example Output size:  torch.Size([32])\n",
      "Example Output:\n",
      " tensor([1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# get a batch of train data\n",
    "train_data_iter = iter(train_dataloader)\n",
    "X_example, y_example = next(train_data_iter)\n",
    "print('Example Input size: ', X_example.size()) # batch_size, seq_length\n",
    "print('Example Input:\\n', X_example)\n",
    "print()\n",
    "print('Example Output size: ', y_example.size()) # batch_size\n",
    "print('Example Output:\\n', y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dimension, embedding_dimension, hidden_dimension, output_dimension):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(input_dimension, embedding_dimension)  \n",
    "        self.rnn_layer = nn.RNN(embedding_dimension, hidden_dimension, num_layers=1)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension, output_dimension)\n",
    "        \n",
    "    def forward(self, sequence):\n",
    "        # sequence shape = (sequence_length, batch_size)\n",
    "        embedding = self.embedding_layer(sequence)  \n",
    "        # embedding shape = [sequence_length, batch_size, embedding_dimension]\n",
    "        output, hidden_state = self.rnn_layer(embedding)\n",
    "        # output shape = [sequence_length, batch_size, hidden_dimension]\n",
    "        # hidden_state shape = [1, batch_size, hidden_dimension]\n",
    "        final_output = self.fc_layer(hidden_state[-1,:,:].squeeze(0))      \n",
    "        return final_output\n",
    "    \n",
    "input_dimension = len(vocab_to_token)+1 # +1 to account for padding\n",
    "embedding_dimension = 100\n",
    "hidden_dimension = 32\n",
    "output_dimension = 1\n",
    "\n",
    "rnn_model = RNN(input_dimension, embedding_dimension, hidden_dimension, output_dimension)\n",
    "\n",
    "optim = torch.optim.Adam(rnn_model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss() # a numerically stable computation of a sigmoid function, followed by a binary cross-entropy\n",
    "\n",
    "rnn_model = rnn_model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "        Returns 0 or 1 for the given set of predictions and ground truth\n",
    "    \"\"\"\n",
    "    rounded_predictions = torch.round(torch.sigmoid(predictions))\n",
    "    success = (rounded_predictions == ground_truth).float()\n",
    "    acc = success.sum()/len(success)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "    for sequence, sentiment in dataloader:\n",
    "        optim.zero_grad()\n",
    "        preds = model(sequence.T).squeeze(1)\n",
    "        \n",
    "        loss_curr = loss_func(preds, sentiment)\n",
    "        accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "        \n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "\n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sequence, sentiment in dataloader:\n",
    "            preds = model(sequence.T).squeeze(1)\n",
    "            \n",
    "            loss_curr = loss_func(preds, sentiment)\n",
    "            accuracy_curr = accuracy_metric(preds, sentiment)\n",
    "\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "\n",
    "    return loss/len(dataloader), accuracy/len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Time: 24.05s\n",
      "Train Loss: 0.6235 | Train Accuracy: 0.6663\n",
      "Validation Loss: 1.0162 | Validation Accuracy: 0.2065\n",
      "--------------------------------------------------\n",
      "Epoch 2/5 | Time: 23.51s\n",
      "Train Loss: 0.5302 | Train Accuracy: 0.7404\n",
      "Validation Loss: 0.8853 | Validation Accuracy: 0.4906\n",
      "--------------------------------------------------\n",
      "Epoch 3/5 | Time: 24.01s\n",
      "Train Loss: 0.4322 | Train Accuracy: 0.8086\n",
      "Validation Loss: 0.7966 | Validation Accuracy: 0.5952\n",
      "--------------------------------------------------\n",
      "Epoch 4/5 | Time: 24.28s\n",
      "Train Loss: 0.4049 | Train Accuracy: 0.8273\n",
      "Validation Loss: 0.9485 | Validation Accuracy: 0.5321\n",
      "--------------------------------------------------\n",
      "Epoch 5/5 | Time: 23.96s\n",
      "Train Loss: 0.3078 | Train Accuracy: 0.8770\n",
      "Validation Loss: 0.8093 | Validation Accuracy: 0.6418\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 5\n",
    "best_validation_score = float('inf')\n",
    "for ep in range(num_epoch):\n",
    "    time_start = time.time()\n",
    "    train_loss, train_accuracy = train(rnn_model, train_dataloader, optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(rnn_model, validation_dataloader, loss_func)\n",
    "    time_end = time.time()\n",
    "    time_elapsed = time_end - time_start\n",
    "    if validation_loss < best_validation_score:\n",
    "        best_validation_score = validation_loss\n",
    "        torch.save(rnn_model.state_dict(), 'rnn_model.pt')\n",
    "    print(f'Epoch {ep+1}/{num_epoch} | Time: {time_elapsed:.2f}s')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}')\n",
    "    print(f'Validation Loss: {validation_loss:.4f} | Validation Accuracy: {validation_accuracy:.4f}')\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "\n",
    "    # text transformation\n",
    "    sentence = sentence.lower()\n",
    "    sentence = ''.join([c for c in sentence if c not in punctuation])\n",
    "    tokenized = [vocab_to_token.get(token, 0) for token in sentence.split()]\n",
    "    tokenized = np.pad(tokenized, (512-len(tokenized), 0), 'constant')\n",
    "\n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42833492159843445\n",
      "0.24501614272594452\n",
      "0.9373890161514282\n",
      "0.9171954393386841\n",
      "0.9674911499023438\n",
      "0.8580657839775085\n"
     ]
    }
   ],
   "source": [
    "print(sentiment_inference(rnn_model, \"This film is horrible\"))\n",
    "print(sentiment_inference(rnn_model, \"Director tried too hard but this film is bad\"))\n",
    "print(sentiment_inference(rnn_model, \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(rnn_model, \"I just really loved the movie\"))\n",
    "print(sentiment_inference(rnn_model, \"are you kidding me?\")) #!!\n",
    "print(sentiment_inference(rnn_model, \"Do Not Watch This Movie\")) #!! # so the model is still too basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further improvement:\n",
    "- work on data processing and maybe skip the \"stop words\"\n",
    "- encoder-decoder model maybe?\n",
    "- maybe using Bidirectional LSTMs rather than unidirectional one? --> [here](./bidirectional_lstm.ipynb)\n",
    "- maybe text processing and feature extraction through CNNs and then use RNNs for convergence?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35dc02d",
   "metadata": {},
   "source": [
    "## using Bidirectional LSTMs for sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7053d4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "LSTMs, as we know, are more capable of handling longer sequences due to their *memory cell gates*,\n",
    "which help retain important information from several time steps before and forget irrelevant information even if it was recent. With the exploding and vanishing gradients problem in check, LSTMs\n",
    "should be able to perform well when processing long movie reviews in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59964df3",
   "metadata": {},
   "source": [
    "so, we will be using a bidirectional model as it broadens the context window at any time step\n",
    "for the model to make a more informed decision about the sentiment of the movie review. The RNN\n",
    "model we looked at [here](./RNN.ipynb) overfitted the dataset during training, so to tackle that,\n",
    "we will be using *dropouts* as a *regularization* mechanism in our LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a938e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.9 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from torch==1.9) (4.13.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchtext==0.10 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: tqdm in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from torchtext==0.10) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from torchtext==0.10) (2.32.3)\n",
      "Requirement already satisfied: torch in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from torchtext==0.10) (1.9.0)\n",
      "Requirement already satisfied: numpy in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from torchtext==0.10) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from requests->torchtext==0.10) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from requests->torchtext==0.10) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from requests->torchtext==0.10) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from requests->torchtext==0.10) (2025.1.31)\n",
      "Requirement already satisfied: typing-extensions in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from torch->torchtext==0.10) (4.13.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib==3.8.3 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (3.8.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from matplotlib==3.8.3) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib==3.8.3) (3.21.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib==3.8.3) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# requires python3.9\n",
    "!pip install torch==1.9\n",
    "!pip install torchtext==0.10\n",
    "!pip install matplotlib==3.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74921ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4aff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchtext.legacy import datasets\n",
    "from torchtext.legacy import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65ca1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FIELD = data.Field(tokenize = data.get_tokenizer(\"basic_english\"), include_lengths = True)\n",
    "LABEL_FIELD = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_dataset, test_dataset = datasets.IMDB.splits(TEXT_FIELD, LABEL_FIELD)\n",
    "train_dataset, valid_dataset = train_dataset.split(random_state = random.seed(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1796afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCABULARY_SIZE = 25000\n",
    "\n",
    "TEXT_FIELD.build_vocab(train_dataset, \n",
    "                 max_size = MAX_VOCABULARY_SIZE)\n",
    "\n",
    "LABEL_FIELD.build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d1618b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data_iterator, valid_data_iterator, test_data_iterator = data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset, test_dataset), \n",
    "    batch_size = B_SIZE,\n",
    "    sort_within_batch = True,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0db02",
   "metadata": {},
   "source": [
    "#### instantiating and training LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171a14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you are training using GPUs, we need to use the following function for the pack_padded_sequence method to work \n",
    "## (reference : https://discuss.pytorch.org/t/error-with-lengths-in-pack-padded-sequence/35517/3)\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, PackedSequence\n",
    "\n",
    "def cuda_pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True):\n",
    "    lengths = torch.as_tensor(lengths, dtype=torch.int64)\n",
    "    lengths = lengths.cpu()\n",
    "    if enforce_sorted:\n",
    "        sorted_indices = None\n",
    "    else:\n",
    "        lengths, sorted_indices = torch.sort(lengths, descending=True)\n",
    "        sorted_indices = sorted_indices.to(input.device)\n",
    "    batch_dim = 0 if batch_first else 1\n",
    "    input = input.index_select(batch_dim, sorted_indices)\n",
    "\n",
    "    data, batch_sizes = \\\n",
    "    torch._C._VariableFunctions._pack_padded_sequence(input, lengths, batch_first)\n",
    "    return PackedSequence(data, batch_sizes, sorted_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d0b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lyteatnyte/Dev/github/Algorithms/venv3.9/lib/python3.9/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dimension, hidden_dimension, output_dimension, dropout, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocabulary_size, embedding_dimension, padding_idx = pad_index)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dimension, \n",
    "                           hidden_dimension, \n",
    "                           num_layers=1, \n",
    "                           bidirectional=True, \n",
    "                           dropout=dropout)\n",
    "        self.fc_layer = nn.Linear(hidden_dimension * 2, output_dimension) # note that we are using a bidirectional LSTM\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, sequence, sequence_lengths=None):\n",
    "        if sequence_lengths is None:\n",
    "            sequence_lengths = torch.LongTensor([len(sequence)])\n",
    "        \n",
    "        # sequence := (sequence_length, batch_size)\n",
    "        embedded_output = self.dropout_layer(self.embedding_layer(sequence))\n",
    "        \n",
    "        \n",
    "        # embedded_output := (sequence_length, batch_size, embedding_dimension)\n",
    "        if torch.cuda.is_available():\n",
    "            packed_embedded_output = cuda_pack_padded_sequence(embedded_output, sequence_lengths)\n",
    "        else:\n",
    "            packed_embedded_output = nn.utils.rnn.pack_padded_sequence(embedded_output, sequence_lengths)\n",
    "        \n",
    "        packed_output, (hidden_state, cell_state) = self.lstm_layer(packed_embedded_output)\n",
    "        # hidden_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        # cell_state := (num_layers * num_directions, batch_size, hidden_dimension)\n",
    "        \n",
    "        op, op_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "        # op := (sequence_length, batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        hidden_output = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)        \n",
    "        # hidden_output := (batch_size, hidden_dimension * num_directions)\n",
    "        \n",
    "        return self.fc_layer(hidden_output)\n",
    "\n",
    "    \n",
    "INPUT_DIMENSION = len(TEXT_FIELD.vocab)\n",
    "EMBEDDING_DIMENSION = 100\n",
    "HIDDEN_DIMENSION = 32\n",
    "OUTPUT_DIMENSION = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.pad_token]\n",
    "\n",
    "lstm_model = LSTM(INPUT_DIMENSION, \n",
    "            EMBEDDING_DIMENSION, \n",
    "            HIDDEN_DIMENSION, \n",
    "            OUTPUT_DIMENSION, \n",
    "            DROPOUT, \n",
    "            PAD_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b96cba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_INDEX = TEXT_FIELD.vocab.stoi[TEXT_FIELD.unk_token]\n",
    "\n",
    "lstm_model.embedding_layer.weight.data[UNK_INDEX] = torch.zeros(EMBEDDING_DIMENSION)\n",
    "lstm_model.embedding_layer.weight.data[PAD_INDEX] = torch.zeros(EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ba26fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(lstm_model.parameters())\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lstm_model = lstm_model.to(device)\n",
    "loss_func = loss_func.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dfd7dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(predictions, ground_truth):\n",
    "    predictions = torch.round(torch.sigmoid(predictions))\n",
    "    correct_predictions = (predictions == ground_truth).float()\n",
    "    accuracy = correct_predictions.sum() / len(correct_predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da441f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_iterator, optim, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.train()\n",
    "\n",
    "    for curr_batch in data_iterator:\n",
    "        optim.zero_grad()\n",
    "        sequence, sequence_lengths = curr_batch.text\n",
    "        preds = lstm_model(sequence, sequence_lengths).squeeze(1)\n",
    "\n",
    "        loss_curr = loss_func(preds, curr_batch.label)\n",
    "        accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "\n",
    "        loss_curr.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss += loss_curr.item()\n",
    "        accuracy += accuracy_curr.item()\n",
    "    \n",
    "    return loss / len(data_iterator), accuracy / len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e7d6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_iterator, loss_func):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for curr_batch in data_iterator:\n",
    "            sequence, sequence_lengths = curr_batch.text\n",
    "            preds = lstm_model(sequence, sequence_lengths).squeeze(1)\n",
    "\n",
    "            loss_curr = loss_func(preds, curr_batch.label)\n",
    "            accuracy_curr = accuracy_metric(preds, curr_batch.label)\n",
    "\n",
    "            loss += loss_curr.item()\n",
    "            accuracy += accuracy_curr.item()\n",
    "\n",
    "    return loss / len(data_iterator), accuracy / len(data_iterator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e28f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    time_start = time.time()\n",
    "\n",
    "    training_loss, train_accuracy = train(lstm_model, train_data_iterator, optim, loss_func)\n",
    "    validation_loss, validation_accuracy = validate(lstm_model, valid_data_iterator, loss_func)\n",
    "\n",
    "    time_end = time.time()\n",
    "    time_delta = time_end - time_start\n",
    "\n",
    "    if validation_loss < best_validation_loss:\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(lstm_model.state_dict(), 'lstm_model.pt')\n",
    "    \n",
    "    print(f'Epoch Number: {ep+1} | Time Elapsed: {time_delta}s')\n",
    "    print(f'Training Loss: {training_loss:.3f} | Training Accuracy: {train_accuracy*100:.2f}%')\n",
    "    print(f'validation loss: {validation_loss:.3f} |  validation accuracy: {validation_accuracy*100:.2f}%')\n",
    "    print(50*\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f3ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best-performing model and evaluate on test-set\n",
    "lstm_model.load_state_dict(torch.load('./lstm_model.pt'))\n",
    "\n",
    "test_loss, test_accuracy = validate(lstm_model, test_data_iterator, loss_func)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Accuracy: {test_accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_inference(model, sentence):\n",
    "    model.eval()\n",
    "\n",
    "    # text transformation\n",
    "    tokenized = data.get_tokenizer('basic_english')(sentence)\n",
    "    tokenized = [TEXT_FIELD.vocab.stoi[t] for t in tokenized]\n",
    "\n",
    "    # model inference\n",
    "    model_input = torch.LongTensor(tokenized).to(device)\n",
    "    model_input = model_input.unsqueeze(1)\n",
    "\n",
    "    pred = torch.sigmoid(model(model_input))\n",
    "\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentiment_inference(lstm_model, \"This film is horrible\"))\n",
    "print(sentiment_inference(lstm_model, \"Director tried too hard but this film is bad\"))\n",
    "print(sentiment_inference(lstm_model, \"This film will be houseful for weeks\"))\n",
    "print(sentiment_inference(lstm_model, \"I just really loved the movie\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

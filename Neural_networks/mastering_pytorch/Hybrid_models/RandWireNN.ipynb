{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9be589f",
   "metadata": {},
   "source": [
    "#### RandWireNN (the idea of optimizing for model architectures rather than optimizing for just the model parameters while fixing the architecture.)\n",
    "\n",
    "worth noting the actual [paper](https://arxiv.org/pdf/1904.01569) which introduced the concept of Randomly generating neural networks in Neural Architecture Search (NAS) framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d133a7",
   "metadata": {},
   "source": [
    "##### Some note and overview of the concept:\n",
    "\n",
    "The concept of RandWireNN (Randomly Wired Neural Networks) is a fascinating idea that explores the use of **random graph theory** in the design of neural network architectures. Instead of manually designing the topology of a neural network (like ResNet or DenseNet), RandWireNN *generates the architecture as a random graph where nodes and edges define the computation and data flow*\n",
    "\n",
    "in this directed acyclic graph (DAG) $G(V, E)$:\n",
    "- $V$ (*Nodes*): Each node is a computational unit (e.g., **Conv+BN+ReLU**).\n",
    "- $E$ (*Edges*): Each edge defines a data connection between two nodes.\n",
    "\n",
    "Data flows only along the edges, and computation occurs at the nodes.\n",
    "\n",
    "Each node receives data from its input edges, **aggregates** them (e.g., using summation), then applies:\n",
    "- Standard Convolution (like a 3×3 or 1×1 conv)\n",
    "- BatchNorm\n",
    "- ReLU\n",
    "- Possibly Dropout or other variants\n",
    "\n",
    "So a node is like a **tiny layer**, but its position and number of connections (degree) are decided by the random graph.\n",
    "\n",
    "the concept of **Watts–Strogatz (WS)**:\n",
    "\n",
    "Starts with a ring lattice and randomly rewires edges to create small-world networks, preserving locality while adding shortcuts. This gives **high clustering** and **short average path lengths**.\n",
    "\n",
    "The overall architecture is divided into **stages**, and each stage is a *randomly wired graph block* (e.g., like a ResNet stage). Between stages, downsampling happens. \n",
    "\n",
    "The process:\n",
    "1. Input image goes through a stem layer (initial conv).\n",
    "2. Data flows through several random graph blocks, one per stage.\n",
    "3. Final node’s output goes to the classifier (like an FC + Softmax).\n",
    "\n",
    "Each stage is independently generated and wired using one of the graph models (ER, WS, BA), and nodes within a stage operate in parallel with inputs routed via edges.\n",
    "\n",
    "overall we can summerize the concept as follows:\n",
    "\n",
    "| Element      | Meaning in RandWireNN                                           |\n",
    "|--------------|------------------------------------------------------------------|\n",
    "| **Node**     | A computational unit (Conv + BN + ReLU)                         |\n",
    "| **Edge**     | A data path; connects one node's output to another’s input      |\n",
    "| **Graph**    | A DAG generated randomly (ER, WS, BA)                           |\n",
    "| **Stage**    | A group of nodes/edges treated as a layer block                 |\n",
    "| **Network**  | Sequentially stacked graph-based stages                         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25126399",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==2.2\n",
    "!pip install torchvision==0.17.0\n",
    "!pip install networkx==2.8.5\n",
    "!pip install torchviz==0.0.2\n",
    "!pip install matplotlib==3.5.2\n",
    "!pip install yaml\n",
    "\n",
    "!brew install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497c9c3e",
   "metadata": {},
   "source": [
    "a **random graph generation algorithm** is used to generate a random graph with a predefined\n",
    "number of nodes. This graph is converted into a neural network by a few definitions being imposed\n",
    "on it, such as the following:\n",
    "- **Directed**: The graph is restricted to be a directed graph, and the direction of the edge is the\n",
    "direction of the data flow in the equivalent neural network.\n",
    "- **Aggregation**: Multiple incoming edges to a node (or neuron) are aggregated by weighted sum,\n",
    "where the weights are learnable.\n",
    "- **Transformation**: Inside each node of this graph, a standard operation is applied: ReLU, followed\n",
    "by $3 \\times 3$ separable convolution (that is, a regular $3 \\times 3$ convolution followed by a $1 \\times 1$ pointwise\n",
    "convolution), followed by batch normalization. This operation is also referred to as a ReLU-\n",
    "Conv-BN triplet.\n",
    "- **Distribution**: Lastly, multiple outgoing edges from each neuron carry a copy of the triplet\n",
    "operation.\n",
    "\n",
    "One final piece in the puzzle is to add a single input node (**source**) and a single output node (**sink**) to\n",
    "this graph to fully transform the random graph into a neural network. Once the graph is realized as\n",
    "a neural network, it can be trained for various machine learning tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339d4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchviz import make_dot\n",
    "\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c223ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot\n",
    "def plot_results(list_of_epochs, list_of_train_losses, list_of_train_accuracies, list_of_val_accuracies):\n",
    "    plt.figure(figsize=(20, 9))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list_of_epochs, list_of_train_losses, label='training loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(list_of_epochs, list_of_train_accuracies, label='training accuracy')\n",
    "    plt.plot(list_of_epochs, list_of_val_accuracies, label='validation accuracy')\n",
    "    plt.legend()\n",
    "    if not os.path.isdir('./result_plots'):\n",
    "        os.makedirs('./result_plots')\n",
    "    plt.savefig('./result_plots/accuracy_plot_per_epoch.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4b0a7",
   "metadata": {},
   "source": [
    "##### Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75461117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_lr(optim, epoch_num, lrate):\n",
    "    \"\"\"adjusts lr to starting lr thereafter reduced by 10% at every 20 epochs\"\"\"\n",
    "    lrate = lrate * (0.1 ** (epoch_num // 20))\n",
    "    for params in optim.param_groups:\n",
    "        params['lr'] = lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9294c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optim, loss_func, epoch_num, lrate):\n",
    "    model.train()\n",
    "    loop_iter = 0\n",
    "    training_loss = 0\n",
    "    training_accuracy = 0\n",
    "    for training_data, training_label in train_dataloader:\n",
    "        set_lr(optim, epoch_num, lrate)\n",
    "        training_data, training_label = training_data.to(device), training_label.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_raw = model(training_data)\n",
    "        curr_loss = loss_func(pred_raw, training_label)\n",
    "        curr_loss.backward()\n",
    "        optim.step()\n",
    "        training_loss += curr_loss.data\n",
    "        pred = pred_raw.data.max(1)[1]\n",
    "\n",
    "        curr_accuracy = float(pred.eq(training_label.data).sum()) * 100. / len(training_data) \n",
    "        training_accuracy += curr_accuracy\n",
    "        loop_iter += 1\n",
    "        if loop_iter % 100 == 0:\n",
    "            print(f\"epoch {epoch_num}, loss: {curr_loss.data}, accuracy: {curr_accuracy}\")\n",
    "\n",
    "    data_size = len(train_dataloader.dataset) // batch_size\n",
    "    return training_loss / data_size, training_accuracy / data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9caca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, test_data_loader):\n",
    "    model.eval()\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for test_data, test_label in test_data_loader:\n",
    "            test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "            pred_raw = model(test_data)\n",
    "            pred = pred_raw.data.max(1)[1]\n",
    "            success += pred.eq(test_label.data).sum()\n",
    "\n",
    "    return float(success) * 100. / len(test_data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9626c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "num_epochs = 5\n",
    "graph_probability = 0.7\n",
    "node_channel_count = 64\n",
    "num_nodes = 16\n",
    "lrate = 0.1\n",
    "batch_size = 64\n",
    "train_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df0360",
   "metadata": {},
   "source": [
    "##### DataLoader and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c6d92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(batch_size):\n",
    "    transform_train_dataset = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4983, 0.4795, 0.4382), (0.2712, 0.2602, 0.2801)),\n",
    "    ])\n",
    "\n",
    "    transform_test_dataset = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4983, 0.4795, 0.4382), (0.2712, 0.2602, 0.2801)),\n",
    "    ])\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('dataset', transform=transform_train_dataset, train=True, download=True),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('dataset', transform=transform_test_dataset, train=False),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return train_dataloader, test_dataloader\n",
    "train_dataloader, test_dataloader = load_dataset(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356170aa",
   "metadata": {},
   "source": [
    "##### define a graph generator in order to generate a random graph that will be later used as a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b93bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RndGraph(object):\n",
    "    \"\"\" \n",
    "        This class handles the generation and management of the random graph \n",
    "        topology using Watts-Strogatz small-world networks from networkx.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes, graph_probability, nearest_neighbour_k=4, num_edges_attach=5):\n",
    "        self.num_nodes = num_nodes\n",
    "        self.graph_probability = graph_probability\n",
    "        self.nearest_neighbour_k = nearest_neighbour_k\n",
    "        self.num_edges_attach = num_edges_attach\n",
    "\n",
    "    def make_graph_obj(self):\n",
    "        graph_obj = nx.random_graphs.connected_watts_strogatz_graph(self.num_nodes, self.nearest_neighbour_k, \n",
    "                                                                self.graph_probability)\n",
    "        return graph_obj\n",
    "\n",
    "    def get_graph_config(self, graph_obj):\n",
    "        \"\"\" converts the undirected graph to a directed acyclic graph (DAG) to ensure proper forward computation\"\"\"\n",
    "        incoming_edges = {}\n",
    "        incoming_edges[0] = [] # input node has no inputs\n",
    "        node_list = [0]\n",
    "        last = []\n",
    "        for n in graph_obj.nodes():\n",
    "            neighbor_list = list(graph_obj.neighbors(n))\n",
    "            neighbor_list.sort()\n",
    "\n",
    "            edge_list = []\n",
    "            passed_list = []\n",
    "            for nbr in neighbor_list:\n",
    "                # Only allow edges from lower-numbered nodes to higher ones\n",
    "                if n > nbr:\n",
    "                    edge_list.append(nbr + 1)\n",
    "                    passed_list.append(nbr)\n",
    "            if not edge_list:\n",
    "                edge_list.append(0)\n",
    "            incoming_edges[n + 1] = edge_list\n",
    "            if passed_list == neighbor_list:\n",
    "                last.append(n + 1)\n",
    "            node_list.append(n + 1)\n",
    "        incoming_edges[self.num_nodes + 1] = last # output node gets all leaf nodes\n",
    "        node_list.append(self.num_nodes + 1)\n",
    "        return node_list, incoming_edges\n",
    "\n",
    "    def save_graph(self, graph_obj, path_to_write):\n",
    "        if not os.path.isdir(\"cached_graph_obj\"):\n",
    "            os.mkdir(\"cached_graph_obj\")\n",
    "        with open(f\"./cached_graph_obj/{path_to_write}\", \"w\") as fh:\n",
    "            yaml.dump(graph_obj, fh)\n",
    "\n",
    "    def load_graph(self, path_to_read):\n",
    "        with open(f\"./cached_graph_obj/{path_to_read}\", \"r\") as fh:\n",
    "            return yaml.load(fh, Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed59e7b4",
   "metadata": {},
   "source": [
    "##### Transforming Random Graph to Neural network\n",
    "\n",
    "First, design some neural modules to facilitate that transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layer):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(layer.weight)\n",
    "        if layer.bias is not None:\n",
    "            torch.nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4b1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The separable convolutional layer is a cascade of a regular 3x3 2D convolutional layer, followed\n",
    "# by a pointwise 1x1 2D convolutional layer\n",
    "class SepConv2d(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, kernel_length=3, dilation_size=1, padding_size=1, stride_length=1, bias_flag=True):\n",
    "        super(SepConv2d, self).__init__()\n",
    "        # as the definition for standard Conv Layer -> 3x3 followed by 1x1\n",
    "        self.conv_layer = nn.Conv2d(input_ch, input_ch, kernel_length, stride_length, padding_size, dilation_size, \n",
    "                              bias=bias_flag, groups=input_ch)\n",
    "        self.pointwise_layer = nn.Conv2d(input_ch, output_ch, kernel_size=1, stride=1, padding=0, dilation=1, \n",
    "                                         groups=1, bias=bias_flag)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pointwise_layer(self.conv_layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96dea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitLayer(nn.Module):\n",
    "    def __init__(self, input_ch, output_ch, stride_length=1):\n",
    "        super(UnitLayer, self).__init__()\n",
    "\n",
    "        self.dropout = 0.3\n",
    "\n",
    "        self.unit_layer = nn.Sequential(\n",
    "            # ReLU-Conv-BN triplet unit\n",
    "            nn.ReLU(),\n",
    "            SepConv2d(input_ch, output_ch, stride_length=stride_length),\n",
    "            nn.BatchNorm2d(output_ch),\n",
    "            nn.Dropout(self.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.unit_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd4e83c",
   "metadata": {},
   "source": [
    "With the triplet unit in place, we can now define a node in the graph with all of the **aggregation**,\n",
    "**transformation**, and **distribution** functionalities we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48787357",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNode(nn.Module):\n",
    "    def __init__(self, input_degree, input_ch, output_ch, stride_length=1):\n",
    "        super(GraphNode, self).__init__()\n",
    "        self.input_degree = input_degree\n",
    "        if len(self.input_degree) > 1:\n",
    "            self.params = nn.Parameter(torch.ones(len(self.input_degree), requires_grad=True))\n",
    "        self.unit_layer = UnitLayer(input_ch, output_ch, stride_length=stride_length)\n",
    "\n",
    "\n",
    "    def forward(self, *ip):\n",
    "        if len(self.input_degree) > 1:\n",
    "            op = (ip[0] * torch.sigmoid(self.params[0]))\n",
    "            for idx in range(1, len(ip)):\n",
    "                op += (ip[idx] * torch.sigmoid(self.params[idx]))\n",
    "            # The triplet unit is applied to the weighted average and the transformed (ReLU-Conv-\n",
    "            # BN-ed) output is returned\n",
    "            return self.unit_layer(op)\n",
    "        else:\n",
    "            return self.unit_layer(ip[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16534b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandWireGraph(nn.Module):\n",
    "    \"\"\" represents one stage of a RandWireNN. A stage is modeled as a random DAG where\"\"\"\n",
    "    def __init__(self, num_nodes, graph_prob, input_ch, output_ch, train_mode, graph_name):\n",
    "        super(RandWireGraph, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.graph_prob = graph_prob\n",
    "        self.input_ch = input_ch\n",
    "        self.output_ch = output_ch\n",
    "        self.train_mode = train_mode\n",
    "        self.graph_name = graph_name\n",
    "\n",
    "        # get graph nodes and in edges\n",
    "        rnd_graph_node = RndGraph(self.num_nodes, self.graph_prob)\n",
    "        if self.train_mode is True:\n",
    "            print(\"train_mode: ON\")\n",
    "            rnd_graph = rnd_graph_node.make_graph_obj()\n",
    "            self.node_list, self.incoming_edge_list = rnd_graph_node.get_graph_config(rnd_graph)\n",
    "            rnd_graph_node.save_graph(rnd_graph, graph_name)\n",
    "        else:\n",
    "            rnd_graph = rnd_graph_node.load_graph(graph_name)\n",
    "            self.node_list, self.incoming_edge_list = rnd_graph_node.get_graph_config(rnd_graph)\n",
    "\n",
    "        # define input Node\n",
    "        self.list_of_modules = nn.ModuleList([GraphNode(self.incoming_edge_list[0], self.input_ch, self.output_ch, \n",
    "                                                        stride_length=2)])\n",
    "        # define the rest Node\n",
    "        self.list_of_modules.extend([GraphNode(self.incoming_edge_list[n], self.output_ch, self.output_ch) \n",
    "                                     for n in self.node_list if n > 0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem_dict = {} # store the outputs of each node\n",
    "        # start vertex\n",
    "        op = self.list_of_modules[0].forward(x)\n",
    "        mem_dict[0] = op\n",
    "\n",
    "        # the rest vertex\n",
    "        for n in range(1, len(self.node_list) - 1):\n",
    "            # print(node, self.in_edges[node][0], self.in_edges[node])\n",
    "            if len(self.incoming_edge_list[n]) > 1:\n",
    "                op = self.list_of_modules[n].forward(*[mem_dict[incoming_vtx] \n",
    "                                                       for incoming_vtx in self.incoming_edge_list[n]])\n",
    "            else:\n",
    "                op = self.list_of_modules[n].forward(mem_dict[self.incoming_edge_list[n][0]])\n",
    "            mem_dict[n] = op\n",
    "            \n",
    "        op = mem_dict[self.incoming_edge_list[self.num_nodes + 1][0]]\n",
    "        for incoming_vtx in range(1, len(self.incoming_edge_list[self.num_nodes + 1])):\n",
    "            op += mem_dict[self.incoming_edge_list[self.num_nodes + 1][incoming_vtx]]\n",
    "        return op / len(self.incoming_edge_list[self.num_nodes + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea137ce",
   "metadata": {},
   "source": [
    "summary until now: (flow is top to buttom)\n",
    "\n",
    "Graph Construction (RndGraph) --> Node & Edge list\n",
    "\n",
    "Each Node = GraphNode = f(inputs, learnable_weights)\n",
    "\n",
    "Connect via DAG edges\n",
    "\n",
    "Forward pass = flow of tensor data\n",
    "\n",
    "Final node = Aggregation of leaf nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054bec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandWireNNModel(nn.Module):\n",
    "    def __init__(self, num_nodes, graph_prob, input_ch, output_ch, train_mode):\n",
    "        super(RandWireNNModel, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.graph_prob = graph_prob\n",
    "        self.input_ch = input_ch\n",
    "        self.output_ch = output_ch\n",
    "        self.train_mode = train_mode\n",
    "        self.dropout = 0.3\n",
    "        self.class_num = 10\n",
    "            \n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.output_ch, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(self.output_ch),\n",
    "        )\n",
    "\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            RandWireGraph(self.num_nodes, self.graph_prob, self.input_ch, self.output_ch*2, self.train_mode, \n",
    "                          graph_name=\"conv_layer_2\")\n",
    "        )\n",
    "        self.conv_layer_3 = nn.Sequential(\n",
    "            RandWireGraph(self.num_nodes, self.graph_prob, self.input_ch*2, self.output_ch*4, self.train_mode, \n",
    "                          graph_name=\"conv_layer_3\")\n",
    "        )\n",
    "        self.conv_layer_4 = nn.Sequential(\n",
    "            RandWireGraph(self.num_nodes, self.graph_prob, self.input_ch*4, self.output_ch*8, self.train_mode, \n",
    "                          graph_name=\"conv_layer_4\")\n",
    "        )\n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.input_ch*8, out_channels=1280, kernel_size=1),\n",
    "            nn.BatchNorm2d(1280)\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(1280, self.class_num)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer_4(x)\n",
    "        x = self.classifier_layer(x)\n",
    "\n",
    "        # global average pooling\n",
    "        _, _, h, w = x.size() # (N, C, H, W)\n",
    "        # average pooling helps reduce dimensionality and the number of\n",
    "        # parameters in the network.\n",
    "        x = F.avg_pool2d(x, kernel_size=[h, w]) # get the spatial size of the feature map after Conv layers\n",
    "        x = torch.squeeze(x)\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd5a8b8",
   "metadata": {},
   "source": [
    "#### Training Routine and execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "graph_probability = 0.7\n",
    "node_channel_count = 64\n",
    "num_nodes = 16\n",
    "lrate = 0.1\n",
    "batch_size = 64\n",
    "train_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_wire_model = RandWireNNModel(num_nodes, graph_probability, node_channel_count, node_channel_count, train_mode).to(device)\n",
    "\n",
    "optim_module = optim.SGD(rand_wire_model.parameters(), lr=lrate, weight_decay=1e-4, momentum=0.8)\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "epochs = []\n",
    "test_accuracies = []\n",
    "training_accuracies = []\n",
    "training_losses = []\n",
    "best_test_accuracy = 0\n",
    "\n",
    "start_time = time.time()\n",
    "for ep in range(1, num_epochs + 1):\n",
    "    epochs.append(ep)\n",
    "    training_loss, training_accuracy = train(rand_wire_model, train_dataloader, optim_module, loss_func, ep, lrate)\n",
    "    test_accuracy = accuracy(rand_wire_model, test_dataloader)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    training_losses.append(training_loss)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    print('test acc: {0:.2f}%, best test acc: {1:.2f}%'.format(test_accuracy, best_test_accuracy))\n",
    "\n",
    "    if best_test_accuracy < test_accuracy:\n",
    "        model_state = {\n",
    "            'model': rand_wire_model.state_dict(),\n",
    "            'accuracy': test_accuracy,\n",
    "            'ep': ep,\n",
    "        }\n",
    "        if not os.path.isdir('model_checkpoint'):\n",
    "            os.mkdir('model_checkpoint')\n",
    "        model_filename = \"ch_count_\" + str(node_channel_count) + \"_prob_\" + str(graph_probability)\n",
    "        torch.save(model_state, './model_checkpoint/' + model_filename + 'ckpt.t7')\n",
    "        best_test_accuracy = test_accuracy\n",
    "        plot_results(epochs, training_losses, training_accuracies, test_accuracies)\n",
    "    print(\"model train time: \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215fef69",
   "metadata": {},
   "source": [
    "#### Evaluating and visualizing the RandWireNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_model_params(model_obj):\n",
    "    num_params = 0\n",
    "    for l in list(model_obj.parameters()):\n",
    "        l_p = 1\n",
    "        for p in list(l.size()):\n",
    "            l_p *= p\n",
    "        num_params += l_p\n",
    "    return num_params\n",
    "print(\"total model params: \", num_model_params(rand_wire_model))\n",
    "\n",
    "# total model params:  6900326 -> roughly 6M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64fd620",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./model_checkpoint\"):\n",
    "    rand_wire_nn_model = RandWireNNModel(num_nodes, graph_probability, node_channel_count, node_channel_count, \n",
    "                                         train_mode=False).to(device)\n",
    "    model_filename = \"ch_count_\" + str(node_channel_count) + \"_prob_\" + str(graph_probability)\n",
    "    model_checkpoint = torch.load('./model_checkpoint/' + model_filename + 'ckpt.t7')\n",
    "    rand_wire_nn_model.load_state_dict(model_checkpoint['model'])\n",
    "    last_ep = model_checkpoint['ep']\n",
    "    best_model_accuracy = model_checkpoint['accuracy']\n",
    "    print(f\"best model accuracy: {best_model_accuracy}%, last epoch: {last_ep}\")\n",
    "\n",
    "    rand_wire_nn_model.eval()\n",
    "    success = 0\n",
    "    for test_data, test_label in test_dataloader:\n",
    "        test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "        pred_raw = rand_wire_nn_model(test_data)\n",
    "        pred = pred_raw.data.max(1)[1]\n",
    "        success += pred.eq(test_label.data).sum()\n",
    "    print(f\"test accuracy: {float(success) * 100. / len(test_dataloader.dataset)} %\")\n",
    "\n",
    "else:\n",
    "    assert False, \"File not found. Please check again.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f2fdb",
   "metadata": {},
   "source": [
    "full image of the trained Neural Net Architecture can be found [here](./randwirenn.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

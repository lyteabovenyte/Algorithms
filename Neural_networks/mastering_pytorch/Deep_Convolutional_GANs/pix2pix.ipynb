{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7beb525e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    body {\n",
       "        --vscode-font-family: \"Sherif\",;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "    body {\n",
    "        --vscode-font-family: \"Sherif\",;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77057f48",
   "metadata": {},
   "source": [
    "#### Using GANs for style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea0664",
   "metadata": {},
   "source": [
    "pix2pix is a conditional GAN that differs from regular GANs in that it uses paired input-output data during training, which allows it to learn a direct mapping between input\n",
    "and output images. This enables it to generate high-quality images with fine details, while\n",
    "other GANs may struggle with this due to the lack of paired training data. Additionally,\n",
    "pix2pix can generate images that satisfy specific constraints or requirements, making it\n",
    "useful for tasks such as image-to-image translation and image editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aab939",
   "metadata": {},
   "source": [
    "Instead of taking in random noise as input and generating an image, the generator\n",
    "in a pix2pix model takes in a real image as input and tries to generate a translated version of that image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5b3557",
   "metadata": {},
   "source": [
    "<img src=../images/pix2pix-arch.png width=750 style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67982c94",
   "metadata": {},
   "source": [
    "#### More on U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeef481",
   "metadata": {},
   "source": [
    "A key property of UNet is the **skip connections** â€“ that is, the concatenation of features (along the depth\n",
    "dimension) from the encoder section to the decoder section. Using features from the encoder section helps the decoder to better localize the high-resolution information at each upsampling step. in this architecture, Essentially, the encoder section is a sequence of down-convolutional blocks, where each down-convolutional block is itself a sequence of a 2D convolutional layer, an instance *normalization layer*, and a\n",
    "*leaky ReLU activation*. Similarly, the decoder section consists of a sequence of up-convolutional blocks,\n",
    "where each block is a sequence of a 2D-transposed convolutional layer, an instance normalization\n",
    "layer, and a ReLU activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f7cd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpConvBlock(nn.Module):\n",
    "    def __init__(self, ip_sz, op_sz, dropout=0.0):\n",
    "        super(UpConvBlock, self).__init__()\n",
    "        self.layers = [\n",
    "            nn.ConvTranspose2d(ip_sz, op_sz, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(op_sz),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "        if dropout:\n",
    "            self.layers += [nn.Dropout(dropout)]\n",
    "    def forward(self, x, enc_ip):\n",
    "        x = nn.Sequential(*(self.layers))(x)\n",
    "        op = torch.cat((x, enc_ip), 1) # skip connection from encoder\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e241c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownConvBlock(nn.Module):\n",
    "    def __init__(self, ip_sz, op_sz, norm=True, dropout=0.0):\n",
    "        super(DownConvBlock, self).__init__()\n",
    "        self.layers = [nn.Conv2d(ip_sz, op_sz, 4, 2, 1)]\n",
    "        if norm:\n",
    "            self.layers.append(nn.InstanceNorm2d(op_sz))\n",
    "        self.layers += [nn.LeakyReLU(0.2)]\n",
    "        if dropout:\n",
    "            self.layers += [nn.Dropout(dropout)]\n",
    "    def forward(self, x):\n",
    "        op = nn.Sequential(*(self.layers))(x)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd477d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, chnls_in=3, chnls_op=3):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "        self.down_conv_layer_1 = DownConvBlock(chnls_in, 64, norm=False)\n",
    "        self.down_conv_layer_2 = DownConvBlock(64, 128)\n",
    "        self.down_conv_layer_3 = DownConvBlock(128, 256)\n",
    "        self.down_conv_layer_4 = DownConvBlock(256, 512, dropout=0.5)\n",
    "        self.down_conv_layer_5 = DownConvBlock(512, 512, dropout=0.5)\n",
    "        self.down_conv_layer_6 = DownConvBlock(512, 512, dropout=0.5)\n",
    "        self.down_conv_layer_7 = DownConvBlock(512, 512, dropout=0.5)\n",
    "        self.down_conv_layer_8 = DownConvBlock(512, 512, norm=False, dropout=0.5)\n",
    "        self.up_conv_layer_1 = UpConvBlock(512, 512, dropout=0.5)\n",
    "        self.up_conv_layer_2 = UpConvBlock(1024, 512, dropout=0.5)\n",
    "        self.up_conv_layer_3 = UpConvBlock(1024, 512, dropout=0.5)\n",
    "        self.up_conv_layer_4 = UpConvBlock(1024, 512, dropout=0.5)\n",
    "        self.up_conv_layer_5 = UpConvBlock(1024, 256)\n",
    "        self.up_conv_layer_6 = UpConvBlock(512, 128)\n",
    "        self.up_conv_layer_7 = UpConvBlock(256, 64)\n",
    "        self.upsample_layer = nn.Upsample(scale_factor=2)\n",
    "        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0))\n",
    "        self.conv_layer_1 = nn.Conv2d(128, chnls_op, 4, padding=1)\n",
    "        self.activation = nn.Tanh()\n",
    "    def forward(self, x):\n",
    "        enc1 = self.down_conv_layer_1(x)\n",
    "        enc2 = self.down_conv_layer_2(enc1)\n",
    "        enc3 = self.down_conv_layer_3(enc2)\n",
    "        enc4 = self.down_conv_layer_4(enc3)\n",
    "        enc5 = self.down_conv_layer_5(enc4)\n",
    "        enc6 = self.down_conv_layer_6(enc5)\n",
    "        enc7 = self.down_conv_layer_7(enc6)\n",
    "        enc8 = self.down_conv_layer_8(enc7)\n",
    "        dec1 = self.up_conv_layer_1(enc8, enc7)\n",
    "        dec2 = self.up_conv_layer_2(dec1, enc6)\n",
    "        dec3 = self.up_conv_layer_3(dec2, enc5)\n",
    "        dec4 = self.up_conv_layer_4(dec3, enc4)\n",
    "        dec5 = self.up_conv_layer_5(dec4, enc3)\n",
    "        dec6 = self.up_conv_layer_6(dec5, enc2)\n",
    "        dec7 = self.up_conv_layer_7(dec6, enc1)\n",
    "        final = self.upsample_layer(dec7)\n",
    "        final = self.zero_pad(final)\n",
    "        final = self.conv_layer_1(final)\n",
    "        return self.activation(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b9315c",
   "metadata": {},
   "source": [
    "<img src=../images/pix2pix_disc.png width=650 style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97178021",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2PixDiscriminator(nn.Module):\n",
    "    def __init__(self, chnls_in=3):\n",
    "        super(Pix2PixDiscriminator, self).__init__()\n",
    "        def disc_conv_block(chnls_in, chnls_op, norm=1):\n",
    "            layers = [nn.Conv2d(chnls_in, chnls_op, 4, stride=2, padding=1)]\n",
    "            if norm:\n",
    "                layers.append(nn.InstanceNorm2d(chnls_op))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        self.lyr1 = disc_conv_block(chnls_in * 2, 64, norm=0)\n",
    "        self.lyr2 = disc_conv_block(64, 128)\n",
    "        self.lyr3 = disc_conv_block(128, 256)\n",
    "        self.lyr4 = disc_conv_block(256, 512)\n",
    "    def forward(self, real_image, translated_image):\n",
    "        ip = torch.cat((real_image, translated_image), 1)\n",
    "        op = self.lyr1(ip)\n",
    "        op = self.lyr2(op)\n",
    "        op = self.lyr3(op)\n",
    "        op = self.lyr4(op)\n",
    "        op = nn.ZeroPad2d((1, 0, 1, 0))(op)\n",
    "        op = nn.Conv2d(512, 1, 4, padding=1)(op)\n",
    "        return op"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

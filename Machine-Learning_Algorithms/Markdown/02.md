#### Markov chain monte carlo

- Any probabilistic model that explains a part of reality
exists in a high-dimensional parameter space because it is described by high dimensional model parameters.
*Markov chain Monte Carlo (MCMC)* is a methodology of
sampling from high-dimensional parameter spaces to approximate the posterior distribution $p(\theta |x)$.

- A *Gaussian mixture* is a weighted sum of Gaussian probability distributions. A mixture of Gaussians consists of K Gaussian RVs scaled by mixture proportions πk that are positive and add up to 1

- A Gaussian Mixture Model (GMM) is a probabilistic model that represents a dataset as a combination of multiple Gaussian distributions. It is a soft clustering technique that generalizes the *k-means* algorithm by allowing clusters to have different shapes, sizes, and orientations. $p(x) = \sum_{k=1}^{K} \pi_k \cdot \mathcal{N}(x \mid \mu_k, \Sigma_k)$

- How to find the Gaussian mixture parameters via the *expectation-maximization (EM)* algorithm?

- With MCMC algorithms, we attempt to approximate the posterior distribution
through samples. In fact, most of the Bayesian inference is designed to efficiently
approximate the posterior distribution

- be familiar with this notation: $p(x|\theta)$, which is, by definition, the **likelihood** of data x, given the parameter $\theta$.

- Variance Definition:
    - Variance helps us quantify the spread of a dataset mathematically.
    - For a random variable  $X$  with expected value  $E[X] = \mu$ , the variance, denoted as  $\text{Var}(X)$ , is: $\text{Var}(X) = E[(X - \mu)^2]$
    - This gives us an idea of how “far” values are from the mean on average.
    - For a discrete random variable  $X$  with $PMF  p(x)$ , variance is: $\text{Var}(X) = \sum_{x} (x - \mu)^2 p(x)$
    - For a continuous random variable  $X$  with $\text{PDF}  f(x)$ , variance is: $\text{Var}(X) = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) dx$
- Alternative Variance Formula:
    - A useful identity for variance is: $\text{Var}(X) = E[X^2] - (E[X])^2$
    - Proof:
        - $\text{Var}(X) = E[(X - \mu)^2] = E[X^2 - 2\mu X + \mu^2] = E[X^2] - 2\mu E[X] + \mu^2$,  since $E[X] = \mu$, $\text{Var}(X) = E[X^2] - \mu^2$

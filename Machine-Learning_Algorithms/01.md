- Understanding 'Gaussian':
  - “Gaussian” refers to the Gaussian (Normal) distribution, one of the most fundamental probability distributions in statistics.
  - It is bell-shaped, symmetric around the mean, and fully described by two parameters:
  - Gaussian (Normal) Distribution Formula: $P(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left(-\frac{(x - \mu)^2}{2\sigma^2}\right)$

- Understanding ‘Bayesian’:
  - The term “Bayesian” comes from Bayes’ theorem, a fundamental rule in probability theory.
  - Bayesian methods incorporate prior knowledge and update beliefs as new data arrives.
  - It is widely used in Bayesian statistics, probabilistic modeling, and machine learning algorithms like Bayesian Networks and Bayesian Optimization.
  - Bayesian Theorem Formula: $P(A | B) = \frac{P(B | A) P(A)}{P(B)}$

- In the case of unsupervised learning, we are
often interested in discovering patterns in data and learning data representations.

- During *training*, ML algorithms learn the rules (e.g., classification boundaries) based on
training examples by **optimizing an objective function**. During *testing*, ML algorithms
apply previously learned rules to new input data points and provide a prediction,

- Supervised learning can be subdivided into *classification* and *regression*, depending on
the quantity we are trying to predict. If our output y is a discrete quantity (e.g.,*K* distinct
classes), we have a classification problem. On the other hand, if our output y is a con-
tinuous quantity (e.g., a real number, such as stock price) we have a regression problem.

- *average misclassification rate*: $L(\theta) = \frac{1}{n}\sum_{i=1}^n 1[y_i ≠ f(x_i;\theta)]$
. here $1[]$ is an indicator function that is equal to 1 when the argument inside is true
and 0 otherwise

- *mean squared error*: $L(\theta) = \frac{1}{n}\sum_{i=1}^n[y_i - f(x_i;\theta)]^2$ . we subtract our prediction from the ground truth label, square it, and average the result over the data points

- The simplest example of unsupervised learning is finding clusters within data. Intui-
tively, we know that data points that belong to the same cluster have similar character-
istics. In fact, data points within a cluster can be represented by the cluster center as
an exemplar and used as part of a *data compression algorithm*. Alternatively, we can
look at the distances between clusters in a projected lower-dimensional space to
understand the interrelation between different groups. Additionally, a point that’s far
away from all the existing clusters can be considered an anomaly, leading to an anomaly detection
algorithms.

- *deep learning* ; The name comes from a stack of computational layers forming a computational graph
together. deep learning models gradually refine their parameters through a *backpropagation* algorithm until they meet the objective function.

- <img src=images/dnn.png>

- *Bayesian inference* allows us to update our beliefs about the world given observed data.

- Any *probabilistic model* is described by a set of
parameters $\theta$ modeled as random variables, which control the behavior of the model,
and associated data *x*.

- bayesian inference equation:
- The goal of Bayesian inference is to find the *posterior* distribution $p(\theta |x)$ (distribution over the parameters given the data) to capture a particular aspect of reality well.
The posterior distribution is proportional to the product of the likelihood $p(x|\theta)$ (distribution over the data given the parameters) and the prior $p(\theta)$ (initial distribution over the parameters)
<img src=images/bayesian-inference-equation.png>

- overall the goals of Bayesian inference remain the same: to find the posterior distribution over model parameters, given observed data.

- In contrast to PGMs, where the connections are specified by
domain experts, deep learning models learn representations of the world automatically
through the backpropagation algorithm that minimizes an objective function

- Deep neural networks (DNNs) consist of multiple layers parameterized by weight matrices
and bias parameters. Mathematically, DNNs can be expressed as a composition of
individual layer functions.

- **Stochastic Gradient Descent (SGD)** is an optimization algorithm used to minimize a function (usually a loss function in machine learning and deep learning) by iteratively updating the model’s parameters in the opposite direction of the gradient of the loss function. It is a variant of Gradient Descent (GD), but instead of computing the gradient using the entire dataset, SGD updates parameters using only a single or small batch of randomly sampled data points.

- Mathematical Formulation of SGD:

    - Given a loss function  $J(\theta)$ , where  $\theta$  represents the model parameters:
    - *SG*: $\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t)$
        - $\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t)$
        - $\eta$  (learning rate) controls the step size
        - $\nabla J(\theta)$ is the gradient computed over the entire dataset
    - **SGD**: $\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t; x_i, y_i)$
        -  $\nabla J(\theta_t; x_i, y_i)$ is the gradient computed on a single sample.

- there are Two main camps of Bayesian inference: MCMC and VI

- *Markov chain Monte Carlo (MCMC)* is a methodology of sampling from high-dimensional
parameter spaces to approximate the posterior distribution $p(\theta|x)$. In statistics, *sampling*
refers to generating a random sample of values from a given probability distribution: $F(x|\lambda) = \int_{i}^{x}P(x | \lambda) dx = 1 - e^{(-\lambda x)}, x \ge 0$

- 



















